{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torcheval.metrics.functional import binary_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing/ setting up data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data from tsv and get the sequence length to use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance the positive and negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sit_ups = train_df[train_df['Y'] == 1]\n",
    "train_df_non_sit_ups = train_df[train_df['Y'] == 0]\n",
    "\n",
    "\n",
    "train_df_non_sit_ups_balanced = train_df_non_sit_ups.sample(n=len(train_df_sit_ups), random_state=0)\n",
    "\n",
    "\n",
    "train_df = pd.concat([train_df_sit_ups, train_df_non_sit_ups_balanced])\n",
    "\n",
    "test_df_sit_ups = test_df[test_df['Y'] == 1]\n",
    "test_df_non_sit_ups = test_df[test_df['Y'] == 0]\n",
    "\n",
    "test_df_non_sit_ups = test_df_non_sit_ups.sample(n=len(test_df_sit_ups), random_state=0)\n",
    "\n",
    "test_df_balanced = pd.concat([test_df_sit_ups, test_df_non_sit_ups])\n",
    "test_df = test_df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate x and y from test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "\n",
    "test_x = test_df.drop(['Y'], axis=1)\n",
    "\n",
    "\n",
    "test_y = test_df['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the cells in each collumn of the dataframe from string to a numpy array and convert the whole dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1087, 749, 3)\n",
      "[ 0.51171875 -0.10406494 -0.10406494]\n"
     ]
    }
   ],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            row_array.append(col_float)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "test_x_numpy = convert_rows_to_nupy_array(test_x)\n",
    "\n",
    "print(train_x_numpy.shape)\n",
    "print(test_x_numpy[1][5])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Padding from numpy and create separate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1087,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78686/2768496912.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n",
      "/tmp/ipykernel_78686/2768496912.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_x_numpy_no_pad = np.array(test_x_numpy_no_pad)\n"
     ]
    }
   ],
   "source": [
    "train_y_no_pad = train_y\n",
    "test_y_no_pad = test_y\n",
    "\n",
    "# remove the numpy array where you hava a [0,0,0] array\n",
    "\n",
    "test_np_array = np.array([[1,2,1], [0,0,0], [1,2,1]])\n",
    "\n",
    "# remove the [0,0,0] array from the numpy array test_np_array\n",
    "\n",
    "test_np_array = test_np_array[test_np_array[:,0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "train_x_numpy_no_pad = []\n",
    "# row_index = 0\n",
    "for row in range(len(train_x_numpy)):\n",
    "    temp = train_x_numpy[row][train_x_numpy[row][:,0] != 0]\n",
    "    temp_list = []\n",
    "    for col in temp:\n",
    "        item_list = []\n",
    "        for item in col:\n",
    "            item_list.append(item)\n",
    "        temp_list.append(item_list)\n",
    "    train_x_numpy_no_pad.append(temp_list)\n",
    "train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n",
    "\n",
    "test_x_numpy_no_pad = []\n",
    "# row_index = 0\n",
    "for row in range(len(test_x_numpy)):\n",
    "    temp = test_x_numpy[row][test_x_numpy[row][:,0] != 0]\n",
    "    temp_list = []\n",
    "    for col in temp:\n",
    "        item_list = []\n",
    "        for item in col:\n",
    "            item_list.append(item)\n",
    "        temp_list.append(item_list)\n",
    "    test_x_numpy_no_pad.append(temp_list)\n",
    "test_x_numpy_no_pad = np.array(test_x_numpy_no_pad)\n",
    "print(train_x_numpy_no_pad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1087, 749, 3) (1087,)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Dummy Classifer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "print(train_x_numpy.shape, train_y.shape)\n",
    "dummy_clf.fit(train_x_numpy, train_y)\n",
    "y_pred = dummy_clf.predict(test_x_numpy)\n",
    "print(accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return  self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVariableLengthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3                                                                           \n",
    "num_classes = 1\n",
    "hidden_size = 3\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing reduce to certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of train_x_numpy_no_pad to 1000\n",
    "# train_x_numpy = train_x_numpy[:50]\n",
    "# train_y = train_y[:50]\n",
    "\n",
    "# test_x_numpy = test_x_numpy[:50]\n",
    "# test_y = test_y[:50]\n",
    "\n",
    "\n",
    "# print how many 1 and 0 in the train_y\n",
    "print(train_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitUpDetector(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers,drop_prob=0.5):\n",
    "        super(SitUpDetector, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_max_sequence = None\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,dropout=drop_prob, batch_first=False)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x,hidden_state):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # pad_sequence(x, )\n",
    "\n",
    "        x = x.unsqueeze(0)\n",
    "        out, hidden= self.lstm(x, hidden_state)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        # out = self.softmax(out)\n",
    "\n",
    "\n",
    "        # get the last output in out\n",
    "        out = out.squeeze(0)\n",
    "        out = out.squeeze(1)\n",
    "        \n",
    "        out = out[-1]\n",
    "        out = self.softmax(out)\n",
    "        return out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = (torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device), torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device))\n",
    "        self.hidden_max_sequence = hidden\n",
    "        print(\"hidden state\", self.hidden_max_sequence[0].shape)\n",
    "        return hidden\n",
    "    \n",
    "    def get_hidden_state_for_sequence(self, sequence):\n",
    "        return tuple([self.hidden_max_sequence[0].split(sequence, dim=1),self.hidden_max_sequence[1].split(sequence, dim=1)])\n",
    "\n",
    "    def update_hidden_state(self, hidden_state, sequence):\n",
    "        self.hidden_max_sequence = (torch.cat(hidden_state[0], dim=1), torch.cat(hidden_state[1], dim=1))\n",
    "        print(\"hidden state\", self.hidden_max_sequence[0].shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Optimizers, dataloader and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SitUpDetector(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "\n",
    "myVariableLengthDataset = MyVariableLengthDataset(train_x_numpy_no_pad,train_y)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myVariableLengthDataset, batch_size=batch_size)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "valid_loss_min = np.Inf\n",
    "total_steps = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state torch.Size([3, 749, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78686/3540042403.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m features, target \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputs, hidden \u001b[39m=\u001b[39m model(features\u001b[39m.\u001b[39;49mfloat(),h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mupdate_hidden_state(hidden, features\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 32\u001b[0m in \u001b[0;36mSitUpDetector.forward\u001b[0;34m(self, x, hidden_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# pad_sequence(x, )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m out, hidden\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, hidden_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:689\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 689\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    690\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    692\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:633\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    628\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    629\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    630\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    631\u001b[0m                        ):\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 633\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_hidden_size(hidden[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_expected_hidden_size(\u001b[39minput\u001b[39;49m, batch_sizes),\n\u001b[1;32m    634\u001b[0m                            \u001b[39m'\u001b[39;49m\u001b[39mExpected hidden[0] size \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m, got \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    635\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    636\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:225\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_hidden_size\u001b[39m(\u001b[39mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[1;32m    224\u001b[0m                       msg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExpected hidden size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39;49msize() \u001b[39m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m    226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(expected_hidden_size, \u001b[39mlist\u001b[39m(hx\u001b[39m.\u001b[39msize())))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    h = model.init_hidden(sequence_length)\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "        features = torch.tensor(features)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        h = model.get_hidden_state_for_sequence(features.size(0))\n",
    "\n",
    "        # h = tuple([each.data for each in h])\n",
    "\n",
    "        features, target = features.to(device), target.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs, hidden = model(features.float(),h)\n",
    "        model.update_hidden_state(hidden, features.size(0))\n",
    "        outputs = outputs.unsqueeze(0)\n",
    "        loss = criterion(outputs, target)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # if (i+1) % 50 == 0:\n",
    "        #     val_h = model.init_hidden(batch_size)\n",
    "        #     val_losses = []\n",
    "        #     model.eval()\n",
    "        #     for inp, lab in train_loader:\n",
    "        #         inp = torch.tensor(inp)\n",
    "        #         lab = torch.tensor(lab)\n",
    "        #         val_h = model.init_hidden(inp.size(0))\n",
    "        #         inp, lab = inp.float().to(device), lab.to(device)\n",
    "        #         out, val_h = model(inp, val_h)\n",
    "        #         out = out.unsqueeze(0)\n",
    "        #         print(\"out\",out)\n",
    "        #         print(\"lab\",lab)\n",
    "        #         val_loss = criterion(out, lab)\n",
    "        #         val_losses.append(val_loss.item())\n",
    "                \n",
    "        #     model.train()\n",
    "        #     print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "        #           \"Step: {}...\".format(i),\n",
    "        #           \"Loss: {:.6f}...\".format(loss.item()),\n",
    "        #           \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "        #     losses.append(loss.item())\n",
    "        #     # print(\"why\",\"{:.6f}\".format(loss.item().detach().numpy()))\n",
    "\n",
    "        print(\"loss\",loss.item())\n",
    "        losses.append(loss.item())\n",
    "            # if np.mean(val_losses) <= valid_loss_min:\n",
    "            #     torch.save(model.state_dict(), './state_dict.pt')\n",
    "            #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "            #     valid_loss_min = np.mean(val_losses)\n",
    "\n",
    "# graph the losses \n",
    "\n",
    "# get the lenght of the losses array\n",
    "\n",
    "# create an array of the same length as the losses array with index values\n",
    "for i in range(len(losses)):\n",
    "    if type(losses[i]) == torch.Tensor:\n",
    "        losses[i] = losses[i].detach().numpy()\n",
    "\n",
    "y = np.array(losses)\n",
    "print(\"plt y shpae loss\", y.shape)\n",
    "# print(\"y shpae\")\n",
    "# print(y)\n",
    "\n",
    "x = np.arange(0,len(losses))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,y)\n",
    "plt.savefig(f\"loss-for-params-num-{num_layers}-learning_rate{learning_rate}-epochs-{num_epochs}.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cm = confusion_matrix(test_y, all_predictions)\n",
    "# print(cm)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "torch.save(model.state_dict(), 'sit_up_detector.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy\n",
    "test_dataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1)\n",
    "n_correct = 0\n",
    "n_samples = 0\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for inputs, label in train_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, label = inputs.to(device), label.to(device)\n",
    "    output, h = model(inputs.float(), h)\n",
    "    test_loss = criterion(output, label.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(label.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_dataloader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with different network with data spread out and with simpler rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_flatten = train_x_numpy[0].flatten()\n",
    "\n",
    "# # flatten the rows in train_x_numpy to 1D array\n",
    "# train_x_numpy_flatten_list = []\n",
    "# for i in range(len(train_x_numpy)):\n",
    "#     train_x_numpy_flatten_list.append(train_x_numpy[i].flatten().tolist())\n",
    "\n",
    "# train_x_flatten_numpy = np.array(train_x_numpy_flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFlattenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVariableLengthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "num_classes = 2\n",
    "hidden_size = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitUpDetectorSimpleRNN(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size):\n",
    "        super(SitUpDetectorSimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, num_classes)\n",
    "    \n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x.unsqueeze(0), hidden_state),1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output  tensor([[-0.1074,  0.0873]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[0.0849, 0.1704]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1293,  0.1040]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.0969,  0.1710]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1368,  0.1107]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1261,  0.1417]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1611,  0.1517]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1649,  0.1631]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1967,  0.1349]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.0751,  0.2437]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.0400,  0.2809]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1530,  0.2581]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.0582,  0.3679]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.0490,  0.3348]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.2335,  0.2085]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.2321,  0.2312]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.2436,  0.2480]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1446,  0.3558]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1341,  0.3816]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.2817,  0.2311]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.2875,  0.2494]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.2737,  0.3008]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.3000,  0.2714]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.1668,  0.5189]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n",
      "output  tensor([[-0.2118,  0.4528]], grad_fn=<AddmmBackward0>)\n",
      "output shape torch.Size([1, 2])\n",
      "target shape torch.Size([1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m xyz \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     xyz \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(xyz)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     output, hidden_state \u001b[39m=\u001b[39m model(xyz\u001b[39m.\u001b[39;49mfloat(), hidden_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput \u001b[39m\u001b[39m\"\u001b[39m, output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput shape\u001b[39m\u001b[39m\"\u001b[39m, output\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 42\u001b[0m in \u001b[0;36mSitUpDetectorSimpleRNN.forward\u001b[0;34m(self, x, hidden_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hidden_state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), hidden_state),\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     hidden \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min2hidden(combined))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min2output(combined)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  create pytorch lstm variable recurrent classifier\n",
    "\n",
    "# train SitUpDetectorVariableInput\n",
    "model = SitUpDetectorSimpleRNN(input_size, num_classes, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "myDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(train_x_numpy_no_pad,train_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for xyz in features:\n",
    "            xyz = torch.tensor(xyz)\n",
    "            output, hidden_state = model(xyz.float(), hidden_state)\n",
    "        print(\"output \", output)\n",
    "        print(\"output shape\", output.shape)\n",
    "        print(\"target shape\", target.shape)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "            f\"Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "    losses.append(loss.item())\n",
    "\n",
    "\n",
    "y = np.array(losses)\n",
    "x = np.arange(0,len(losses))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,y)\n",
    "plt.savefig(f\"simple-rnn-loss-for-params-num-{num_layers}-learning_rate{learning_rate}-epochs-{num_epochs}.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myTestDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(test_x_numpy_no_pad,test_y)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=myTestDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "num_correct = 0\n",
    "num_samples = len(test_loader)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (features, target) in enumerate(test_loader):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for xyz in features:\n",
    "            xyz = torch.tensor(xyz)\n",
    "            output, hidden_state = model(xyz.float(), hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == target)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "\n",
    "# save the accuracy and model hyperparameters to a text file \n",
    "\n",
    "with open(f\"simple-rnn-accuracy-for-params-num-{num_layers}-learning_rate{learning_rate}-epochs-{num_epochs}.txt\", \"w\") as f:\n",
    "    f.write(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "    f.write(f\"num_layers: {num_layers}\")\n",
    "    f.write(f\"learning_rate: {learning_rate}\")\n",
    "    f.write(f\"num_epochs: {num_epochs}\")\n",
    "    f.write(f\"hidden_size: {hidden_size}\")\n",
    "    f.write(f\"input_size: {input_size}\")\n",
    "    f.write(f\"num_classes: {num_classes}\")\n",
    "    f.write(f\"optimizer: {optimizer}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper param tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nbmultitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.3704\n",
      "Epoch [2/200], Loss: 0.1424\n",
      "Epoch [3/200], Loss: 0.0913\n",
      "Epoch [4/200], Loss: 0.0744\n",
      "Epoch [5/200], Loss: 0.0677\n",
      "Epoch [6/200], Loss: 0.0645\n",
      "Epoch [7/200], Loss: 0.0626\n",
      "Epoch [8/200], Loss: 0.0613\n",
      "Epoch [9/200], Loss: 0.0602\n",
      "Epoch [10/200], Loss: 0.0593\n",
      "Epoch [11/200], Loss: 0.0585\n",
      "Epoch [12/200], Loss: 0.0577\n",
      "Epoch [13/200], Loss: 0.0569\n",
      "Epoch [14/200], Loss: 0.0562\n",
      "Epoch [15/200], Loss: 0.0555\n",
      "Epoch [16/200], Loss: 0.0549\n",
      "Epoch [17/200], Loss: 0.0543\n",
      "Epoch [18/200], Loss: 0.0537\n",
      "Epoch [19/200], Loss: 0.0531\n",
      "Epoch [20/200], Loss: 0.0525\n",
      "Epoch [21/200], Loss: 0.0519\n",
      "Epoch [22/200], Loss: 0.0514\n",
      "Epoch [23/200], Loss: 0.0508\n",
      "Epoch [24/200], Loss: 0.0503\n",
      "Epoch [25/200], Loss: 0.0498\n",
      "Epoch [26/200], Loss: 0.0494\n",
      "Epoch [27/200], Loss: 0.0490\n",
      "Epoch [28/200], Loss: 0.0486\n",
      "Epoch [29/200], Loss: 0.0482\n",
      "Epoch [30/200], Loss: 0.0479\n",
      "Epoch [31/200], Loss: 0.0476\n",
      "Epoch [32/200], Loss: 0.0473\n",
      "Epoch [33/200], Loss: 0.0470\n",
      "Epoch [34/200], Loss: 0.0467\n",
      "Epoch [35/200], Loss: 0.0464\n",
      "Epoch [36/200], Loss: 0.0462\n",
      "Epoch [37/200], Loss: 0.0459\n",
      "Epoch [38/200], Loss: 0.0457\n",
      "Epoch [39/200], Loss: 0.0455\n",
      "Epoch [40/200], Loss: 0.0453\n",
      "Epoch [41/200], Loss: 0.0452\n",
      "Epoch [42/200], Loss: 0.0450\n",
      "Epoch [43/200], Loss: 0.0449\n",
      "Epoch [44/200], Loss: 0.0448\n",
      "Epoch [45/200], Loss: 0.0447\n",
      "Epoch [46/200], Loss: 0.0447\n",
      "Epoch [47/200], Loss: 0.0446\n",
      "Epoch [48/200], Loss: 0.0446\n",
      "Epoch [49/200], Loss: 0.0445\n",
      "Epoch [50/200], Loss: 0.0445\n",
      "Epoch [51/200], Loss: 0.0445\n",
      "Epoch [52/200], Loss: 0.0445\n",
      "Epoch [53/200], Loss: 0.0444\n",
      "Epoch [54/200], Loss: 0.0444\n",
      "Epoch [55/200], Loss: 0.0444\n",
      "Epoch [56/200], Loss: 0.0444\n",
      "Epoch [57/200], Loss: 0.0443\n",
      "Epoch [58/200], Loss: 0.0443\n",
      "Epoch [59/200], Loss: 0.0443\n",
      "Epoch [60/200], Loss: 0.0442\n",
      "Epoch [61/200], Loss: 0.0442\n",
      "Epoch [62/200], Loss: 0.0441\n",
      "Epoch [63/200], Loss: 0.0440\n",
      "Epoch [64/200], Loss: 0.0439\n",
      "Epoch [65/200], Loss: 0.0439\n",
      "Epoch [66/200], Loss: 0.0438\n",
      "Epoch [67/200], Loss: 0.0437\n",
      "Epoch [68/200], Loss: 0.0436\n",
      "Epoch [69/200], Loss: 0.0435\n",
      "Epoch [70/200], Loss: 0.0434\n",
      "Epoch [71/200], Loss: 0.0433\n",
      "Epoch [72/200], Loss: 0.0432\n",
      "Epoch [73/200], Loss: 0.0431\n",
      "Epoch [74/200], Loss: 0.0429\n",
      "Epoch [75/200], Loss: 0.0428\n",
      "Epoch [76/200], Loss: 0.0426\n",
      "Epoch [77/200], Loss: 0.0425\n",
      "Epoch [78/200], Loss: 0.0423\n",
      "Epoch [79/200], Loss: 0.0421\n",
      "Epoch [80/200], Loss: 0.0419\n",
      "Epoch [81/200], Loss: 0.0417\n",
      "Epoch [82/200], Loss: 0.0415\n",
      "Epoch [83/200], Loss: 0.0412\n",
      "Epoch [84/200], Loss: 0.0410\n",
      "Epoch [85/200], Loss: 0.0407\n",
      "Epoch [86/200], Loss: 0.0404\n",
      "Epoch [87/200], Loss: 0.0401\n",
      "Epoch [88/200], Loss: 0.0398\n",
      "Epoch [89/200], Loss: 0.0394\n",
      "Epoch [90/200], Loss: 0.0391\n",
      "Epoch [91/200], Loss: 0.0387\n",
      "Epoch [92/200], Loss: 0.0382\n",
      "Epoch [93/200], Loss: 0.0378\n",
      "Epoch [94/200], Loss: 0.0373\n",
      "Epoch [95/200], Loss: 0.0368\n",
      "Epoch [96/200], Loss: 0.0362\n",
      "Epoch [97/200], Loss: 0.0357\n",
      "Epoch [98/200], Loss: 0.0351\n",
      "Epoch [99/200], Loss: 0.0345\n",
      "Epoch [100/200], Loss: 0.0339\n",
      "Epoch [101/200], Loss: 0.0332\n",
      "Epoch [102/200], Loss: 0.0326\n",
      "Epoch [103/200], Loss: 0.0319\n",
      "Epoch [104/200], Loss: 0.0312\n",
      "Epoch [105/200], Loss: 0.0305\n",
      "Epoch [106/200], Loss: 0.0298\n",
      "Epoch [107/200], Loss: 0.0291\n",
      "Epoch [108/200], Loss: 0.0284\n",
      "Epoch [109/200], Loss: 0.0278\n",
      "Epoch [110/200], Loss: 0.0271\n",
      "Epoch [111/200], Loss: 0.0265\n",
      "Epoch [112/200], Loss: 0.0258\n",
      "Epoch [113/200], Loss: 0.0253\n",
      "Epoch [114/200], Loss: 0.0247\n",
      "Epoch [115/200], Loss: 0.0242\n",
      "Epoch [116/200], Loss: 0.0237\n",
      "Epoch [117/200], Loss: 0.0232\n",
      "Epoch [118/200], Loss: 0.0227\n",
      "Epoch [119/200], Loss: 0.0222\n",
      "Epoch [120/200], Loss: 0.0218\n",
      "Epoch [121/200], Loss: 0.0213\n",
      "Epoch [122/200], Loss: 0.0209\n",
      "Epoch [123/200], Loss: 0.0204\n",
      "Epoch [124/200], Loss: 0.0200\n",
      "Epoch [125/200], Loss: 0.0197\n",
      "Epoch [126/200], Loss: 0.0193\n",
      "Epoch [127/200], Loss: 0.0190\n",
      "Epoch [128/200], Loss: 0.0186\n",
      "Epoch [129/200], Loss: 0.0183\n",
      "Epoch [130/200], Loss: 0.0180\n",
      "Epoch [131/200], Loss: 0.0177\n",
      "Epoch [132/200], Loss: 0.0175\n",
      "Epoch [133/200], Loss: 0.0172\n",
      "Epoch [134/200], Loss: 0.0170\n",
      "Epoch [135/200], Loss: 0.0168\n",
      "Epoch [136/200], Loss: 0.0166\n",
      "Epoch [137/200], Loss: 0.0164\n",
      "Epoch [138/200], Loss: 0.0162\n",
      "Epoch [139/200], Loss: 0.0160\n",
      "Epoch [140/200], Loss: 0.0158\n",
      "Epoch [141/200], Loss: 0.0157\n",
      "Epoch [142/200], Loss: 0.0155\n",
      "Epoch [143/200], Loss: 0.0154\n",
      "Epoch [144/200], Loss: 0.0152\n",
      "Epoch [145/200], Loss: 0.0151\n",
      "Epoch [146/200], Loss: 0.0150\n",
      "Epoch [147/200], Loss: 0.0149\n",
      "Epoch [148/200], Loss: 0.0147\n",
      "Epoch [149/200], Loss: 0.0146\n",
      "Epoch [150/200], Loss: 0.0145\n",
      "Epoch [151/200], Loss: 0.0144\n",
      "Epoch [152/200], Loss: 0.0143\n",
      "Epoch [153/200], Loss: 0.0142\n",
      "Epoch [154/200], Loss: 0.0141\n",
      "Epoch [155/200], Loss: 0.0139\n",
      "Epoch [156/200], Loss: 0.0138\n",
      "Epoch [157/200], Loss: 0.0137\n",
      "Epoch [158/200], Loss: 0.0136\n",
      "Epoch [159/200], Loss: 0.0135\n",
      "Epoch [160/200], Loss: 0.0134\n",
      "Epoch [161/200], Loss: 0.0133\n",
      "Epoch [162/200], Loss: 0.0132\n",
      "Epoch [163/200], Loss: 0.0131\n",
      "Epoch [164/200], Loss: 0.0130\n",
      "Epoch [165/200], Loss: 0.0129\n",
      "Epoch [166/200], Loss: 0.0128\n",
      "Epoch [167/200], Loss: 0.0127\n",
      "Epoch [168/200], Loss: 0.0126\n",
      "Epoch [169/200], Loss: 0.0125\n",
      "Epoch [170/200], Loss: 0.0124\n",
      "Epoch [171/200], Loss: 0.0123\n",
      "Epoch [172/200], Loss: 0.0121\n",
      "Epoch [173/200], Loss: 0.0120\n",
      "Epoch [174/200], Loss: 0.0119\n",
      "Epoch [175/200], Loss: 0.0118\n",
      "Epoch [176/200], Loss: 0.0117\n",
      "Epoch [177/200], Loss: 0.0116\n",
      "Epoch [178/200], Loss: 0.0115\n",
      "Epoch [179/200], Loss: 0.0114\n",
      "Epoch [180/200], Loss: 0.0112\n",
      "Epoch [181/200], Loss: 0.0111\n",
      "Epoch [182/200], Loss: 0.0110\n",
      "Epoch [183/200], Loss: 0.0109\n",
      "Epoch [184/200], Loss: 0.0108\n",
      "Epoch [185/200], Loss: 0.0107\n",
      "Epoch [186/200], Loss: 0.0106\n",
      "Epoch [187/200], Loss: 0.0104\n",
      "Epoch [188/200], Loss: 0.0103\n",
      "Epoch [189/200], Loss: 0.0102\n",
      "Epoch [190/200], Loss: 0.0101\n",
      "Epoch [191/200], Loss: 0.0099\n",
      "Epoch [192/200], Loss: 0.0098\n",
      "Epoch [193/200], Loss: 0.0097\n",
      "Epoch [194/200], Loss: 0.0096\n",
      "Epoch [195/200], Loss: 0.0095\n",
      "Epoch [196/200], Loss: 0.0093\n",
      "Epoch [197/200], Loss: 0.0092\n",
      "Epoch [198/200], Loss: 0.0091\n",
      "Epoch [199/200], Loss: 0.0090\n",
      "Epoch [200/200], Loss: 0.0090\n",
      "Accuracy: 65.6780%\n",
      "Epoch [1/200], Loss: 0.0930\n",
      "Epoch [2/200], Loss: 0.0524\n",
      "Epoch [3/200], Loss: 0.0419\n",
      "Epoch [4/200], Loss: 0.0387\n",
      "Epoch [5/200], Loss: 0.0378\n",
      "Epoch [6/200], Loss: 0.0376\n",
      "Epoch [7/200], Loss: 0.0375\n",
      "Epoch [8/200], Loss: 0.0374\n",
      "Epoch [9/200], Loss: 0.0373\n",
      "Epoch [10/200], Loss: 0.0370\n",
      "Epoch [11/200], Loss: 0.0368\n",
      "Epoch [12/200], Loss: 0.0365\n",
      "Epoch [13/200], Loss: 0.0362\n",
      "Epoch [14/200], Loss: 0.0359\n",
      "Epoch [15/200], Loss: 0.0355\n",
      "Epoch [16/200], Loss: 0.0352\n",
      "Epoch [17/200], Loss: 0.0349\n",
      "Epoch [18/200], Loss: 0.0346\n",
      "Epoch [19/200], Loss: 0.0343\n",
      "Epoch [20/200], Loss: 0.0340\n",
      "Epoch [21/200], Loss: 0.0338\n",
      "Epoch [22/200], Loss: 0.0335\n",
      "Epoch [23/200], Loss: 0.0334\n",
      "Epoch [24/200], Loss: 0.0332\n",
      "Epoch [25/200], Loss: 0.0330\n",
      "Epoch [26/200], Loss: 0.0328\n",
      "Epoch [27/200], Loss: 0.0326\n",
      "Epoch [28/200], Loss: 0.0324\n",
      "Epoch [29/200], Loss: 0.0321\n",
      "Epoch [30/200], Loss: 0.0318\n",
      "Epoch [31/200], Loss: 0.0314\n",
      "Epoch [32/200], Loss: 0.0310\n",
      "Epoch [33/200], Loss: 0.0305\n",
      "Epoch [34/200], Loss: 0.0299\n",
      "Epoch [35/200], Loss: 0.0293\n",
      "Epoch [36/200], Loss: 0.0287\n",
      "Epoch [37/200], Loss: 0.0280\n",
      "Epoch [38/200], Loss: 0.0273\n",
      "Epoch [39/200], Loss: 0.0266\n",
      "Epoch [40/200], Loss: 0.0258\n",
      "Epoch [41/200], Loss: 0.0250\n",
      "Epoch [42/200], Loss: 0.0242\n",
      "Epoch [43/200], Loss: 0.0233\n",
      "Epoch [44/200], Loss: 0.0225\n",
      "Epoch [45/200], Loss: 0.0217\n",
      "Epoch [46/200], Loss: 0.0209\n",
      "Epoch [47/200], Loss: 0.0201\n",
      "Epoch [48/200], Loss: 0.0194\n",
      "Epoch [49/200], Loss: 0.0186\n",
      "Epoch [50/200], Loss: 0.0179\n",
      "Epoch [51/200], Loss: 0.0172\n",
      "Epoch [52/200], Loss: 0.0165\n",
      "Epoch [53/200], Loss: 0.0158\n",
      "Epoch [54/200], Loss: 0.0152\n",
      "Epoch [55/200], Loss: 0.0146\n",
      "Epoch [56/200], Loss: 0.0140\n",
      "Epoch [57/200], Loss: 0.0134\n",
      "Epoch [58/200], Loss: 0.0129\n",
      "Epoch [59/200], Loss: 0.0124\n",
      "Epoch [60/200], Loss: 0.0119\n",
      "Epoch [61/200], Loss: 0.0115\n",
      "Epoch [62/200], Loss: 0.0110\n",
      "Epoch [63/200], Loss: 0.0106\n",
      "Epoch [64/200], Loss: 0.0102\n",
      "Epoch [65/200], Loss: 0.0099\n",
      "Epoch [66/200], Loss: 0.0095\n",
      "Epoch [67/200], Loss: 0.0092\n",
      "Epoch [68/200], Loss: 0.0089\n",
      "Epoch [69/200], Loss: 0.0086\n",
      "Epoch [70/200], Loss: 0.0083\n",
      "Epoch [71/200], Loss: 0.0081\n",
      "Epoch [72/200], Loss: 0.0078\n",
      "Epoch [73/200], Loss: 0.0076\n",
      "Epoch [74/200], Loss: 0.0074\n",
      "Epoch [75/200], Loss: 0.0072\n",
      "Epoch [76/200], Loss: 0.0070\n",
      "Epoch [77/200], Loss: 0.0068\n",
      "Epoch [78/200], Loss: 0.0066\n",
      "Epoch [79/200], Loss: 0.0064\n",
      "Epoch [80/200], Loss: 0.0063\n",
      "Epoch [81/200], Loss: 0.0061\n",
      "Epoch [82/200], Loss: 0.0060\n",
      "Epoch [83/200], Loss: 0.0059\n",
      "Epoch [84/200], Loss: 0.0057\n",
      "Epoch [85/200], Loss: 0.0056\n",
      "Epoch [86/200], Loss: 0.0055\n",
      "Epoch [87/200], Loss: 0.0054\n",
      "Epoch [88/200], Loss: 0.0053\n",
      "Epoch [89/200], Loss: 0.0052\n",
      "Epoch [90/200], Loss: 0.0051\n",
      "Epoch [91/200], Loss: 0.0050\n",
      "Epoch [92/200], Loss: 0.0049\n",
      "Epoch [93/200], Loss: 0.0049\n",
      "Epoch [94/200], Loss: 0.0048\n",
      "Epoch [95/200], Loss: 0.0047\n",
      "Epoch [96/200], Loss: 0.0047\n",
      "Epoch [97/200], Loss: 0.0046\n",
      "Epoch [98/200], Loss: 0.0045\n",
      "Epoch [99/200], Loss: 0.0045\n",
      "Epoch [100/200], Loss: 0.0044\n",
      "Epoch [101/200], Loss: 0.0044\n",
      "Epoch [102/200], Loss: 0.0044\n",
      "Epoch [103/200], Loss: 0.0043\n",
      "Epoch [104/200], Loss: 0.0043\n",
      "Epoch [105/200], Loss: 0.0042\n",
      "Epoch [106/200], Loss: 0.0042\n",
      "Epoch [107/200], Loss: 0.0042\n",
      "Epoch [108/200], Loss: 0.0041\n",
      "Epoch [109/200], Loss: 0.0041\n",
      "Epoch [110/200], Loss: 0.0041\n",
      "Epoch [111/200], Loss: 0.0041\n",
      "Epoch [112/200], Loss: 0.0040\n",
      "Epoch [113/200], Loss: 0.0040\n",
      "Epoch [114/200], Loss: 0.0040\n",
      "Epoch [115/200], Loss: 0.0040\n",
      "Epoch [116/200], Loss: 0.0040\n",
      "Epoch [117/200], Loss: 0.0040\n",
      "Epoch [118/200], Loss: 0.0039\n",
      "Epoch [119/200], Loss: 0.0039\n",
      "Epoch [120/200], Loss: 0.0039\n",
      "Epoch [121/200], Loss: 0.0039\n",
      "Epoch [122/200], Loss: 0.0039\n",
      "Epoch [123/200], Loss: 0.0039\n",
      "Epoch [124/200], Loss: 0.0039\n",
      "Epoch [125/200], Loss: 0.0039\n",
      "Epoch [126/200], Loss: 0.0039\n",
      "Epoch [127/200], Loss: 0.0039\n",
      "Epoch [128/200], Loss: 0.0039\n",
      "Epoch [129/200], Loss: 0.0039\n",
      "Epoch [130/200], Loss: 0.0039\n",
      "Epoch [131/200], Loss: 0.0040\n",
      "Epoch [132/200], Loss: 0.0040\n",
      "Epoch [133/200], Loss: 0.0040\n",
      "Epoch [134/200], Loss: 0.0040\n",
      "Epoch [135/200], Loss: 0.0040\n",
      "Epoch [136/200], Loss: 0.0040\n",
      "Epoch [137/200], Loss: 0.0040\n",
      "Epoch [138/200], Loss: 0.0040\n",
      "Epoch [139/200], Loss: 0.0040\n",
      "Epoch [140/200], Loss: 0.0040\n",
      "Epoch [141/200], Loss: 0.0040\n",
      "Epoch [142/200], Loss: 0.0040\n",
      "Epoch [143/200], Loss: 0.0040\n",
      "Epoch [144/200], Loss: 0.0040\n",
      "Epoch [145/200], Loss: 0.0040\n",
      "Epoch [146/200], Loss: 0.0040\n",
      "Epoch [147/200], Loss: 0.0041\n",
      "Epoch [148/200], Loss: 0.0041\n",
      "Epoch [149/200], Loss: 0.0042\n",
      "Epoch [150/200], Loss: 0.0043\n",
      "Epoch [151/200], Loss: 0.0043\n",
      "Epoch [152/200], Loss: 0.0044\n",
      "Epoch [153/200], Loss: 0.0044\n",
      "Epoch [154/200], Loss: 0.0043\n",
      "Epoch [155/200], Loss: 0.0043\n",
      "Epoch [156/200], Loss: 0.0043\n",
      "Epoch [157/200], Loss: 0.0042\n",
      "Epoch [158/200], Loss: 0.0041\n",
      "Epoch [159/200], Loss: 0.0039\n",
      "Epoch [160/200], Loss: 0.0037\n",
      "Epoch [161/200], Loss: 0.0033\n",
      "Epoch [162/200], Loss: 0.0030\n",
      "Epoch [163/200], Loss: 0.0027\n",
      "Epoch [164/200], Loss: 0.0024\n",
      "Epoch [165/200], Loss: 0.0022\n",
      "Epoch [166/200], Loss: 0.0019\n",
      "Epoch [167/200], Loss: 0.0017\n",
      "Epoch [168/200], Loss: 0.0015\n",
      "Epoch [169/200], Loss: 0.0014\n",
      "Epoch [170/200], Loss: 0.0013\n",
      "Epoch [171/200], Loss: 0.0013\n",
      "Epoch [172/200], Loss: 0.0012\n",
      "Epoch [173/200], Loss: 0.0012\n",
      "Epoch [174/200], Loss: 0.0012\n",
      "Epoch [175/200], Loss: 0.0012\n",
      "Epoch [176/200], Loss: 0.0012\n",
      "Epoch [177/200], Loss: 0.0012\n",
      "Epoch [178/200], Loss: 0.0012\n",
      "Epoch [179/200], Loss: 0.0013\n",
      "Epoch [180/200], Loss: 0.0013\n",
      "Epoch [181/200], Loss: 0.0013\n",
      "Epoch [182/200], Loss: 0.0013\n",
      "Epoch [183/200], Loss: 0.0013\n",
      "Epoch [184/200], Loss: 0.0013\n",
      "Epoch [185/200], Loss: 0.0014\n",
      "Epoch [186/200], Loss: 0.0014\n",
      "Epoch [187/200], Loss: 0.0014\n",
      "Epoch [188/200], Loss: 0.0014\n",
      "Epoch [189/200], Loss: 0.0014\n",
      "Epoch [190/200], Loss: 0.0014\n",
      "Epoch [191/200], Loss: 0.0014\n",
      "Epoch [192/200], Loss: 0.0014\n",
      "Epoch [193/200], Loss: 0.0014\n",
      "Epoch [194/200], Loss: 0.0014\n",
      "Epoch [195/200], Loss: 0.0014\n",
      "Epoch [196/200], Loss: 0.0014\n",
      "Epoch [197/200], Loss: 0.0014\n",
      "Epoch [198/200], Loss: 0.0014\n",
      "Epoch [199/200], Loss: 0.0014\n",
      "Epoch [200/200], Loss: 0.0014\n",
      "Accuracy: 61.4407%\n",
      "Epoch [1/200], Loss: 0.0163\n",
      "Epoch [2/200], Loss: 0.0113\n",
      "Epoch [3/200], Loss: 0.0095\n",
      "Epoch [4/200], Loss: 0.0090\n",
      "Epoch [5/200], Loss: 0.0093\n",
      "Epoch [6/200], Loss: 0.0099\n",
      "Epoch [7/200], Loss: 0.0106\n",
      "Epoch [8/200], Loss: 0.0112\n",
      "Epoch [9/200], Loss: 0.0119\n",
      "Epoch [10/200], Loss: 0.0124\n",
      "Epoch [11/200], Loss: 0.0128\n",
      "Epoch [12/200], Loss: 0.0132\n",
      "Epoch [13/200], Loss: 0.0135\n",
      "Epoch [14/200], Loss: 0.0137\n",
      "Epoch [15/200], Loss: 0.0139\n",
      "Epoch [16/200], Loss: 0.0140\n",
      "Epoch [17/200], Loss: 0.0141\n",
      "Epoch [18/200], Loss: 0.0142\n",
      "Epoch [19/200], Loss: 0.0142\n",
      "Epoch [20/200], Loss: 0.0141\n",
      "Epoch [21/200], Loss: 0.0140\n",
      "Epoch [22/200], Loss: 0.0137\n",
      "Epoch [23/200], Loss: 0.0133\n",
      "Epoch [24/200], Loss: 0.0128\n",
      "Epoch [25/200], Loss: 0.0123\n",
      "Epoch [26/200], Loss: 0.0118\n",
      "Epoch [27/200], Loss: 0.0114\n",
      "Epoch [28/200], Loss: 0.0110\n",
      "Epoch [29/200], Loss: 0.0106\n",
      "Epoch [30/200], Loss: 0.0102\n",
      "Epoch [31/200], Loss: 0.0099\n",
      "Epoch [32/200], Loss: 0.0094\n",
      "Epoch [33/200], Loss: 0.0090\n",
      "Epoch [34/200], Loss: 0.0085\n",
      "Epoch [35/200], Loss: 0.0080\n",
      "Epoch [36/200], Loss: 0.0074\n",
      "Epoch [37/200], Loss: 0.0069\n",
      "Epoch [38/200], Loss: 0.0064\n",
      "Epoch [39/200], Loss: 0.0059\n",
      "Epoch [40/200], Loss: 0.0054\n",
      "Epoch [41/200], Loss: 0.0049\n",
      "Epoch [42/200], Loss: 0.0045\n",
      "Epoch [43/200], Loss: 0.0041\n",
      "Epoch [44/200], Loss: 0.0037\n",
      "Epoch [45/200], Loss: 0.0034\n",
      "Epoch [46/200], Loss: 0.0031\n",
      "Epoch [47/200], Loss: 0.0028\n",
      "Epoch [48/200], Loss: 0.0025\n",
      "Epoch [49/200], Loss: 0.0023\n",
      "Epoch [50/200], Loss: 0.0021\n",
      "Epoch [51/200], Loss: 0.0019\n",
      "Epoch [52/200], Loss: 0.0018\n",
      "Epoch [53/200], Loss: 0.0016\n",
      "Epoch [54/200], Loss: 0.0015\n",
      "Epoch [55/200], Loss: 0.0014\n",
      "Epoch [56/200], Loss: 0.0013\n",
      "Epoch [57/200], Loss: 0.0012\n",
      "Epoch [58/200], Loss: 0.0011\n",
      "Epoch [59/200], Loss: 0.0011\n",
      "Epoch [60/200], Loss: 0.0010\n",
      "Epoch [61/200], Loss: 0.0010\n",
      "Epoch [62/200], Loss: 0.0009\n",
      "Epoch [63/200], Loss: 0.0009\n",
      "Epoch [64/200], Loss: 0.0009\n",
      "Epoch [65/200], Loss: 0.0008\n",
      "Epoch [66/200], Loss: 0.0008\n",
      "Epoch [67/200], Loss: 0.0008\n",
      "Epoch [68/200], Loss: 0.0007\n",
      "Epoch [69/200], Loss: 0.0007\n",
      "Epoch [70/200], Loss: 0.0007\n",
      "Epoch [71/200], Loss: 0.0007\n",
      "Epoch [72/200], Loss: 0.0007\n",
      "Epoch [73/200], Loss: 0.0007\n",
      "Epoch [74/200], Loss: 0.0007\n",
      "Epoch [75/200], Loss: 0.0007\n",
      "Epoch [76/200], Loss: 0.0007\n",
      "Epoch [77/200], Loss: 0.0007\n",
      "Epoch [78/200], Loss: 0.0007\n",
      "Epoch [79/200], Loss: 0.0007\n",
      "Epoch [80/200], Loss: 0.0006\n",
      "Epoch [81/200], Loss: 0.0006\n",
      "Epoch [82/200], Loss: 0.0006\n",
      "Epoch [83/200], Loss: 0.0006\n",
      "Epoch [84/200], Loss: 0.0005\n",
      "Epoch [85/200], Loss: 0.0005\n",
      "Epoch [86/200], Loss: 0.0005\n",
      "Epoch [87/200], Loss: 0.0004\n",
      "Epoch [88/200], Loss: 0.0003\n",
      "Epoch [89/200], Loss: 0.0003\n",
      "Epoch [90/200], Loss: 0.0002\n",
      "Epoch [91/200], Loss: 0.0002\n",
      "Epoch [92/200], Loss: 0.0002\n",
      "Epoch [93/200], Loss: 0.0002\n",
      "Epoch [94/200], Loss: 0.0002\n",
      "Epoch [95/200], Loss: 0.0002\n",
      "Epoch [96/200], Loss: 0.0002\n",
      "Epoch [97/200], Loss: 0.0002\n",
      "Epoch [98/200], Loss: 0.0002\n",
      "Epoch [99/200], Loss: 0.0002\n",
      "Epoch [100/200], Loss: 0.0003\n",
      "Epoch [101/200], Loss: 0.0003\n",
      "Epoch [102/200], Loss: 0.0003\n",
      "Epoch [103/200], Loss: 0.0003\n",
      "Epoch [104/200], Loss: 0.0004\n",
      "Epoch [105/200], Loss: 0.0004\n",
      "Epoch [106/200], Loss: 0.0004\n",
      "Epoch [107/200], Loss: 0.0003\n",
      "Epoch [108/200], Loss: 0.0004\n",
      "Epoch [109/200], Loss: 0.0003\n",
      "Epoch [110/200], Loss: 0.0004\n",
      "Epoch [111/200], Loss: 0.0003\n",
      "Epoch [112/200], Loss: 0.0003\n",
      "Epoch [113/200], Loss: 0.0003\n",
      "Epoch [114/200], Loss: 0.0004\n",
      "Epoch [115/200], Loss: 0.0004\n",
      "Epoch [116/200], Loss: 0.0004\n",
      "Epoch [117/200], Loss: 0.0004\n",
      "Epoch [118/200], Loss: 0.0004\n",
      "Epoch [119/200], Loss: 0.0004\n",
      "Epoch [120/200], Loss: 0.0004\n",
      "Epoch [121/200], Loss: 0.0004\n",
      "Epoch [122/200], Loss: 0.0005\n",
      "Epoch [123/200], Loss: 0.0005\n",
      "Epoch [124/200], Loss: 0.0005\n",
      "Epoch [125/200], Loss: 0.0005\n",
      "Epoch [126/200], Loss: 0.0005\n",
      "Epoch [127/200], Loss: 0.0005\n",
      "Epoch [128/200], Loss: 0.0005\n",
      "Epoch [129/200], Loss: 0.0005\n",
      "Epoch [130/200], Loss: 0.0005\n",
      "Epoch [131/200], Loss: 0.0006\n",
      "Epoch [132/200], Loss: 0.0006\n",
      "Epoch [133/200], Loss: 0.0006\n",
      "Epoch [134/200], Loss: 0.0006\n",
      "Epoch [135/200], Loss: 0.0006\n",
      "Epoch [136/200], Loss: 0.0006\n",
      "Epoch [137/200], Loss: 0.0006\n",
      "Epoch [138/200], Loss: 0.0005\n",
      "Epoch [139/200], Loss: 0.0005\n",
      "Epoch [140/200], Loss: 0.0006\n",
      "Epoch [141/200], Loss: 0.0005\n",
      "Epoch [142/200], Loss: 0.0004\n",
      "Epoch [143/200], Loss: 0.0005\n",
      "Epoch [144/200], Loss: 0.0005\n",
      "Epoch [145/200], Loss: 0.0005\n",
      "Epoch [146/200], Loss: 0.0005\n",
      "Epoch [147/200], Loss: 0.0005\n",
      "Epoch [148/200], Loss: 0.0005\n",
      "Epoch [149/200], Loss: 0.0005\n",
      "Epoch [150/200], Loss: 0.0005\n",
      "Epoch [151/200], Loss: 0.0005\n",
      "Epoch [152/200], Loss: 0.0005\n",
      "Epoch [153/200], Loss: 0.0004\n",
      "Epoch [154/200], Loss: 0.0004\n",
      "Epoch [155/200], Loss: 0.0003\n",
      "Epoch [156/200], Loss: 0.0003\n",
      "Epoch [157/200], Loss: 0.0002\n",
      "Epoch [158/200], Loss: 0.0002\n",
      "Epoch [159/200], Loss: 0.0002\n",
      "Epoch [160/200], Loss: 0.0002\n",
      "Epoch [161/200], Loss: 0.0002\n",
      "Epoch [162/200], Loss: 0.0002\n",
      "Epoch [163/200], Loss: 0.0002\n",
      "Epoch [164/200], Loss: 0.0002\n",
      "Epoch [165/200], Loss: 0.0002\n",
      "Epoch [166/200], Loss: 0.0002\n",
      "Epoch [167/200], Loss: 0.0002\n",
      "Epoch [168/200], Loss: 0.0001\n",
      "Epoch [169/200], Loss: 0.0002\n",
      "Epoch [170/200], Loss: 0.0002\n",
      "Epoch [171/200], Loss: 0.0002\n",
      "Epoch [172/200], Loss: 0.0001\n",
      "Epoch [173/200], Loss: 0.0001\n",
      "Epoch [174/200], Loss: 0.0001\n",
      "Epoch [175/200], Loss: 0.0001\n",
      "Epoch [176/200], Loss: 0.0001\n",
      "Epoch [177/200], Loss: 0.0001\n",
      "Epoch [178/200], Loss: 0.0001\n",
      "Epoch [179/200], Loss: 0.0001\n",
      "Epoch [180/200], Loss: 0.0001\n",
      "Epoch [181/200], Loss: 0.0001\n",
      "Epoch [182/200], Loss: 0.0001\n",
      "Epoch [183/200], Loss: 0.0001\n",
      "Epoch [184/200], Loss: 0.0001\n",
      "Epoch [185/200], Loss: 0.0001\n",
      "Epoch [186/200], Loss: 0.0001\n",
      "Epoch [187/200], Loss: 0.0001\n",
      "Epoch [188/200], Loss: 0.0001\n",
      "Epoch [189/200], Loss: 0.0001\n",
      "Epoch [190/200], Loss: 0.0001\n",
      "Epoch [191/200], Loss: 0.0001\n",
      "Epoch [192/200], Loss: 0.0001\n",
      "Epoch [193/200], Loss: 0.0001\n",
      "Epoch [194/200], Loss: 0.0001\n",
      "Epoch [195/200], Loss: 0.0001\n",
      "Epoch [196/200], Loss: 0.0001\n",
      "Epoch [197/200], Loss: 0.0001\n",
      "Epoch [198/200], Loss: 0.0001\n",
      "Epoch [199/200], Loss: 0.0001\n",
      "Epoch [200/200], Loss: 0.0001\n",
      "Accuracy: 65.6780%\n",
      "Epoch [1/200], Loss: 0.0054\n",
      "Epoch [2/200], Loss: 0.0037\n",
      "Epoch [3/200], Loss: 0.0030\n",
      "Epoch [4/200], Loss: 0.0028\n",
      "Epoch [5/200], Loss: 0.0029\n",
      "Epoch [6/200], Loss: 0.0031\n",
      "Epoch [7/200], Loss: 0.0034\n",
      "Epoch [8/200], Loss: 0.0038\n",
      "Epoch [9/200], Loss: 0.0042\n",
      "Epoch [10/200], Loss: 0.0046\n",
      "Epoch [11/200], Loss: 0.0050\n",
      "Epoch [12/200], Loss: 0.0054\n",
      "Epoch [13/200], Loss: 0.0058\n",
      "Epoch [14/200], Loss: 0.0061\n",
      "Epoch [15/200], Loss: 0.0064\n",
      "Epoch [16/200], Loss: 0.0065\n",
      "Epoch [17/200], Loss: 0.0066\n",
      "Epoch [18/200], Loss: 0.0064\n",
      "Epoch [19/200], Loss: 0.0060\n",
      "Epoch [20/200], Loss: 0.0056\n",
      "Epoch [21/200], Loss: 0.0050\n",
      "Epoch [22/200], Loss: 0.0040\n",
      "Epoch [23/200], Loss: 0.0035\n",
      "Epoch [24/200], Loss: 0.0033\n",
      "Epoch [25/200], Loss: 0.0030\n",
      "Epoch [26/200], Loss: 0.0025\n",
      "Epoch [27/200], Loss: 0.0023\n",
      "Epoch [28/200], Loss: 0.0017\n",
      "Epoch [29/200], Loss: 0.0017\n",
      "Epoch [30/200], Loss: 0.0016\n",
      "Epoch [31/200], Loss: 0.0016\n",
      "Epoch [32/200], Loss: 0.0016\n",
      "Epoch [33/200], Loss: 0.0014\n",
      "Epoch [34/200], Loss: 0.0014\n",
      "Epoch [35/200], Loss: 0.0014\n",
      "Epoch [36/200], Loss: 0.0013\n",
      "Epoch [37/200], Loss: 0.0012\n",
      "Epoch [38/200], Loss: 0.0012\n",
      "Epoch [39/200], Loss: 0.0011\n",
      "Epoch [40/200], Loss: 0.0011\n",
      "Epoch [41/200], Loss: 0.0010\n",
      "Epoch [42/200], Loss: 0.0010\n",
      "Epoch [43/200], Loss: 0.0010\n",
      "Epoch [44/200], Loss: 0.0009\n",
      "Epoch [45/200], Loss: 0.0009\n",
      "Epoch [46/200], Loss: 0.0009\n",
      "Epoch [47/200], Loss: 0.0008\n",
      "Epoch [48/200], Loss: 0.0008\n",
      "Epoch [49/200], Loss: 0.0008\n",
      "Epoch [50/200], Loss: 0.0007\n",
      "Epoch [51/200], Loss: 0.0007\n",
      "Epoch [52/200], Loss: 0.0007\n",
      "Epoch [53/200], Loss: 0.0006\n",
      "Epoch [54/200], Loss: 0.0006\n",
      "Epoch [55/200], Loss: 0.0006\n",
      "Epoch [56/200], Loss: 0.0005\n",
      "Epoch [57/200], Loss: 0.0005\n",
      "Epoch [58/200], Loss: 0.0005\n",
      "Epoch [59/200], Loss: 0.0005\n",
      "Epoch [60/200], Loss: 0.0005\n",
      "Epoch [61/200], Loss: 0.0004\n",
      "Epoch [62/200], Loss: 0.0004\n",
      "Epoch [63/200], Loss: 0.0004\n",
      "Epoch [64/200], Loss: 0.0004\n",
      "Epoch [65/200], Loss: 0.0003\n",
      "Epoch [66/200], Loss: 0.0003\n",
      "Epoch [67/200], Loss: 0.0003\n",
      "Epoch [68/200], Loss: 0.0003\n",
      "Epoch [69/200], Loss: 0.0003\n",
      "Epoch [70/200], Loss: 0.0003\n",
      "Epoch [71/200], Loss: 0.0002\n",
      "Epoch [72/200], Loss: 0.0002\n",
      "Epoch [73/200], Loss: 0.0002\n",
      "Epoch [74/200], Loss: 0.0001\n",
      "Epoch [75/200], Loss: 0.0001\n",
      "Epoch [76/200], Loss: 0.0001\n",
      "Epoch [77/200], Loss: 0.0001\n",
      "Epoch [78/200], Loss: 0.0001\n",
      "Epoch [79/200], Loss: 0.0001\n",
      "Epoch [80/200], Loss: 0.0001\n",
      "Epoch [81/200], Loss: 0.0001\n",
      "Epoch [82/200], Loss: 0.0001\n",
      "Epoch [83/200], Loss: 0.0001\n",
      "Epoch [84/200], Loss: 0.0001\n",
      "Epoch [85/200], Loss: 0.0001\n",
      "Epoch [86/200], Loss: 0.0001\n",
      "Epoch [87/200], Loss: 0.0001\n",
      "Epoch [88/200], Loss: 0.0001\n",
      "Epoch [89/200], Loss: 0.0001\n",
      "Epoch [90/200], Loss: 0.0001\n",
      "Epoch [91/200], Loss: 0.0001\n",
      "Epoch [92/200], Loss: 0.0001\n",
      "Epoch [93/200], Loss: 0.0001\n",
      "Epoch [94/200], Loss: 0.0001\n",
      "Epoch [95/200], Loss: 0.0001\n",
      "Epoch [96/200], Loss: 0.0001\n",
      "Epoch [97/200], Loss: 0.0001\n",
      "Epoch [98/200], Loss: 0.0001\n",
      "Epoch [99/200], Loss: 0.0001\n",
      "Epoch [100/200], Loss: 0.0001\n",
      "Epoch [101/200], Loss: 0.0001\n",
      "Epoch [102/200], Loss: 0.0001\n",
      "Epoch [103/200], Loss: 0.0001\n",
      "Epoch [104/200], Loss: 0.0002\n",
      "Epoch [105/200], Loss: 0.0001\n",
      "Epoch [106/200], Loss: 0.0001\n",
      "Epoch [107/200], Loss: 0.0001\n",
      "Epoch [108/200], Loss: 0.0002\n",
      "Epoch [109/200], Loss: 0.0001\n",
      "Epoch [110/200], Loss: 0.0001\n",
      "Epoch [111/200], Loss: 0.0001\n",
      "Epoch [112/200], Loss: 0.0001\n",
      "Epoch [113/200], Loss: 0.0001\n",
      "Epoch [114/200], Loss: 0.0001\n",
      "Epoch [115/200], Loss: 0.0001\n",
      "Epoch [116/200], Loss: 0.0001\n",
      "Epoch [117/200], Loss: 0.0001\n",
      "Epoch [118/200], Loss: 0.0001\n",
      "Epoch [119/200], Loss: 0.0001\n",
      "Epoch [120/200], Loss: 0.0001\n",
      "Epoch [121/200], Loss: 0.0001\n",
      "Epoch [122/200], Loss: 0.0001\n",
      "Epoch [123/200], Loss: 0.0001\n",
      "Epoch [124/200], Loss: 0.0001\n",
      "Epoch [125/200], Loss: 0.0001\n",
      "Epoch [126/200], Loss: 0.0002\n",
      "Epoch [127/200], Loss: 0.0002\n",
      "Epoch [128/200], Loss: 0.0002\n",
      "Epoch [129/200], Loss: 0.0002\n",
      "Epoch [130/200], Loss: 0.0002\n",
      "Epoch [131/200], Loss: 0.0002\n",
      "Epoch [132/200], Loss: 0.0002\n",
      "Epoch [133/200], Loss: 0.0002\n",
      "Epoch [134/200], Loss: 0.0002\n",
      "Epoch [135/200], Loss: 0.0002\n",
      "Epoch [136/200], Loss: 0.0002\n",
      "Epoch [137/200], Loss: 0.0002\n",
      "Epoch [138/200], Loss: 0.0002\n",
      "Epoch [139/200], Loss: 0.0002\n",
      "Epoch [140/200], Loss: 0.0002\n",
      "Epoch [141/200], Loss: 0.0002\n",
      "Epoch [142/200], Loss: 0.0002\n",
      "Epoch [143/200], Loss: 0.0002\n",
      "Epoch [144/200], Loss: 0.0002\n",
      "Epoch [145/200], Loss: 0.0002\n",
      "Epoch [146/200], Loss: 0.0002\n",
      "Epoch [147/200], Loss: 0.0002\n",
      "Epoch [148/200], Loss: 0.0002\n",
      "Epoch [149/200], Loss: 0.0002\n",
      "Epoch [150/200], Loss: 0.0002\n",
      "Epoch [151/200], Loss: 0.0002\n",
      "Epoch [152/200], Loss: 0.0002\n",
      "Epoch [153/200], Loss: 0.0002\n",
      "Epoch [154/200], Loss: 0.0002\n",
      "Epoch [155/200], Loss: 0.0002\n",
      "Epoch [156/200], Loss: 0.0002\n",
      "Epoch [157/200], Loss: 0.0002\n",
      "Epoch [158/200], Loss: 0.0002\n",
      "Epoch [159/200], Loss: 0.0002\n",
      "Epoch [160/200], Loss: 0.0002\n",
      "Epoch [161/200], Loss: 0.0002\n",
      "Epoch [162/200], Loss: 0.0002\n",
      "Epoch [163/200], Loss: 0.0002\n",
      "Epoch [164/200], Loss: 0.0002\n",
      "Epoch [165/200], Loss: 0.0001\n",
      "Epoch [166/200], Loss: 0.0002\n",
      "Epoch [167/200], Loss: 0.0002\n",
      "Epoch [168/200], Loss: 0.0002\n",
      "Epoch [169/200], Loss: 0.0002\n",
      "Epoch [170/200], Loss: 0.0002\n",
      "Epoch [171/200], Loss: 0.0002\n",
      "Epoch [172/200], Loss: 0.0002\n",
      "Epoch [173/200], Loss: 0.0002\n",
      "Epoch [174/200], Loss: 0.0001\n",
      "Epoch [175/200], Loss: 0.0001\n",
      "Epoch [176/200], Loss: 0.0002\n",
      "Epoch [177/200], Loss: 0.0001\n",
      "Epoch [178/200], Loss: 0.0001\n",
      "Epoch [179/200], Loss: 0.0001\n",
      "Epoch [180/200], Loss: 0.0001\n",
      "Epoch [181/200], Loss: 0.0001\n",
      "Epoch [182/200], Loss: 0.0001\n",
      "Epoch [183/200], Loss: 0.0001\n",
      "Epoch [184/200], Loss: 0.0002\n",
      "Epoch [185/200], Loss: 0.0002\n",
      "Epoch [186/200], Loss: 0.0001\n",
      "Epoch [187/200], Loss: 0.0002\n",
      "Epoch [188/200], Loss: 0.0002\n",
      "Epoch [189/200], Loss: 0.0002\n",
      "Epoch [190/200], Loss: 0.0002\n",
      "Epoch [191/200], Loss: 0.0002\n",
      "Epoch [192/200], Loss: 0.0002\n",
      "Epoch [193/200], Loss: 0.0002\n",
      "Epoch [194/200], Loss: 0.0002\n",
      "Epoch [195/200], Loss: 0.0001\n",
      "Epoch [196/200], Loss: 0.0001\n",
      "Epoch [197/200], Loss: 0.0002\n",
      "Epoch [198/200], Loss: 0.0001\n",
      "Epoch [199/200], Loss: 0.0001\n",
      "Epoch [200/200], Loss: 0.0001\n",
      "Accuracy: 79.2373%\n",
      "Epoch [1/200], Loss: 0.0019\n",
      "Epoch [2/200], Loss: 0.0011\n",
      "Epoch [3/200], Loss: 0.0009\n",
      "Epoch [4/200], Loss: 0.0010\n",
      "Epoch [5/200], Loss: 0.0010\n",
      "Epoch [6/200], Loss: 0.0009\n",
      "Epoch [7/200], Loss: 0.0012\n",
      "Epoch [8/200], Loss: 0.0016\n",
      "Epoch [9/200], Loss: 0.0018\n",
      "Epoch [10/200], Loss: 0.0022\n",
      "Epoch [11/200], Loss: 0.0027\n",
      "Epoch [12/200], Loss: 0.0030\n",
      "Epoch [13/200], Loss: 0.0037\n",
      "Epoch [14/200], Loss: 0.0039\n",
      "Epoch [15/200], Loss: 0.0042\n",
      "Epoch [16/200], Loss: 0.0043\n",
      "Epoch [17/200], Loss: 0.0042\n",
      "Epoch [18/200], Loss: 0.0041\n",
      "Epoch [19/200], Loss: 0.0039\n",
      "Epoch [20/200], Loss: 0.0039\n",
      "Epoch [21/200], Loss: 0.0040\n",
      "Epoch [22/200], Loss: 0.0040\n",
      "Epoch [23/200], Loss: 0.0041\n",
      "Epoch [24/200], Loss: 0.0039\n",
      "Epoch [25/200], Loss: 0.0038\n",
      "Epoch [26/200], Loss: 0.0037\n",
      "Epoch [27/200], Loss: 0.0037\n",
      "Epoch [28/200], Loss: 0.0036\n",
      "Epoch [29/200], Loss: 0.0034\n",
      "Epoch [30/200], Loss: 0.0033\n",
      "Epoch [31/200], Loss: 0.0028\n",
      "Epoch [32/200], Loss: 0.0025\n",
      "Epoch [33/200], Loss: 0.0023\n",
      "Epoch [34/200], Loss: 0.0021\n",
      "Epoch [35/200], Loss: 0.0020\n",
      "Epoch [36/200], Loss: 0.0018\n",
      "Epoch [37/200], Loss: 0.0015\n",
      "Epoch [38/200], Loss: 0.0013\n",
      "Epoch [39/200], Loss: 0.0010\n",
      "Epoch [40/200], Loss: 0.0007\n",
      "Epoch [41/200], Loss: 0.0005\n",
      "Epoch [42/200], Loss: 0.0003\n",
      "Epoch [43/200], Loss: 0.0003\n",
      "Epoch [44/200], Loss: 0.0002\n",
      "Epoch [45/200], Loss: 0.0002\n",
      "Epoch [46/200], Loss: 0.0002\n",
      "Epoch [47/200], Loss: 0.0002\n",
      "Epoch [48/200], Loss: 0.0002\n",
      "Epoch [49/200], Loss: 0.0002\n",
      "Epoch [50/200], Loss: 0.0002\n",
      "Epoch [51/200], Loss: 0.0002\n",
      "Epoch [52/200], Loss: 0.0002\n",
      "Epoch [53/200], Loss: 0.0002\n",
      "Epoch [54/200], Loss: 0.0002\n",
      "Epoch [55/200], Loss: 0.0002\n",
      "Epoch [56/200], Loss: 0.0002\n",
      "Epoch [57/200], Loss: 0.0002\n",
      "Epoch [58/200], Loss: 0.0002\n",
      "Epoch [59/200], Loss: 0.0001\n",
      "Epoch [60/200], Loss: 0.0001\n",
      "Epoch [61/200], Loss: 0.0001\n",
      "Epoch [62/200], Loss: 0.0001\n",
      "Epoch [63/200], Loss: 0.0001\n",
      "Epoch [64/200], Loss: 0.0001\n",
      "Epoch [65/200], Loss: 0.0001\n",
      "Epoch [66/200], Loss: 0.0001\n",
      "Epoch [67/200], Loss: 0.0001\n",
      "Epoch [68/200], Loss: 0.0001\n",
      "Epoch [69/200], Loss: 0.0001\n",
      "Epoch [70/200], Loss: 0.0001\n",
      "Epoch [71/200], Loss: 0.0001\n",
      "Epoch [72/200], Loss: 0.0002\n",
      "Epoch [73/200], Loss: 0.0002\n",
      "Epoch [74/200], Loss: 0.0002\n",
      "Epoch [75/200], Loss: 0.0002\n",
      "Epoch [76/200], Loss: 0.0002\n",
      "Epoch [77/200], Loss: 0.0002\n",
      "Epoch [78/200], Loss: 0.0002\n",
      "Epoch [79/200], Loss: 0.0002\n",
      "Epoch [80/200], Loss: 0.0002\n",
      "Epoch [81/200], Loss: 0.0002\n",
      "Epoch [82/200], Loss: 0.0003\n",
      "Epoch [83/200], Loss: 0.0003\n",
      "Epoch [84/200], Loss: 0.0003\n",
      "Epoch [85/200], Loss: 0.0003\n",
      "Epoch [86/200], Loss: 0.0003\n",
      "Epoch [87/200], Loss: 0.0004\n",
      "Epoch [88/200], Loss: 0.0004\n",
      "Epoch [89/200], Loss: 0.0004\n",
      "Epoch [90/200], Loss: 0.0004\n",
      "Epoch [91/200], Loss: 0.0004\n",
      "Epoch [92/200], Loss: 0.0004\n",
      "Epoch [93/200], Loss: 0.0004\n",
      "Epoch [94/200], Loss: 0.0004\n",
      "Epoch [95/200], Loss: 0.0005\n",
      "Epoch [96/200], Loss: 0.0005\n",
      "Epoch [97/200], Loss: 0.0006\n",
      "Epoch [98/200], Loss: 0.0006\n",
      "Epoch [99/200], Loss: 0.0006\n",
      "Epoch [100/200], Loss: 0.0007\n",
      "Epoch [101/200], Loss: 0.0008\n",
      "Epoch [102/200], Loss: 0.0009\n",
      "Epoch [103/200], Loss: 0.0010\n",
      "Epoch [104/200], Loss: 0.0010\n",
      "Epoch [105/200], Loss: 0.0010\n",
      "Epoch [106/200], Loss: 0.0011\n",
      "Epoch [107/200], Loss: 0.0011\n",
      "Epoch [108/200], Loss: 0.0012\n",
      "Epoch [109/200], Loss: 0.0012\n",
      "Epoch [110/200], Loss: 0.0012\n",
      "Epoch [111/200], Loss: 0.0011\n",
      "Epoch [112/200], Loss: 0.0011\n",
      "Epoch [113/200], Loss: 0.0011\n",
      "Epoch [114/200], Loss: 0.0012\n",
      "Epoch [115/200], Loss: 0.0013\n",
      "Epoch [116/200], Loss: 0.0013\n",
      "Epoch [117/200], Loss: 0.0013\n",
      "Epoch [118/200], Loss: 0.0012\n",
      "Epoch [119/200], Loss: 0.0011\n",
      "Epoch [120/200], Loss: 0.0011\n",
      "Epoch [121/200], Loss: 0.0011\n",
      "Epoch [122/200], Loss: 0.0010\n",
      "Epoch [123/200], Loss: 0.0010\n",
      "Epoch [124/200], Loss: 0.0010\n",
      "Epoch [125/200], Loss: 0.0010\n",
      "Epoch [126/200], Loss: 0.0010\n",
      "Epoch [127/200], Loss: 0.0010\n",
      "Epoch [128/200], Loss: 0.0011\n",
      "Epoch [129/200], Loss: 0.0011\n",
      "Epoch [130/200], Loss: 0.0010\n",
      "Epoch [131/200], Loss: 0.0011\n",
      "Epoch [132/200], Loss: 0.0010\n",
      "Epoch [133/200], Loss: 0.0010\n",
      "Epoch [134/200], Loss: 0.0010\n",
      "Epoch [135/200], Loss: 0.0010\n",
      "Epoch [136/200], Loss: 0.0010\n",
      "Epoch [137/200], Loss: 0.0010\n",
      "Epoch [138/200], Loss: 0.0010\n",
      "Epoch [139/200], Loss: 0.0010\n",
      "Epoch [140/200], Loss: 0.0009\n",
      "Epoch [141/200], Loss: 0.0009\n",
      "Epoch [142/200], Loss: 0.0008\n",
      "Epoch [143/200], Loss: 0.0007\n",
      "Epoch [144/200], Loss: 0.0008\n",
      "Epoch [145/200], Loss: 0.0007\n",
      "Epoch [146/200], Loss: 0.0008\n",
      "Epoch [147/200], Loss: 0.0008\n",
      "Epoch [148/200], Loss: 0.0008\n",
      "Epoch [149/200], Loss: 0.0008\n",
      "Epoch [150/200], Loss: 0.0008\n",
      "Epoch [151/200], Loss: 0.0008\n",
      "Epoch [152/200], Loss: 0.0009\n",
      "Epoch [153/200], Loss: 0.0009\n",
      "Epoch [154/200], Loss: 0.0010\n",
      "Epoch [155/200], Loss: 0.0010\n",
      "Epoch [156/200], Loss: 0.0011\n",
      "Epoch [157/200], Loss: 0.0011\n",
      "Epoch [158/200], Loss: 0.0011\n",
      "Epoch [159/200], Loss: 0.0010\n",
      "Epoch [160/200], Loss: 0.0009\n",
      "Epoch [161/200], Loss: 0.0009\n",
      "Epoch [162/200], Loss: 0.0009\n",
      "Epoch [163/200], Loss: 0.0009\n",
      "Epoch [164/200], Loss: 0.0009\n",
      "Epoch [165/200], Loss: 0.0010\n",
      "Epoch [166/200], Loss: 0.0009\n",
      "Epoch [167/200], Loss: 0.0009\n",
      "Epoch [168/200], Loss: 0.0009\n",
      "Epoch [169/200], Loss: 0.0010\n",
      "Epoch [170/200], Loss: 0.0009\n",
      "Epoch [171/200], Loss: 0.0009\n",
      "Epoch [172/200], Loss: 0.0008\n",
      "Epoch [173/200], Loss: 0.0008\n",
      "Epoch [174/200], Loss: 0.0009\n",
      "Epoch [175/200], Loss: 0.0010\n",
      "Epoch [176/200], Loss: 0.0011\n",
      "Epoch [177/200], Loss: 0.0011\n",
      "Epoch [178/200], Loss: 0.0011\n",
      "Epoch [179/200], Loss: 0.0010\n",
      "Epoch [180/200], Loss: 0.0009\n",
      "Epoch [181/200], Loss: 0.0008\n",
      "Epoch [182/200], Loss: 0.0008\n",
      "Epoch [183/200], Loss: 0.0007\n",
      "Epoch [184/200], Loss: 0.0007\n",
      "Epoch [185/200], Loss: 0.0007\n",
      "Epoch [186/200], Loss: 0.0006\n",
      "Epoch [187/200], Loss: 0.0006\n",
      "Epoch [188/200], Loss: 0.0006\n",
      "Epoch [189/200], Loss: 0.0006\n",
      "Epoch [190/200], Loss: 0.0005\n",
      "Epoch [191/200], Loss: 0.0005\n",
      "Epoch [192/200], Loss: 0.0005\n",
      "Epoch [193/200], Loss: 0.0005\n",
      "Epoch [194/200], Loss: 0.0005\n",
      "Epoch [195/200], Loss: 0.0004\n",
      "Epoch [196/200], Loss: 0.0004\n",
      "Epoch [197/200], Loss: 0.0004\n",
      "Epoch [198/200], Loss: 0.0003\n",
      "Epoch [199/200], Loss: 0.0003\n",
      "Epoch [200/200], Loss: 0.0003\n",
      "Accuracy: 78.8136%\n",
      "Epoch [1/200], Loss: 0.0007\n",
      "Epoch [2/200], Loss: 0.0006\n",
      "Epoch [3/200], Loss: 0.0006\n",
      "Epoch [4/200], Loss: 0.0007\n",
      "Epoch [5/200], Loss: 0.0011\n",
      "Epoch [6/200], Loss: 0.0009\n",
      "Epoch [7/200], Loss: 0.0004\n",
      "Epoch [8/200], Loss: 0.0004\n",
      "Epoch [9/200], Loss: 0.0010\n",
      "Epoch [10/200], Loss: 0.0007\n",
      "Epoch [11/200], Loss: 0.0004\n",
      "Epoch [12/200], Loss: 0.0008\n",
      "Epoch [13/200], Loss: 0.0005\n",
      "Epoch [14/200], Loss: 0.0004\n",
      "Epoch [15/200], Loss: 0.0006\n",
      "Epoch [16/200], Loss: 0.0000\n",
      "Epoch [17/200], Loss: 0.0004\n",
      "Epoch [18/200], Loss: 0.0003\n",
      "Epoch [19/200], Loss: 0.0009\n",
      "Epoch [20/200], Loss: 0.0006\n",
      "Epoch [21/200], Loss: 0.0010\n",
      "Epoch [22/200], Loss: 0.0008\n",
      "Epoch [23/200], Loss: 0.0010\n",
      "Epoch [24/200], Loss: 0.0006\n",
      "Epoch [25/200], Loss: 0.0010\n",
      "Epoch [26/200], Loss: 0.0002\n",
      "Epoch [27/200], Loss: 0.0003\n",
      "Epoch [28/200], Loss: 0.0005\n",
      "Epoch [29/200], Loss: 0.0004\n",
      "Epoch [30/200], Loss: 0.0004\n",
      "Epoch [31/200], Loss: 0.0002\n",
      "Epoch [32/200], Loss: 0.0004\n",
      "Epoch [33/200], Loss: 0.0001\n",
      "Epoch [34/200], Loss: 0.0001\n",
      "Epoch [35/200], Loss: 0.0000\n",
      "Epoch [36/200], Loss: 0.0001\n",
      "Epoch [37/200], Loss: 0.0000\n",
      "Epoch [38/200], Loss: 0.0001\n",
      "Epoch [39/200], Loss: 0.0001\n",
      "Epoch [40/200], Loss: 0.0000\n",
      "Epoch [41/200], Loss: 0.0000\n",
      "Epoch [42/200], Loss: 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 46\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m xyz \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     xyz \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(xyz)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     output, hidden_state \u001b[39m=\u001b[39m model(xyz\u001b[39m.\u001b[39;49mfloat(), hidden_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 46\u001b[0m in \u001b[0;36mSitUpDetectorSimpleRNN.forward\u001b[0;34m(self, x, hidden_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hidden_state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), hidden_state),\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     hidden \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min2hidden(combined))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min2output(combined)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X62sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# find the best hyperparameters for the model\n",
    "\n",
    "# create a list of hyperparameters to test\n",
    "import itertools\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005,0.0005, 0.0001]\n",
    "num_epochs = [200,400,800]\n",
    "hidden_sizes = [2,3,6,12, 24, 36, 48]\n",
    "num_classes = 2\n",
    "input_size = 3\n",
    "optimizers = [torch.optim.Adam]\n",
    "# create a list of all possible combinations of hyperparameters\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, num_epochs, hidden_sizes, optimizers))\n",
    "# create a list to store the accuracy of each combination of hyperparameters\n",
    "accuracy_list = []\n",
    "# loop through each combination of hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for combination in hyperparameter_combinations:\n",
    "\n",
    "    # create pytorch lstm variable recurrent classifier\n",
    "    model = SitUpDetectorSimpleRNN(input_size, num_classes, combination[2]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = combination[3](model.parameters(), lr=combination[0])\n",
    "\n",
    "    myDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(train_x_numpy_no_pad,train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=myDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    losses = []\n",
    "    loss = None\n",
    "    total_steps = len(train_loader)\n",
    "    for epoch in range(combination[1]):\n",
    "        for i, (features, target) in enumerate(train_loader):\n",
    "            hidden_state = model.init_hidden()\n",
    "            for xyz in features:\n",
    "                xyz = torch.tensor(xyz)\n",
    "                output, hidden_state = model(xyz.float(), hidden_state)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "        print(\n",
    "                f\"Epoch [{epoch + 1}/{combination[1]}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "    myTestDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(test_x_numpy_no_pad,test_y)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=myTestDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    myTestDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(test_x_numpy_no_pad,test_y)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=myTestDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = len(test_loader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (features, target) in enumerate(test_loader):\n",
    "            hidden_state = model.init_hidden()\n",
    "            for xyz in features:\n",
    "                xyz = torch.tensor(xyz)\n",
    "                output, hidden_state = model(xyz.float(), hidden_state)\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            num_correct += bool(pred == target)\n",
    "\n",
    "    print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "    accuracy_list.append(num_correct / num_samples * 100)\n",
    "\n",
    "# display the best hyperparameters\n",
    "print(f\"Best hyperparameters: {hyperparameter_combinations[accuracy_list.index(max(accuracy_list))]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: (0.001, 200, 12, <class 'torch.optim.adam.Adam'>)\n",
      "Best accuracy: 79.23728813559322\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters = hyperparameter_combinations[accuracy_list.index(max(accuracy_list))]\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "print(f\"Best accuracy: {max(accuracy_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sktime.classification.compose import ColumnEnsembleClassifier\n",
    "from sktime.classification.dictionary_based import BOSSEnsemble\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "# from sktime.classification.shapelet_based import MrSEQLClassifier\n",
    "from sktime.datasets import load_basic_motions\n",
    "from sktime.transformations.panel.compose import ColumnConcatenator\n",
    "\n",
    "\n",
    "from sktime.datatypes._panel._convert import (\n",
    "    from_3d_numpy_to_nested,\n",
    "    from_multi_index_to_3d_numpy,\n",
    "    from_nested_to_3d_numpy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_basic_motions(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "X_train.head()\n",
    "np.unique(y_train)\n",
    "\n",
    "\n",
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score\", score)\n",
    "\n",
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n",
    "        (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=5), [3]),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"score \", score)\n",
    "\n",
    "# clf = MrSEQLClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement on my data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert current dataset to sktime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "y = train_y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        col_x = []\n",
    "        col_y = []\n",
    "        col_z = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            col_x.append(col_float[0])   \n",
    "            col_y.append(col_float[1])\n",
    "            col_z.append(col_float[2]) \n",
    "        row_array.append(col_x)\n",
    "        row_array.append(col_y)\n",
    "        row_array.append(col_z)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "\n",
    "print(train_x_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multivariate_dataframe_train_x = from_3d_numpy_to_nested(train_x_numpy_x_y_z_separate_arrays)\n",
    "# multivariate_dataframe_test_x = from_3d_numpy_to_nested(multivariate_list_np_test)\n",
    "print(multivariate_dataframe_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(multivariate_dataframe_train_x, y, random_state=42)\n",
    "\n",
    "\n",
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score is\", score)\n",
    "\n",
    "\n",
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n",
    "        (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=5,n_jobs=-1), [2]),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convulational Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (conv1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rnn): LSTM(256, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the CRNN architecture\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        \n",
    "        # Define the pooling layers\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        \n",
    "        # Define the RNN\n",
    "        self.rnn = nn.LSTM(input_size=32 * 8 * 1, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolutional and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the output of the pooling layers\n",
    "        x = x.view(x.size(0), -1, 32 * 8 * 1)\n",
    "        \n",
    "        # Apply the RNN\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Take the output from the final time step of the RNN\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Apply the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create an instance of the CRNN\n",
    "model = CRNN()\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type input item <class 'list'>\n",
      "type target item <class 'numpy.int64'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv1d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 63\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Iterate over the training data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m# Feed the data into the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(output, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 63\u001b[0m in \u001b[0;36mCRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Apply the convolutional and pooling layers\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#Y114sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Flatten the output of the pooling layers\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/conv.py:301\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/conv.py:297\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    295\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    296\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 297\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    298\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mTypeError\u001b[0m: conv1d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "# Define the CRNN model\n",
    "model = CRNN()\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model on the training set\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate over the training data\n",
    "    for data, labels in train_loader:\n",
    "        # Feed the data into the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Update the model's parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, labels in val_loader:\n",
    "            # Feed the data into the model\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        # Print the validation accuracy\n",
    "        print(\"Epoch {}: Validation Accuracy = {}\".format(epoch, correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "hyperparameters = {'learning_rate': [1e-3, 1e-4],\n",
    "                    'hidden_size': [128, 256, 512],\n",
    "                    'num_layers': [1, 2, 3]}\n",
    "\n",
    "# Use grid search to find the best hyperparameters\n",
    "model = GridSearchCV(CRNN, hyperparameters, cv=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# possible why to infer on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model\n",
    "torch.jit.save(model, 'model.pt')\n",
    "\n",
    "# Load the trained model on the Android device\n",
    "model = torch.jit.load('model.pt')\n",
    "\n",
    "# Use the model to make predictions on new data\n",
    "output = model(x)\n",
    "\n",
    "# Convert the model to ONNX\n",
    "torch.onnx.export(model, x, 'model.onnx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('time-series-data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36938ddbb6c08077ca1515a91809541f7be305de7f4efe514c09144966008f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])\n",
    "    \n",
    "\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "\n",
    "# drop the collumn Y from the dataframe\n",
    "\n",
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y']\n",
    "\n",
    "# make the tran_x data rows have the same length as the sequence_length by padding with [0,0,0] for each collumn\n",
    "\n",
    "# train_x = train_x.reindex(range(sequence_length), fill_value=[0,0,0])\n",
    "\n",
    "test_x = test_df.drop(['Y'], axis=1)\n",
    "\n",
    "\n",
    "test_y = test_df['Y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3)\n",
      "[ 0.51171875 -0.10406494 -0.10406494]\n"
     ]
    }
   ],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            row_array.append(col_float)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "test_x_numpy = convert_rows_to_nupy_array(test_x)\n",
    "\n",
    "print(train_x_numpy.shape)\n",
    "print(test_x_numpy[1][5])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Padding from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-3.4326172e-01  9.3945310e-01  9.3945310e-01]\n",
      "  [-3.4326172e-01  9.3945310e-01  9.3945310e-01]\n",
      "  [-3.4326172e-01  9.3945310e-01  9.3945310e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 6.2613687e-03  2.1991149e-02  2.1991149e-02]\n",
      "  [ 1.1148291e-02  2.0769420e-02  2.0769420e-02]\n",
      "  [ 1.4813482e-02  2.3212880e-02  2.3212880e-02]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 2.2692871e-01 -3.9367676e-02 -3.9367676e-02]\n",
      "  [ 2.2692871e-01 -3.9367676e-02 -3.9367676e-02]\n",
      "  [ 2.2692871e-01 -3.9367676e-02 -3.9367676e-02]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.9016100e-03  3.0543262e-04  3.0543262e-04]\n",
      "  [ 4.5814895e-04  3.0543262e-04  3.0543262e-04]\n",
      "  [-7.6358154e-04  1.5271631e-03  1.5271631e-03]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 8.7506440e-02 -7.6205440e-01 -7.6205440e-01]\n",
      "  [ 7.0096780e-02 -7.5121150e-01 -7.5121150e-01]\n",
      "  [ 5.1618114e-02 -7.5121150e-01 -7.5121150e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 8.4910266e-02  1.6035212e-02  1.6035212e-02]\n",
      "  [ 8.0023350e-02  1.6035212e-02  1.6035212e-02]\n",
      "  [ 6.7653330e-02  1.8325957e-02  1.8325957e-02]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "train_y_no_pad = train_y\n",
    "test_y_no_pad = test_y\n",
    "\n",
    "# remove the numpy array at the axis 3 that has values of a numpy array with all zeros\n",
    "\n",
    "train_x_numpy_no_pad = np.delete(train_x_numpy, np.where(~train_x_numpy.any(axis=(0,1,2))), axis=0)\n",
    "test_x_numpy_no_pad = np.delete(test_x_numpy, np.where(~test_x_numpy.any(axis=(0,1,2))), axis=0)\n",
    "print(train_x_numpy_no_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2057, 2058]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dummy_clf \u001b[39m=\u001b[39m DummyClassifier(strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmost_frequent\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m dummy_clf\u001b[39m.\u001b[39;49mfit(train_x_numpy, train_y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_pred \u001b[39m=\u001b[39m dummy_clf\u001b[39m.\u001b[39mpredict(test_x_numpy)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(test_y, y_pred))\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sklearn/dummy.py:198\u001b[0m, in \u001b[0;36mDummyClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(y, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 198\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2057, 2058]"
     ]
    }
   ],
   "source": [
    "# Dummy Classifer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(train_x_numpy, train_y)\n",
    "y_pred = dummy_clf.predict(test_x_numpy)\n",
    "print(accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/2057], Loss: 0.1597\n",
      "Epoch [1/100], Step [200/2057], Loss: 1.3734\n",
      "Epoch [1/100], Step [300/2057], Loss: 0.2067\n",
      "Epoch [1/100], Step [400/2057], Loss: 1.2511\n",
      "Epoch [1/100], Step [500/2057], Loss: 0.2528\n",
      "Epoch [1/100], Step [600/2057], Loss: 0.2503\n",
      "Epoch [1/100], Step [700/2057], Loss: 0.2768\n",
      "Epoch [1/100], Step [800/2057], Loss: 1.2031\n",
      "Epoch [1/100], Step [900/2057], Loss: 0.2962\n",
      "Epoch [1/100], Step [1000/2057], Loss: 0.2959\n",
      "Epoch [1/100], Step [1100/2057], Loss: 0.2957\n",
      "Epoch [1/100], Step [1200/2057], Loss: 0.3633\n",
      "Epoch [1/100], Step [1300/2057], Loss: 0.3614\n",
      "Epoch [1/100], Step [1400/2057], Loss: 0.3259\n",
      "Epoch [1/100], Step [1500/2057], Loss: 0.3964\n",
      "Epoch [1/100], Step [1600/2057], Loss: 0.2617\n",
      "Epoch [1/100], Step [1700/2057], Loss: 1.3364\n",
      "Epoch [1/100], Step [1800/2057], Loss: 1.4455\n",
      "Epoch [1/100], Step [1900/2057], Loss: 1.1420\n",
      "Epoch [1/100], Step [2000/2057], Loss: 1.1775\n",
      "Epoch [2/100], Step [100/2057], Loss: 0.2431\n",
      "Epoch [2/100], Step [200/2057], Loss: 1.4006\n",
      "Epoch [2/100], Step [300/2057], Loss: 0.2951\n",
      "Epoch [2/100], Step [400/2057], Loss: 1.3390\n",
      "Epoch [2/100], Step [500/2057], Loss: 0.2692\n",
      "Epoch [2/100], Step [600/2057], Loss: 0.2915\n",
      "Epoch [2/100], Step [700/2057], Loss: 0.2635\n",
      "Epoch [2/100], Step [800/2057], Loss: 1.2131\n",
      "Epoch [2/100], Step [900/2057], Loss: 0.3005\n",
      "Epoch [2/100], Step [1000/2057], Loss: 0.2875\n",
      "Epoch [2/100], Step [1100/2057], Loss: 0.2983\n",
      "Epoch [2/100], Step [1200/2057], Loss: 0.3511\n",
      "Epoch [2/100], Step [1300/2057], Loss: 0.3437\n",
      "Epoch [2/100], Step [1400/2057], Loss: 0.3209\n",
      "Epoch [2/100], Step [1500/2057], Loss: 0.3817\n",
      "Epoch [2/100], Step [1600/2057], Loss: 0.2631\n",
      "Epoch [2/100], Step [1700/2057], Loss: 1.3485\n",
      "Epoch [2/100], Step [1800/2057], Loss: 1.4338\n",
      "Epoch [2/100], Step [1900/2057], Loss: 1.2034\n",
      "Epoch [2/100], Step [2000/2057], Loss: 1.1830\n",
      "Epoch [3/100], Step [100/2057], Loss: 0.2610\n",
      "Epoch [3/100], Step [200/2057], Loss: 1.4069\n",
      "Epoch [3/100], Step [300/2057], Loss: 0.3021\n",
      "Epoch [3/100], Step [400/2057], Loss: 1.3652\n",
      "Epoch [3/100], Step [500/2057], Loss: 0.2727\n",
      "Epoch [3/100], Step [600/2057], Loss: 0.2974\n",
      "Epoch [3/100], Step [700/2057], Loss: 0.2935\n",
      "Epoch [3/100], Step [800/2057], Loss: 1.2574\n",
      "Epoch [3/100], Step [900/2057], Loss: 0.3030\n",
      "Epoch [3/100], Step [1000/2057], Loss: 0.2842\n",
      "Epoch [3/100], Step [1100/2057], Loss: 0.2964\n",
      "Epoch [3/100], Step [1200/2057], Loss: 0.3417\n",
      "Epoch [3/100], Step [1300/2057], Loss: 0.3357\n",
      "Epoch [3/100], Step [1400/2057], Loss: 0.3188\n",
      "Epoch [3/100], Step [1500/2057], Loss: 0.3724\n",
      "Epoch [3/100], Step [1600/2057], Loss: 0.2731\n",
      "Epoch [3/100], Step [1700/2057], Loss: 1.3520\n",
      "Epoch [3/100], Step [1800/2057], Loss: 1.4301\n",
      "Epoch [3/100], Step [1900/2057], Loss: 1.2287\n",
      "Epoch [3/100], Step [2000/2057], Loss: 1.1897\n",
      "Epoch [4/100], Step [100/2057], Loss: 0.2653\n",
      "Epoch [4/100], Step [200/2057], Loss: 1.3989\n",
      "Epoch [4/100], Step [300/2057], Loss: 0.3045\n",
      "Epoch [4/100], Step [400/2057], Loss: 1.3742\n",
      "Epoch [4/100], Step [500/2057], Loss: 0.2740\n",
      "Epoch [4/100], Step [600/2057], Loss: 0.2987\n",
      "Epoch [4/100], Step [700/2057], Loss: 0.2950\n",
      "Epoch [4/100], Step [800/2057], Loss: 1.2676\n",
      "Epoch [4/100], Step [900/2057], Loss: 0.3037\n",
      "Epoch [4/100], Step [1000/2057], Loss: 0.2848\n",
      "Epoch [4/100], Step [1100/2057], Loss: 0.2957\n",
      "Epoch [4/100], Step [1200/2057], Loss: 0.3372\n",
      "Epoch [4/100], Step [1300/2057], Loss: 0.3323\n",
      "Epoch [4/100], Step [1400/2057], Loss: 0.3179\n",
      "Epoch [4/100], Step [1500/2057], Loss: 0.3675\n",
      "Epoch [4/100], Step [1600/2057], Loss: 0.2787\n",
      "Epoch [4/100], Step [1700/2057], Loss: 1.3521\n",
      "Epoch [4/100], Step [1800/2057], Loss: 1.4233\n",
      "Epoch [4/100], Step [1900/2057], Loss: 1.2474\n",
      "Epoch [4/100], Step [2000/2057], Loss: 1.1989\n",
      "Epoch [5/100], Step [100/2057], Loss: 0.2698\n",
      "Epoch [5/100], Step [200/2057], Loss: 1.3961\n",
      "Epoch [5/100], Step [300/2057], Loss: 0.3050\n",
      "Epoch [5/100], Step [400/2057], Loss: 1.3795\n",
      "Epoch [5/100], Step [500/2057], Loss: 0.2749\n",
      "Epoch [5/100], Step [600/2057], Loss: 0.2991\n",
      "Epoch [5/100], Step [700/2057], Loss: 0.2958\n",
      "Epoch [5/100], Step [800/2057], Loss: 1.2738\n",
      "Epoch [5/100], Step [900/2057], Loss: 0.3043\n",
      "Epoch [5/100], Step [1000/2057], Loss: 0.2854\n",
      "Epoch [5/100], Step [1100/2057], Loss: 0.2953\n",
      "Epoch [5/100], Step [1200/2057], Loss: 0.3343\n",
      "Epoch [5/100], Step [1300/2057], Loss: 0.3303\n",
      "Epoch [5/100], Step [1400/2057], Loss: 0.3175\n",
      "Epoch [5/100], Step [1500/2057], Loss: 0.3642\n",
      "Epoch [5/100], Step [1600/2057], Loss: 0.2813\n",
      "Epoch [5/100], Step [1700/2057], Loss: 1.3516\n",
      "Epoch [5/100], Step [1800/2057], Loss: 1.4162\n",
      "Epoch [5/100], Step [1900/2057], Loss: 1.2604\n",
      "Epoch [5/100], Step [2000/2057], Loss: 1.2052\n",
      "Epoch [6/100], Step [100/2057], Loss: 0.0483\n",
      "Epoch [6/100], Step [200/2057], Loss: 1.2576\n",
      "Epoch [6/100], Step [300/2057], Loss: 0.3145\n",
      "Epoch [6/100], Step [400/2057], Loss: 1.1414\n",
      "Epoch [6/100], Step [500/2057], Loss: 0.4129\n",
      "Epoch [6/100], Step [600/2057], Loss: 0.2871\n",
      "Epoch [6/100], Step [700/2057], Loss: 0.3280\n",
      "Epoch [6/100], Step [800/2057], Loss: 1.1279\n",
      "Epoch [6/100], Step [900/2057], Loss: 0.3092\n",
      "Epoch [6/100], Step [1000/2057], Loss: 0.2742\n",
      "Epoch [6/100], Step [1100/2057], Loss: 0.1932\n",
      "Epoch [6/100], Step [1200/2057], Loss: 0.3581\n",
      "Epoch [6/100], Step [1300/2057], Loss: 0.3561\n",
      "Epoch [6/100], Step [1400/2057], Loss: 0.3312\n",
      "Epoch [6/100], Step [1500/2057], Loss: 0.3597\n",
      "Epoch [6/100], Step [1600/2057], Loss: 0.2584\n",
      "Epoch [6/100], Step [1700/2057], Loss: 1.2758\n",
      "Epoch [6/100], Step [1800/2057], Loss: 1.3595\n",
      "Epoch [6/100], Step [1900/2057], Loss: 1.1583\n",
      "Epoch [6/100], Step [2000/2057], Loss: 1.1508\n",
      "Epoch [7/100], Step [100/2057], Loss: 0.2832\n",
      "Epoch [7/100], Step [200/2057], Loss: 1.3984\n",
      "Epoch [7/100], Step [300/2057], Loss: 0.3353\n",
      "Epoch [7/100], Step [400/2057], Loss: 1.3592\n",
      "Epoch [7/100], Step [500/2057], Loss: 0.3095\n",
      "Epoch [7/100], Step [600/2057], Loss: 0.3034\n",
      "Epoch [7/100], Step [700/2057], Loss: 0.3095\n",
      "Epoch [7/100], Step [800/2057], Loss: 1.2036\n",
      "Epoch [7/100], Step [900/2057], Loss: 0.3276\n",
      "Epoch [7/100], Step [1000/2057], Loss: 0.3001\n",
      "Epoch [7/100], Step [1100/2057], Loss: 0.2196\n",
      "Epoch [7/100], Step [1200/2057], Loss: 0.3498\n",
      "Epoch [7/100], Step [1300/2057], Loss: 0.3126\n",
      "Epoch [7/100], Step [1400/2057], Loss: 0.3286\n",
      "Epoch [7/100], Step [1500/2057], Loss: 0.3436\n",
      "Epoch [7/100], Step [1600/2057], Loss: 0.2725\n",
      "Epoch [7/100], Step [1700/2057], Loss: 1.3101\n",
      "Epoch [7/100], Step [1800/2057], Loss: 1.3721\n",
      "Epoch [7/100], Step [1900/2057], Loss: 1.2137\n",
      "Epoch [7/100], Step [2000/2057], Loss: 1.1788\n",
      "Epoch [8/100], Step [100/2057], Loss: 0.2942\n",
      "Epoch [8/100], Step [200/2057], Loss: 1.3881\n",
      "Epoch [8/100], Step [300/2057], Loss: 0.3312\n",
      "Epoch [8/100], Step [400/2057], Loss: 1.4332\n",
      "Epoch [8/100], Step [500/2057], Loss: 0.2869\n",
      "Epoch [8/100], Step [600/2057], Loss: 0.3034\n",
      "Epoch [8/100], Step [700/2057], Loss: 0.3014\n",
      "Epoch [8/100], Step [800/2057], Loss: 1.2468\n",
      "Epoch [8/100], Step [900/2057], Loss: 0.3186\n",
      "Epoch [8/100], Step [1000/2057], Loss: 0.2957\n",
      "Epoch [8/100], Step [1100/2057], Loss: 0.2523\n",
      "Epoch [8/100], Step [1200/2057], Loss: 0.3396\n",
      "Epoch [8/100], Step [1300/2057], Loss: 0.3167\n",
      "Epoch [8/100], Step [1400/2057], Loss: 0.3253\n",
      "Epoch [8/100], Step [1500/2057], Loss: 0.3426\n",
      "Epoch [8/100], Step [1600/2057], Loss: 0.2836\n",
      "Epoch [8/100], Step [1700/2057], Loss: 1.3201\n",
      "Epoch [8/100], Step [1800/2057], Loss: 1.3781\n",
      "Epoch [8/100], Step [1900/2057], Loss: 1.2394\n",
      "Epoch [8/100], Step [2000/2057], Loss: 1.1946\n",
      "Epoch [9/100], Step [100/2057], Loss: 0.2959\n",
      "Epoch [9/100], Step [200/2057], Loss: 1.3890\n",
      "Epoch [9/100], Step [300/2057], Loss: 0.3167\n",
      "Epoch [9/100], Step [400/2057], Loss: 1.4319\n",
      "Epoch [9/100], Step [500/2057], Loss: 0.2739\n",
      "Epoch [9/100], Step [600/2057], Loss: 0.2979\n",
      "Epoch [9/100], Step [700/2057], Loss: 0.2995\n",
      "Epoch [9/100], Step [800/2057], Loss: 1.2644\n",
      "Epoch [9/100], Step [900/2057], Loss: 0.3154\n",
      "Epoch [9/100], Step [1000/2057], Loss: 0.2945\n",
      "Epoch [9/100], Step [1100/2057], Loss: 0.2858\n",
      "Epoch [9/100], Step [1200/2057], Loss: 0.3301\n",
      "Epoch [9/100], Step [1300/2057], Loss: 0.3085\n",
      "Epoch [9/100], Step [1400/2057], Loss: 0.3222\n",
      "Epoch [9/100], Step [1500/2057], Loss: 0.3388\n",
      "Epoch [9/100], Step [1600/2057], Loss: 0.2794\n",
      "Epoch [9/100], Step [1700/2057], Loss: nan\n",
      "Epoch [9/100], Step [1800/2057], Loss: nan\n",
      "Epoch [9/100], Step [1900/2057], Loss: nan\n",
      "Epoch [9/100], Step [2000/2057], Loss: nan\n",
      "Epoch [10/100], Step [100/2057], Loss: nan\n",
      "Epoch [10/100], Step [200/2057], Loss: nan\n",
      "Epoch [10/100], Step [300/2057], Loss: nan\n",
      "Epoch [10/100], Step [400/2057], Loss: nan\n",
      "Epoch [10/100], Step [500/2057], Loss: nan\n",
      "Epoch [10/100], Step [600/2057], Loss: nan\n",
      "Epoch [10/100], Step [700/2057], Loss: nan\n",
      "Epoch [10/100], Step [800/2057], Loss: nan\n",
      "Epoch [10/100], Step [900/2057], Loss: nan\n",
      "Epoch [10/100], Step [1000/2057], Loss: nan\n",
      "Epoch [10/100], Step [1100/2057], Loss: nan\n",
      "Epoch [10/100], Step [1200/2057], Loss: nan\n",
      "Epoch [10/100], Step [1300/2057], Loss: nan\n",
      "Epoch [10/100], Step [1400/2057], Loss: nan\n",
      "Epoch [10/100], Step [1500/2057], Loss: nan\n",
      "Epoch [10/100], Step [1600/2057], Loss: nan\n",
      "Epoch [10/100], Step [1700/2057], Loss: nan\n",
      "Epoch [10/100], Step [1800/2057], Loss: nan\n",
      "Epoch [10/100], Step [1900/2057], Loss: nan\n",
      "Epoch [10/100], Step [2000/2057], Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#W6sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#W6sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]\n",
    "\n",
    "# test_input = np.arange(1,8).reshape(-1,1)\n",
    "# input = torch.tensor(test_input, dtype=torch.float)\n",
    "\n",
    "# dataset = MyDataset(input, 3)\n",
    "\n",
    "# dl = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "# for inp, label in dl:\n",
    "#     print(\"inp shape\",inp.shape)\n",
    "#     print(\"inp\",inp.numpy())\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "input_size = 3\n",
    "num_classes = 2\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "class SitUpDetector(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers):\n",
    "        super(SitUpDetector, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = SitUpDetector(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=1)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        features = features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_steps, loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# test the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_x)\n",
    "    print(output)\n",
    "    print(test_y)\n",
    "\n",
    "\n",
    "# save the model\n",
    "\n",
    "torch.save(model.state_dict(), 'sit_up_detector.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with no Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new Pytorch LSTM model that takes a variable input\n",
    "\n",
    "class SitUpDetectorVariable(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers):\n",
    "        super(SitUpDetectorVariable, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# train the model\n",
    "\n",
    "model = SitUpDetectorVariable(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy_no_pad,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=1)\n",
    "\n",
    "num_epochs = 5\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        features = features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_steps, loss.item()))\n",
    "                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('time-series-data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36938ddbb6c08077ca1515a91809541f7be305de7f4efe514c09144966008f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

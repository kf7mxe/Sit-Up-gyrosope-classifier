{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torcheval.metrics.functional import binary_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])\n",
    "    \n",
    "\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "\n",
    "# drop the collumn Y from the dataframe\n",
    "\n",
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "\n",
    "# make the tran_x data rows have the same length as the sequence_length by padding with [0,0,0] for each collumn\n",
    "\n",
    "# train_x = train_x.reindex(range(sequence_length), fill_value=[0,0,0])\n",
    "\n",
    "test_x = test_df.drop(['Y'], axis=1)\n",
    "\n",
    "\n",
    "test_y = test_df['Y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3)\n",
      "[ 0.51171875 -0.10406494 -0.10406494]\n"
     ]
    }
   ],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            row_array.append(col_float)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "test_x_numpy = convert_rows_to_nupy_array(test_x)\n",
    "\n",
    "print(train_x_numpy.shape)\n",
    "print(test_x_numpy[1][5])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Padding from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13502/3299080516.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n"
     ]
    }
   ],
   "source": [
    "train_y_no_pad = train_y\n",
    "test_y_no_pad = test_y\n",
    "\n",
    "# remove the numpy array where you hava a [0,0,0] array\n",
    "\n",
    "test_np_array = np.array([[1,2,1], [0,0,0], [1,2,1]])\n",
    "\n",
    "# remove the [0,0,0] array from the numpy array test_np_array\n",
    "\n",
    "test_np_array = test_np_array[test_np_array[:,0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "train_x_numpy_no_pad = []\n",
    "# row_index = 0\n",
    "for row in range(len(train_x_numpy)):\n",
    "    temp = train_x_numpy[row][train_x_numpy[row][:,0] != 0]\n",
    "    temp_list = []\n",
    "    for col in temp:\n",
    "        item_list = []\n",
    "        for item in col:\n",
    "            item_list.append(item)\n",
    "        temp_list.append(item_list)\n",
    "    train_x_numpy_no_pad.append(temp_list)\n",
    "train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n",
    "print(train_x_numpy_no_pad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3) (2057,)\n",
      "0.7704280155642024\n"
     ]
    }
   ],
   "source": [
    "# Dummy Classifer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "print(train_x_numpy.shape, train_y.shape)\n",
    "dummy_clf.fit(train_x_numpy, train_y)\n",
    "y_pred = dummy_clf.predict(test_x_numpy)\n",
    "print(accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1513\n",
      "1     544\n",
      "Name: Y, dtype: int64\n",
      "Epoch: 50/2... Step: 49... Loss: 0.100714... Val Loss: 0.702114\n",
      "Validation loss decreased (inf --> 0.702114).  Saving model ...\n",
      "Epoch: 100/2... Step: 99... Loss: 2.846028... Val Loss: 0.792848\n",
      "Epoch: 150/2... Step: 149... Loss: 1.358052... Val Loss: 0.578241\n",
      "Validation loss decreased (0.702114 --> 0.578241).  Saving model ...\n",
      "Epoch: 200/2... Step: 199... Loss: 1.310449... Val Loss: 0.580937\n",
      "Epoch: 250/2... Step: 249... Loss: 0.182626... Val Loss: 0.577685\n",
      "Validation loss decreased (0.578241 --> 0.577685).  Saving model ...\n",
      "Epoch: 300/2... Step: 299... Loss: 0.141326... Val Loss: 0.668930\n",
      "Epoch: 350/2... Step: 349... Loss: 0.203027... Val Loss: 0.578805\n",
      "Epoch: 400/2... Step: 399... Loss: 0.370640... Val Loss: 0.581418\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return  self.input.shape[0]\n",
    "\n",
    "# test_input = np.arange(1,8).reshape(-1,1)\n",
    "# input = torch.tensor(test_input, dtype=torch.float)\n",
    "\n",
    "# dataset = MyDataset(input, 3)\n",
    "\n",
    "# dl = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "# for inp, label in dl:\n",
    "#     print(\"inp shape\",inp.shape)\n",
    "#     print(\"inp\",inp.numpy())\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "input_size = 3\n",
    "num_classes = 1\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "\n",
    "class SitUpDetector(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers,drop_prob=0.5):\n",
    "        super(SitUpDetector, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, _ = self.lstm(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:, -1]\n",
    "        return out,hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device)) \n",
    "        return hidden\n",
    "    \n",
    "\n",
    "\n",
    "model = SitUpDetector(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# reduce size of train_x_numpy_no_pad to 1000\n",
    "# train_x_numpy = train_x_numpy[:50]\n",
    "# train_y = train_y[:50]\n",
    "\n",
    "# test_x_numpy = test_x_numpy[:50]\n",
    "# test_y = test_y[:50]\n",
    "\n",
    "\n",
    "# print how many 1 and 0 in the train_y\n",
    "print(train_y.value_counts())\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=batch_size)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "num_epochs = 2\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        # features = features.float()\n",
    "\n",
    "        features, target = features.to(device), target.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs, h = model(features.float(), h)\n",
    "        loss = criterion(outputs, target.float())\n",
    "        losses.append(loss)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 50 == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in train_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp.float(), val_h)\n",
    "                val_loss = criterion(out, lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "                  \"Step: {}...\".format(i),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "\n",
    "# graph the losses \n",
    "\n",
    "# get the lenght of the losses array\n",
    "\n",
    "# create an array of the same length as the losses array with index values\n",
    "\n",
    "y = np.array(losses)\n",
    "print(\"y shpae\")\n",
    "print(y)\n",
    "\n",
    "x = np.arange(0,num_epochs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cm = confusion_matrix(test_y, all_predictions)\n",
    "# print(cm)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "torch.save(model.state_dict(), 'sit_up_detector.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test y 0      1\n",
      "3      1\n",
      "7      1\n",
      "21     1\n",
      "24     1\n",
      "25     1\n",
      "26     1\n",
      "63     1\n",
      "67     1\n",
      "84     1\n",
      "86     1\n",
      "91     1\n",
      "93     1\n",
      "94     1\n",
      "99     1\n",
      "100    1\n",
      "112    1\n",
      "126    1\n",
      "130    1\n",
      "136    1\n",
      "146    1\n",
      "151    1\n",
      "153    1\n",
      "160    1\n",
      "163    1\n",
      "164    1\n",
      "166    1\n",
      "168    1\n",
      "174    1\n",
      "176    1\n",
      "180    1\n",
      "197    1\n",
      "199    1\n",
      "201    1\n",
      "207    1\n",
      "209    1\n",
      "216    1\n",
      "221    1\n",
      "237    1\n",
      "246    1\n",
      "256    1\n",
      "257    1\n",
      "273    1\n",
      "274    1\n",
      "275    1\n",
      "277    1\n",
      "280    1\n",
      "281    1\n",
      "282    1\n",
      "291    1\n",
      "Name: Y, dtype: int64\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([1])\n",
      "labels\n",
      "tensor([1])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "label\n",
      "tensor([0])\n",
      "labels\n",
      "tensor([0])\n",
      "Test loss: 0.480\n",
      "Test accuracy: 82.000%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy\n",
    "test_dataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1)\n",
    "n_correct = 0\n",
    "n_samples = 0\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for inputs, label in test_dataloader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, label = inputs.to(device), label.to(device)\n",
    "    output, h = model(inputs.float(), h)\n",
    "    test_loss = criterion(output, label.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(label.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_dataloader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with no Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDatasetVariableSequenceLength(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([749, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (features, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# convert Double tensor to Float tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, target\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 13\u001b[0m in \u001b[0;36mSitUpDetectorVariableSequenceLength.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m s, n \u001b[39m=\u001b[39m xx\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(xx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(xx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(s, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m outputs\u001b[39m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:689\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 689\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    690\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    692\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:632\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    628\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    629\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    630\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    631\u001b[0m                        ):\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    634\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    635\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    636\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:201\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    199\u001b[0m expected_input_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m3\u001b[39m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m expected_input_dim:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    203\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    205\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "#  create pytorch lstm variable recurrent classifier\n",
    "\n",
    "input_size = 3\n",
    "num_classes = 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "class SitUpDetectorVariableSequenceLength(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers):\n",
    "        super(SitUpDetectorVariableSequenceLength, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        \n",
    "        # x = torch.squeeze(x) # remove the batch dimension maybe sometime allow for batch size > 1\n",
    "        \n",
    "        for xx in x:\n",
    "            s, n = xx.shape\n",
    "            print(xx.shape)\n",
    "            out, _ = self.lstm(xx)\n",
    "            out = out.view(s, -1)\n",
    "            outputs.append(out)\n",
    "\n",
    "        outputs = torch.stack(outputs)\n",
    "\n",
    "        out = self.fc(outputs)\n",
    "        return out\n",
    "\n",
    "# train SitUpDetectorVariableInput\n",
    "model = SitUpDetectorVariableSequenceLength(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "myDatasetVariableSequenceLengthDataset = MyDatasetVariableSequenceLength(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "num_epochs = 1\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        features = features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "        \n",
    "        loss = criterion(outputs, target.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_steps, loss.item()))\n",
    "    losses.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate time series classification with sktime \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sktime.classification.compose import ColumnEnsembleClassifier\n",
    "from sktime.classification.dictionary_based import BOSSEnsemble\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.shapelet_based import MrSEQLClassifier\n",
    "from sktime.datasets import load_basic_motions\n",
    "from sktime.transformations.panel.compose import ColumnConcatenator\n",
    "\n",
    "\n",
    "X, y = load_basic_motions(return_X_y=True)\n",
    "\n",
    "print(\"multivariate time series classification with sktime\")\n",
    "print(\"X shape\", X.shape)\n",
    "print(X.shape)\n",
    "\n",
    "print(\"y shape\", y.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('time-series-data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36938ddbb6c08077ca1515a91809541f7be305de7f4efe514c09144966008f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

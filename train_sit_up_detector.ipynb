{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing/ setting up data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from tsv and get the sequence length to use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance the positive and negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "train_df_sit_ups = train_df[train_df['Y'] == 1]\n",
    "train_df_non_sit_ups = train_df[train_df['Y'] == 0]\n",
    "\n",
    "print(len(train_df_sit_ups))\n",
    "print(len(train_df_non_sit_ups))\n",
    "\n",
    "train_df_non_sit_ups_balanced = train_df_non_sit_ups.sample(n=len(train_df_sit_ups), random_state=0)\n",
    "\n",
    "\n",
    "train_df = pd.concat([train_df_sit_ups, train_df_non_sit_ups_balanced])\n",
    "\n",
    "test_df_sit_ups = test_df[test_df['Y'] == 1]\n",
    "test_df_non_sit_ups = test_df[test_df['Y'] == 0]\n",
    "\n",
    "test_df_non_sit_ups = test_df_non_sit_ups.sample(n=len(test_df_sit_ups), random_state=0)\n",
    "\n",
    "test_df_balanced = pd.concat([test_df_sit_ups, test_df_non_sit_ups])\n",
    "test_df = test_df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate x and y from test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "\n",
    "test_x = test_df.drop(['Y'], axis=1)\n",
    "\n",
    "\n",
    "test_y = test_df['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the cells in each collumn of the dataframe from string to a numpy array and convert the whole dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            row_array.append(col_float)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "test_x_numpy = convert_rows_to_nupy_array(test_x)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Padding from numpy and create separate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5050/2768496912.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1015,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5050/2768496912.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_x_numpy_no_pad = np.array(test_x_numpy_no_pad)\n"
     ]
    }
   ],
   "source": [
    "train_y_no_pad = train_y\n",
    "test_y_no_pad = test_y\n",
    "\n",
    "# remove the numpy array where you hava a [0,0,0] array\n",
    "\n",
    "test_np_array = np.array([[1,2,1], [0,0,0], [1,2,1]])\n",
    "\n",
    "# remove the [0,0,0] array from the numpy array test_np_array\n",
    "\n",
    "test_np_array = test_np_array[test_np_array[:,0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "train_x_numpy_no_pad = []\n",
    "# row_index = 0\n",
    "for row in range(len(train_x_numpy)):\n",
    "    temp = train_x_numpy[row][train_x_numpy[row][:,0] != 0]\n",
    "    temp_list = []\n",
    "    for col in temp:\n",
    "        item_list = []\n",
    "        for item in col:\n",
    "            item_list.append(item)\n",
    "        temp_list.append(item_list)\n",
    "    train_x_numpy_no_pad.append(temp_list)\n",
    "train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n",
    "\n",
    "test_x_numpy_no_pad = []\n",
    "# row_index = 0\n",
    "for row in range(len(test_x_numpy)):\n",
    "    temp = test_x_numpy[row][test_x_numpy[row][:,0] != 0]\n",
    "    temp_list = []\n",
    "    for col in temp:\n",
    "        item_list = []\n",
    "        for item in col:\n",
    "            item_list.append(item)\n",
    "        temp_list.append(item_list)\n",
    "    test_x_numpy_no_pad.append(temp_list)\n",
    "test_x_numpy_no_pad = np.array(test_x_numpy_no_pad)\n",
    "print(train_x_numpy_no_pad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1015, 2121, 3) (1015,)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Dummy Classifer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "print(train_x_numpy.shape, train_y.shape)\n",
    "dummy_clf.fit(train_x_numpy, train_y)\n",
    "y_pred = dummy_clf.predict(test_x_numpy)\n",
    "print(accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return  self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVariableLengthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3                                                                           \n",
    "num_classes = 1\n",
    "hidden_size = 3\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 1\n",
    "num_epochs = 5\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing reduce to certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    508\n",
      "1    507\n",
      "Name: Y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# reduce size of train_x_numpy_no_pad to 1000\n",
    "# train_x_numpy = train_x_numpy[:50]\n",
    "# train_y = train_y[:50]\n",
    "\n",
    "# test_x_numpy = test_x_numpy[:50]\n",
    "# test_y = test_y[:50]\n",
    "\n",
    "\n",
    "# print how many 1 and 0 in the train_y\n",
    "print(train_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Network Archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitUpDetector(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers,drop_prob=0.5):\n",
    "        super(SitUpDetector, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, _ = self.lstm(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:, -1]\n",
    "        return out,hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device)) \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Optimizers, dataloader and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SitUpDetector(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=batch_size)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "valid_loss_min = np.Inf\n",
    "total_steps = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 49... Loss: 0.544162... Val Loss: 0.707734\n",
      "losses as np array [0.54416203]\n",
      "Validation loss decreased (inf --> 0.707734).  Saving model ...\n",
      "Epoch: 1/5... Step: 99... Loss: 0.514646... Val Loss: 0.716325\n",
      "losses as np array [0.54416203 0.51464558]\n",
      "Epoch: 1/5... Step: 149... Loss: 0.483305... Val Loss: 0.730910\n",
      "losses as np array [0.54416203 0.51464558 0.48330525]\n",
      "Epoch: 1/5... Step: 199... Loss: 0.429529... Val Loss: 0.760231\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949]\n",
      "Epoch: 1/5... Step: 249... Loss: 0.454424... Val Loss: 0.810252\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405]\n",
      "Epoch: 1/5... Step: 299... Loss: 0.441025... Val Loss: 0.870960\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532]\n",
      "Epoch: 1/5... Step: 349... Loss: 0.249727... Val Loss: 0.936649\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704]\n",
      "Epoch: 1/5... Step: 399... Loss: 0.214497... Val Loss: 1.002080\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497  ]\n",
      "Epoch: 1/5... Step: 449... Loss: 0.311233... Val Loss: 1.060251\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292]\n",
      "Epoch: 1/5... Step: 499... Loss: 0.402510... Val Loss: 1.111853\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099 ]\n",
      "Epoch: 1/5... Step: 549... Loss: 1.656819... Val Loss: 0.974484\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875]\n",
      "Epoch: 1/5... Step: 599... Loss: 1.050012... Val Loss: 0.791879\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211]\n",
      "Epoch: 1/5... Step: 649... Loss: 0.899985... Val Loss: 0.713172\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849 ]\n",
      "Epoch: 1/5... Step: 699... Loss: 0.916617... Val Loss: 0.701397\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733]\n",
      "Validation loss decreased (0.707734 --> 0.701397).  Saving model ...\n",
      "Epoch: 1/5... Step: 749... Loss: 0.768387... Val Loss: 0.695479\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666]\n",
      "Validation loss decreased (0.701397 --> 0.695479).  Saving model ...\n",
      "Epoch: 1/5... Step: 799... Loss: 0.692982... Val Loss: 0.693164\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816 ]\n",
      "Validation loss decreased (0.695479 --> 0.693164).  Saving model ...\n",
      "Epoch: 1/5... Step: 849... Loss: 0.820310... Val Loss: 0.695049\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035]\n",
      "Epoch: 1/5... Step: 899... Loss: 0.794664... Val Loss: 0.701822\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414]\n",
      "Epoch: 1/5... Step: 949... Loss: 0.292238... Val Loss: 0.712270\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758]\n",
      "Epoch: 1/5... Step: 999... Loss: 0.529786... Val Loss: 0.726434\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593]\n",
      "Epoch: 2/5... Step: 49... Loss: 0.949829... Val Loss: 0.711839\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922]\n",
      "Epoch: 2/5... Step: 99... Loss: 0.704353... Val Loss: 0.699362\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274]\n",
      "Epoch: 2/5... Step: 149... Loss: 0.699671... Val Loss: 0.694167\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067]\n",
      "Epoch: 2/5... Step: 199... Loss: 0.701298... Val Loss: 0.693425\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842]\n",
      "Epoch: 2/5... Step: 249... Loss: 0.628511... Val Loss: 0.696518\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059]\n",
      "Epoch: 2/5... Step: 299... Loss: 0.509513... Val Loss: 0.703214\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266]\n",
      "Epoch: 2/5... Step: 349... Loss: 0.525000... Val Loss: 0.713857\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992]\n",
      "Epoch: 2/5... Step: 399... Loss: 0.483367... Val Loss: 0.738512\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672 ]\n",
      "Epoch: 2/5... Step: 449... Loss: 0.469756... Val Loss: 0.779513\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558 ]\n",
      "Epoch: 2/5... Step: 499... Loss: 0.238890... Val Loss: 0.828636\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899 ]\n",
      "Epoch: 2/5... Step: 549... Loss: 1.031717... Val Loss: 0.760760\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742]\n",
      "Epoch: 2/5... Step: 599... Loss: 0.949113... Val Loss: 0.722258\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289]\n",
      "Epoch: 2/5... Step: 649... Loss: 0.840658... Val Loss: 0.708133\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819]\n",
      "Epoch: 2/5... Step: 699... Loss: 0.773800... Val Loss: 0.700107\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996]\n",
      "Epoch: 2/5... Step: 749... Loss: 0.832772... Val Loss: 0.695579\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243]\n",
      "Epoch: 2/5... Step: 799... Loss: 0.706488... Val Loss: 0.693396\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754]\n",
      "Epoch: 2/5... Step: 849... Loss: 0.671166... Val Loss: 0.693518\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565]\n",
      "Epoch: 2/5... Step: 899... Loss: 0.631075... Val Loss: 0.696414\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514]\n",
      "Epoch: 2/5... Step: 949... Loss: 0.703954... Val Loss: 0.703691\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422]\n",
      "Epoch: 2/5... Step: 999... Loss: 0.308407... Val Loss: 0.715376\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749]\n",
      "Epoch: 3/5... Step: 49... Loss: 0.872886... Val Loss: 0.706289\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636]\n",
      "Epoch: 3/5... Step: 99... Loss: 0.991221... Val Loss: 0.696466\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065]\n",
      "Epoch: 3/5... Step: 149... Loss: 0.723036... Val Loss: 0.693390\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623]\n",
      "Epoch: 3/5... Step: 199... Loss: 0.691140... Val Loss: 0.693596\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029]\n",
      "Epoch: 3/5... Step: 249... Loss: 0.667509... Val Loss: 0.695510\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878]\n",
      "Epoch: 3/5... Step: 299... Loss: 0.609466... Val Loss: 0.698843\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643]\n",
      "Epoch: 3/5... Step: 349... Loss: 0.523270... Val Loss: 0.703504\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007]\n",
      "Epoch: 3/5... Step: 399... Loss: 0.562383... Val Loss: 0.709564\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258]\n",
      "Epoch: 3/5... Step: 449... Loss: 0.503091... Val Loss: 0.716716\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908 ]\n",
      "Epoch: 3/5... Step: 499... Loss: 0.480133... Val Loss: 0.726258\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261]\n",
      "Epoch: 3/5... Step: 549... Loss: 0.937899... Val Loss: 0.718507\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935]\n",
      "Epoch: 3/5... Step: 599... Loss: 0.902030... Val Loss: 0.708189\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987]\n",
      "Epoch: 3/5... Step: 649... Loss: 0.783881... Val Loss: 0.701596\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053]\n",
      "Epoch: 3/5... Step: 699... Loss: 0.774476... Val Loss: 0.696787\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551]\n",
      "Epoch: 3/5... Step: 749... Loss: 0.693332... Val Loss: 0.693833\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196]\n",
      "Epoch: 3/5... Step: 799... Loss: 0.697621... Val Loss: 0.693217\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075]\n",
      "Epoch: 3/5... Step: 849... Loss: 0.562221... Val Loss: 0.695418\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129]\n",
      "Epoch: 3/5... Step: 899... Loss: 0.563812... Val Loss: 0.701199\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178]\n",
      "Epoch: 3/5... Step: 949... Loss: 0.517402... Val Loss: 0.712452\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211]\n",
      "Epoch: 3/5... Step: 999... Loss: 0.687691... Val Loss: 0.732016\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133]\n",
      "Epoch: 4/5... Step: 49... Loss: 1.071634... Val Loss: 0.715861\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429]\n",
      "Epoch: 4/5... Step: 99... Loss: 1.008405... Val Loss: 0.698463\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461]\n",
      "Epoch: 4/5... Step: 149... Loss: 0.629627... Val Loss: 0.693874\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657]\n",
      "Epoch: 4/5... Step: 199... Loss: 0.717997... Val Loss: 0.693302\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737]\n",
      "Epoch: 4/5... Step: 249... Loss: 0.619983... Val Loss: 0.695127\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349]\n",
      "Epoch: 4/5... Step: 299... Loss: 0.555347... Val Loss: 0.698451\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667]\n",
      "Epoch: 4/5... Step: 349... Loss: 0.545835... Val Loss: 0.703226\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532]\n",
      "Epoch: 4/5... Step: 399... Loss: 0.528011... Val Loss: 0.710005\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067]\n",
      "Epoch: 4/5... Step: 449... Loss: 0.507829... Val Loss: 0.718111\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937]\n",
      "Epoch: 4/5... Step: 499... Loss: 0.476118... Val Loss: 0.729716\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177 ]\n",
      "Epoch: 4/5... Step: 549... Loss: 0.922303... Val Loss: 0.720236\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266]\n",
      "Epoch: 4/5... Step: 599... Loss: 0.910840... Val Loss: 0.708849\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039]\n",
      "Epoch: 4/5... Step: 649... Loss: 0.818485... Val Loss: 0.701379\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846 ]\n",
      "Epoch: 4/5... Step: 699... Loss: 0.766405... Val Loss: 0.696694\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511]\n",
      "Epoch: 4/5... Step: 749... Loss: 0.715344... Val Loss: 0.693873\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377]\n",
      "Epoch: 4/5... Step: 799... Loss: 0.651863... Val Loss: 0.693216\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262]\n",
      "Epoch: 4/5... Step: 849... Loss: 0.738157... Val Loss: 0.695275\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691]\n",
      "Epoch: 4/5... Step: 899... Loss: 0.509194... Val Loss: 0.700798\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378]\n",
      "Epoch: 4/5... Step: 949... Loss: 0.455470... Val Loss: 0.710280\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017]\n",
      "Epoch: 4/5... Step: 999... Loss: 0.418895... Val Loss: 0.724429\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545]\n",
      "Epoch: 5/5... Step: 49... Loss: 0.701029... Val Loss: 0.707885\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293 ]\n",
      "Epoch: 5/5... Step: 99... Loss: 0.681696... Val Loss: 0.697193\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618]\n",
      "Epoch: 5/5... Step: 149... Loss: 0.787065... Val Loss: 0.693662\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461]\n",
      "Epoch: 5/5... Step: 199... Loss: 0.698194... Val Loss: 0.693346\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438]\n",
      "Epoch: 5/5... Step: 249... Loss: 0.621542... Val Loss: 0.695329\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245]\n",
      "Epoch: 5/5... Step: 299... Loss: 0.618129... Val Loss: 0.698591\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913]\n",
      "Epoch: 5/5... Step: 349... Loss: 0.550458... Val Loss: 0.703462\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843]\n",
      "Epoch: 5/5... Step: 399... Loss: 0.544357... Val Loss: 0.709872\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658]\n",
      "Epoch: 5/5... Step: 449... Loss: 0.461369... Val Loss: 0.718933\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859]\n",
      "Epoch: 5/5... Step: 499... Loss: 0.502714... Val Loss: 0.735977\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392]\n",
      "Epoch: 5/5... Step: 549... Loss: 0.972801... Val Loss: 0.719883\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079]\n",
      "Epoch: 5/5... Step: 599... Loss: 0.878655... Val Loss: 0.707570\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508]\n",
      "Epoch: 5/5... Step: 649... Loss: 0.838067... Val Loss: 0.701235\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667 ]\n",
      "Epoch: 5/5... Step: 699... Loss: 0.797976... Val Loss: 0.697036\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667  0.79797643]\n",
      "Epoch: 5/5... Step: 749... Loss: 0.719630... Val Loss: 0.694372\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667  0.79797643 0.71963024]\n",
      "Epoch: 5/5... Step: 799... Loss: 0.736683... Val Loss: 0.693171\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667  0.79797643 0.71963024 0.73668271]\n",
      "Epoch: 5/5... Step: 849... Loss: 0.557876... Val Loss: 0.694117\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667  0.79797643 0.71963024 0.73668271\n",
      " 0.55787623]\n",
      "Epoch: 5/5... Step: 899... Loss: 0.678248... Val Loss: 0.697759\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667  0.79797643 0.71963024 0.73668271\n",
      " 0.55787623 0.67824799]\n",
      "Epoch: 5/5... Step: 949... Loss: 0.570879... Val Loss: 0.705369\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667  0.79797643 0.71963024 0.73668271\n",
      " 0.55787623 0.67824799 0.57087946]\n",
      "Epoch: 5/5... Step: 999... Loss: 0.496995... Val Loss: 0.719559\n",
      "losses as np array [0.54416203 0.51464558 0.48330525 0.42952949 0.45442405 0.44102532\n",
      " 0.24972704 0.214497   0.31123292 0.4025099  1.65681875 1.05001211\n",
      " 0.8999849  0.91661733 0.76838666 0.6929816  0.82031035 0.79466414\n",
      " 0.29223758 0.52978593 0.94982922 0.70435274 0.69967067 0.70129842\n",
      " 0.62851059 0.50951266 0.52499992 0.4833672  0.4697558  0.2388899\n",
      " 1.03171742 0.94911289 0.84065819 0.77379996 0.83277243 0.70648754\n",
      " 0.67116565 0.63107514 0.70395422 0.30840749 0.87288636 0.99122065\n",
      " 0.72303623 0.69114029 0.66750878 0.60946643 0.52327007 0.56238258\n",
      " 0.5030908  0.48013261 0.93789935 0.90202987 0.78388053 0.77447551\n",
      " 0.69333196 0.69762075 0.56222129 0.56381178 0.51740211 0.68769133\n",
      " 1.07163429 1.00840461 0.62962657 0.71799737 0.61998349 0.55534667\n",
      " 0.54583532 0.52801067 0.50782937 0.4761177  0.92230266 0.91084039\n",
      " 0.8184846  0.76640511 0.71534377 0.65186262 0.73815691 0.50919378\n",
      " 0.45547017 0.41889545 0.7010293  0.68169618 0.78706461 0.69819438\n",
      " 0.62154245 0.61812913 0.55045843 0.54435658 0.46136859 0.50271392\n",
      " 0.97280079 0.87865508 0.8380667  0.79797643 0.71963024 0.73668271\n",
      " 0.55787623 0.67824799 0.57087946 0.49699485]\n",
      "plt y shpae loss (100,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "        features, target = features.to(device), target.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs, h = model(features.float(), h)\n",
    "        loss = criterion(outputs, target.float())\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 50 == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in train_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp.float(), val_h)\n",
    "                val_loss = criterion(out, lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "                  \"Step: {}...\".format(i),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            losses.append(loss.item())\n",
    "            # print(\"why\",\"{:.6f}\".format(loss.item().detach().numpy()))\n",
    "\n",
    "\n",
    "            print(\"losses as np array\",np.array(losses))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "\n",
    "# graph the losses \n",
    "\n",
    "# get the lenght of the losses array\n",
    "\n",
    "# create an array of the same length as the losses array with index values\n",
    "for i in range(len(losses)):\n",
    "    if type(losses[i]) == torch.Tensor:\n",
    "        losses[i] = losses[i].detach().numpy()\n",
    "\n",
    "y = np.array(losses)\n",
    "print(\"plt y shpae loss\", y.shape)\n",
    "# print(\"y shpae\")\n",
    "# print(y)\n",
    "\n",
    "x = np.arange(0,len(losses))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot()\n",
    "plt.savefig(f\"loss-for-params-num-{num_layers}-learning_rate{learning_rate}.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'sit_up_detector.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.726\n",
      "Test accuracy: 50.049%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy\n",
    "test_dataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1)\n",
    "n_correct = 0\n",
    "n_samples = 0\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for inputs, label in train_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, label = inputs.to(device), label.to(device)\n",
    "    output, h = model(inputs.float(), h)\n",
    "    test_loss = criterion(output, label.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(label.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_dataloader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with different network with simpler rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVariableLengthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "num_classes = 2\n",
    "hidden_size = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitUpDetectorSimpleRNN(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size):\n",
    "        super(SitUpDetectorSimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, num_classes)\n",
    "    \n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x.unsqueeze(0), hidden_state),1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model info\n",
      "SitUpDetectorSimpleRNN(\n",
      "  (in2hidden): Linear(in_features=15, out_features=12, bias=True)\n",
      "  (in2output): Linear(in_features=15, out_features=2, bias=True)\n",
      ")\n",
      "Epoch [1/100], Loss: 0.0157\n",
      "Epoch [2/100], Loss: 0.0162\n",
      "Epoch [3/100], Loss: 0.0161\n",
      "Epoch [4/100], Loss: 0.0165\n",
      "Epoch [5/100], Loss: 0.0178\n",
      "Epoch [6/100], Loss: 0.0207\n",
      "Epoch [7/100], Loss: 0.0249\n",
      "Epoch [8/100], Loss: 0.0299\n",
      "Epoch [9/100], Loss: 0.0353\n",
      "Epoch [10/100], Loss: 0.0406\n",
      "Epoch [11/100], Loss: 0.0454\n",
      "Epoch [12/100], Loss: 0.0496\n",
      "Epoch [13/100], Loss: 0.0529\n",
      "Epoch [14/100], Loss: 0.0549\n",
      "Epoch [15/100], Loss: 0.0555\n",
      "Epoch [16/100], Loss: 0.0547\n",
      "Epoch [17/100], Loss: 0.0524\n",
      "Epoch [18/100], Loss: 0.0494\n",
      "Epoch [19/100], Loss: 0.0461\n",
      "Epoch [20/100], Loss: 0.0429\n",
      "Epoch [21/100], Loss: 0.0399\n",
      "Epoch [22/100], Loss: 0.0371\n",
      "Epoch [23/100], Loss: 0.0345\n",
      "Epoch [24/100], Loss: 0.0322\n",
      "Epoch [25/100], Loss: 0.0301\n",
      "Epoch [26/100], Loss: 0.0282\n",
      "Epoch [27/100], Loss: 0.0265\n",
      "Epoch [28/100], Loss: 0.0251\n",
      "Epoch [29/100], Loss: 0.0233\n",
      "Epoch [30/100], Loss: 0.0220\n",
      "Epoch [31/100], Loss: 0.0206\n",
      "Epoch [32/100], Loss: 0.0193\n",
      "Epoch [33/100], Loss: 0.0180\n",
      "Epoch [34/100], Loss: 0.0168\n",
      "Epoch [35/100], Loss: 0.0158\n",
      "Epoch [36/100], Loss: 0.0148\n",
      "Epoch [37/100], Loss: 0.0140\n",
      "Epoch [38/100], Loss: 0.0131\n",
      "Epoch [39/100], Loss: 0.0072\n",
      "Epoch [40/100], Loss: 0.0112\n",
      "Epoch [41/100], Loss: 0.0107\n",
      "Epoch [42/100], Loss: 0.0102\n",
      "Epoch [43/100], Loss: 0.0099\n",
      "Epoch [44/100], Loss: 0.0096\n",
      "Epoch [45/100], Loss: 0.0093\n",
      "Epoch [46/100], Loss: 0.0090\n",
      "Epoch [47/100], Loss: 0.0088\n",
      "Epoch [48/100], Loss: 0.0085\n",
      "Epoch [49/100], Loss: 0.0082\n",
      "Epoch [50/100], Loss: 0.0080\n",
      "Epoch [51/100], Loss: 0.0078\n",
      "Epoch [52/100], Loss: 0.0075\n",
      "Epoch [53/100], Loss: 0.0073\n",
      "Epoch [54/100], Loss: 0.0072\n",
      "Epoch [55/100], Loss: 0.0070\n",
      "Epoch [56/100], Loss: 0.0069\n",
      "Epoch [57/100], Loss: 0.0068\n",
      "Epoch [58/100], Loss: 0.0067\n",
      "Epoch [59/100], Loss: 0.0067\n",
      "Epoch [60/100], Loss: 0.0067\n",
      "Epoch [61/100], Loss: 0.0067\n",
      "Epoch [62/100], Loss: 0.0066\n",
      "Epoch [63/100], Loss: 0.0066\n",
      "Epoch [64/100], Loss: 0.0066\n",
      "Epoch [65/100], Loss: 0.0065\n",
      "Epoch [66/100], Loss: 0.0064\n",
      "Epoch [67/100], Loss: 0.0063\n",
      "Epoch [68/100], Loss: 0.0062\n",
      "Epoch [69/100], Loss: 0.0061\n",
      "Epoch [70/100], Loss: 0.0060\n",
      "Epoch [71/100], Loss: 0.0058\n",
      "Epoch [72/100], Loss: 0.0057\n",
      "Epoch [73/100], Loss: 0.0056\n",
      "Epoch [74/100], Loss: 0.0054\n",
      "Epoch [75/100], Loss: 0.0053\n",
      "Epoch [76/100], Loss: 0.0052\n",
      "Epoch [77/100], Loss: 0.0050\n",
      "Epoch [78/100], Loss: 0.0049\n",
      "Epoch [79/100], Loss: 0.0048\n",
      "Epoch [80/100], Loss: 0.0047\n",
      "Epoch [81/100], Loss: 0.0046\n",
      "Epoch [82/100], Loss: 0.0045\n",
      "Epoch [83/100], Loss: 0.0043\n",
      "Epoch [84/100], Loss: 0.0042\n",
      "Epoch [85/100], Loss: 0.0041\n",
      "Epoch [86/100], Loss: 0.0040\n",
      "Epoch [87/100], Loss: 0.0039\n",
      "Epoch [88/100], Loss: 0.0038\n",
      "Epoch [89/100], Loss: 0.0037\n",
      "Epoch [90/100], Loss: 0.0036\n",
      "Epoch [91/100], Loss: 0.0035\n",
      "Epoch [92/100], Loss: 0.0034\n",
      "Epoch [93/100], Loss: 0.0034\n",
      "Epoch [94/100], Loss: 0.0033\n",
      "Epoch [95/100], Loss: 0.0032\n",
      "Epoch [96/100], Loss: 0.0031\n",
      "Epoch [97/100], Loss: 0.0031\n",
      "Epoch [98/100], Loss: 0.0030\n",
      "Epoch [99/100], Loss: 0.0030\n",
      "Epoch [100/100], Loss: 0.0029\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  create pytorch lstm variable recurrent classifier\n",
    "\n",
    "\n",
    "# train SitUpDetectorVariableInput\n",
    "model = SitUpDetectorSimpleRNN(input_size, num_classes, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"model info\")\n",
    "print(model)\n",
    "\n",
    "myDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(train_x_numpy_no_pad,train_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for xyz in features:\n",
    "            xyz = torch.tensor(xyz)\n",
    "            output, hidden_state = model(xyz.float(), hidden_state)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "            f\"Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "    losses.append(loss.item())\n",
    "\n",
    "\n",
    "y = np.array(losses)\n",
    "x = np.arange(0,len(losses))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,y)\n",
    "plt.savefig(f\"simple-rnn-loss-for-params-num-{num_layers}-learning_rate{learning_rate}-epochs-{num_epochs}.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.6032%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "myTestDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(test_x_numpy_no_pad,test_y)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=myTestDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "num_correct = 0\n",
    "num_samples = len(test_loader)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (features, target) in enumerate(test_loader):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for xyz in features:\n",
    "            xyz = torch.tensor(xyz)\n",
    "            output, hidden_state = model(xyz.float(), hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == target)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "\n",
    "# save the accuracy and model hyperparameters to a text file \n",
    "\n",
    "with open(f\"simple-rnn-accuracy-for-params-num-{num_layers}-learning_rate{learning_rate}-epochs-{num_epochs}.txt\", \"w\") as f:\n",
    "    f.write(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "    f.write(f\"num_layers: {num_layers}\")\n",
    "    f.write(f\"learning_rate: {learning_rate}\")\n",
    "    f.write(f\"num_epochs: {num_epochs}\")\n",
    "    f.write(f\"hidden_size: {hidden_size}\")\n",
    "    f.write(f\"input_size: {input_size}\")\n",
    "    f.write(f\"num_classes: {num_classes}\")\n",
    "    f.write(f\"optimizer: {optimizer}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper param tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.3636\n",
      "Epoch [2/200], Loss: 0.3564\n",
      "Epoch [3/200], Loss: 0.3669\n",
      "Epoch [4/200], Loss: 0.3791\n",
      "Epoch [5/200], Loss: 0.3902\n",
      "Epoch [6/200], Loss: 0.3999\n",
      "Epoch [7/200], Loss: 0.4085\n",
      "Epoch [8/200], Loss: 0.4160\n",
      "Epoch [9/200], Loss: 0.4226\n",
      "Epoch [10/200], Loss: 0.4286\n",
      "Epoch [11/200], Loss: 0.4339\n",
      "Epoch [12/200], Loss: 0.4387\n",
      "Epoch [13/200], Loss: 0.4431\n",
      "Epoch [14/200], Loss: 0.4471\n",
      "Epoch [15/200], Loss: 0.4508\n",
      "Epoch [16/200], Loss: 0.4543\n",
      "Epoch [17/200], Loss: 0.4576\n",
      "Epoch [18/200], Loss: 0.4607\n",
      "Epoch [19/200], Loss: 0.4637\n",
      "Epoch [20/200], Loss: 0.4666\n",
      "Epoch [21/200], Loss: 0.4695\n",
      "Epoch [22/200], Loss: 0.4723\n",
      "Epoch [23/200], Loss: 0.4750\n",
      "Epoch [24/200], Loss: 0.4778\n",
      "Epoch [25/200], Loss: 0.4805\n",
      "Epoch [26/200], Loss: 0.4832\n",
      "Epoch [27/200], Loss: 0.4860\n",
      "Epoch [28/200], Loss: 0.4887\n",
      "Epoch [29/200], Loss: 0.4914\n",
      "Epoch [30/200], Loss: 0.4941\n",
      "Epoch [31/200], Loss: 0.4967\n",
      "Epoch [32/200], Loss: 0.4994\n",
      "Epoch [33/200], Loss: 0.5020\n",
      "Epoch [34/200], Loss: 0.5047\n",
      "Epoch [35/200], Loss: 0.5073\n",
      "Epoch [36/200], Loss: 0.5098\n",
      "Epoch [37/200], Loss: 0.5123\n",
      "Epoch [38/200], Loss: 0.5147\n",
      "Epoch [39/200], Loss: 0.5170\n",
      "Epoch [40/200], Loss: 0.5192\n",
      "Epoch [41/200], Loss: 0.5212\n",
      "Epoch [42/200], Loss: 0.5231\n",
      "Epoch [43/200], Loss: 0.5248\n",
      "Epoch [44/200], Loss: 0.5263\n",
      "Epoch [45/200], Loss: 0.5275\n",
      "Epoch [46/200], Loss: 0.5285\n",
      "Epoch [47/200], Loss: 0.5292\n",
      "Epoch [48/200], Loss: 0.5295\n",
      "Epoch [49/200], Loss: 0.5294\n",
      "Epoch [50/200], Loss: 0.5288\n",
      "Epoch [51/200], Loss: 0.5276\n",
      "Epoch [52/200], Loss: 0.5259\n",
      "Epoch [53/200], Loss: 0.5236\n",
      "Epoch [54/200], Loss: 0.5207\n",
      "Epoch [55/200], Loss: 0.5172\n",
      "Epoch [56/200], Loss: 0.5133\n",
      "Epoch [57/200], Loss: 0.5091\n",
      "Epoch [58/200], Loss: 0.5047\n",
      "Epoch [59/200], Loss: 0.5004\n",
      "Epoch [60/200], Loss: 0.4961\n",
      "Epoch [61/200], Loss: 0.4920\n",
      "Epoch [62/200], Loss: 0.4882\n",
      "Epoch [63/200], Loss: 0.4848\n",
      "Epoch [64/200], Loss: 0.4815\n",
      "Epoch [65/200], Loss: 0.4785\n",
      "Epoch [66/200], Loss: 0.4755\n",
      "Epoch [67/200], Loss: 0.4727\n",
      "Epoch [68/200], Loss: 0.4699\n",
      "Epoch [69/200], Loss: 0.4671\n",
      "Epoch [70/200], Loss: 0.4645\n",
      "Epoch [71/200], Loss: 0.4619\n",
      "Epoch [72/200], Loss: 0.4592\n",
      "Epoch [73/200], Loss: 0.4565\n",
      "Epoch [74/200], Loss: 0.4538\n",
      "Epoch [75/200], Loss: 0.4510\n",
      "Epoch [76/200], Loss: 0.4481\n",
      "Epoch [77/200], Loss: 0.4452\n",
      "Epoch [78/200], Loss: 0.4421\n",
      "Epoch [79/200], Loss: 0.4390\n",
      "Epoch [80/200], Loss: 0.4360\n",
      "Epoch [81/200], Loss: 0.4331\n",
      "Epoch [82/200], Loss: 0.4302\n",
      "Epoch [83/200], Loss: 0.4274\n",
      "Epoch [84/200], Loss: 0.4246\n",
      "Epoch [85/200], Loss: 0.4219\n",
      "Epoch [86/200], Loss: 0.4192\n",
      "Epoch [87/200], Loss: 0.4167\n",
      "Epoch [88/200], Loss: 0.4144\n",
      "Epoch [89/200], Loss: 0.4121\n",
      "Epoch [90/200], Loss: 0.4099\n",
      "Epoch [91/200], Loss: 0.4078\n",
      "Epoch [92/200], Loss: 0.4058\n",
      "Epoch [93/200], Loss: 0.4039\n",
      "Epoch [94/200], Loss: 0.4021\n",
      "Epoch [95/200], Loss: 0.4004\n",
      "Epoch [96/200], Loss: 0.3987\n",
      "Epoch [97/200], Loss: 0.3971\n",
      "Epoch [98/200], Loss: 0.3956\n",
      "Epoch [99/200], Loss: 0.3941\n",
      "Epoch [100/200], Loss: 0.3928\n",
      "Epoch [101/200], Loss: 0.3915\n",
      "Epoch [102/200], Loss: 0.3903\n",
      "Epoch [103/200], Loss: 0.3891\n",
      "Epoch [104/200], Loss: 0.3881\n",
      "Epoch [105/200], Loss: 0.3871\n",
      "Epoch [106/200], Loss: 0.3862\n",
      "Epoch [107/200], Loss: 0.3854\n",
      "Epoch [108/200], Loss: 0.3847\n",
      "Epoch [109/200], Loss: 0.3840\n",
      "Epoch [110/200], Loss: 0.3835\n",
      "Epoch [111/200], Loss: 0.3830\n",
      "Epoch [112/200], Loss: 0.3826\n",
      "Epoch [113/200], Loss: 0.3823\n",
      "Epoch [114/200], Loss: 0.3821\n",
      "Epoch [115/200], Loss: 0.3820\n",
      "Epoch [116/200], Loss: 0.3820\n",
      "Epoch [117/200], Loss: 0.3822\n",
      "Epoch [118/200], Loss: 0.3825\n",
      "Epoch [119/200], Loss: 0.3829\n",
      "Epoch [120/200], Loss: 0.3833\n",
      "Epoch [121/200], Loss: 0.3838\n",
      "Epoch [122/200], Loss: 0.3844\n",
      "Epoch [123/200], Loss: 0.3850\n",
      "Epoch [124/200], Loss: 0.3856\n",
      "Epoch [125/200], Loss: 0.3862\n",
      "Epoch [126/200], Loss: 0.3869\n",
      "Epoch [127/200], Loss: 0.3876\n",
      "Epoch [128/200], Loss: 0.3882\n",
      "Epoch [129/200], Loss: 0.3888\n",
      "Epoch [130/200], Loss: 0.3893\n",
      "Epoch [131/200], Loss: 0.3898\n",
      "Epoch [132/200], Loss: 0.3902\n",
      "Epoch [133/200], Loss: 0.3905\n",
      "Epoch [134/200], Loss: 0.3908\n",
      "Epoch [135/200], Loss: 0.3910\n",
      "Epoch [136/200], Loss: 0.3912\n",
      "Epoch [137/200], Loss: 0.3912\n",
      "Epoch [138/200], Loss: 0.3913\n",
      "Epoch [139/200], Loss: 0.3913\n",
      "Epoch [140/200], Loss: 0.3913\n",
      "Epoch [141/200], Loss: 0.3912\n",
      "Epoch [142/200], Loss: 0.3911\n",
      "Epoch [143/200], Loss: 0.3909\n",
      "Epoch [144/200], Loss: 0.3907\n",
      "Epoch [145/200], Loss: 0.3905\n",
      "Epoch [146/200], Loss: 0.3903\n",
      "Epoch [147/200], Loss: 0.3901\n",
      "Epoch [148/200], Loss: 0.3899\n",
      "Epoch [149/200], Loss: 0.3897\n",
      "Epoch [150/200], Loss: 0.3896\n",
      "Epoch [151/200], Loss: 0.3894\n",
      "Epoch [152/200], Loss: 0.3893\n",
      "Epoch [153/200], Loss: 0.3893\n",
      "Epoch [154/200], Loss: 0.3892\n",
      "Epoch [155/200], Loss: 0.3892\n",
      "Epoch [156/200], Loss: 0.3892\n",
      "Epoch [157/200], Loss: 0.3892\n",
      "Epoch [158/200], Loss: 0.3892\n",
      "Epoch [159/200], Loss: 0.3892\n",
      "Epoch [160/200], Loss: 0.3893\n",
      "Epoch [161/200], Loss: 0.3893\n",
      "Epoch [162/200], Loss: 0.3893\n",
      "Epoch [163/200], Loss: 0.3893\n",
      "Epoch [164/200], Loss: 0.3894\n",
      "Epoch [165/200], Loss: 0.3894\n",
      "Epoch [166/200], Loss: 0.3894\n",
      "Epoch [167/200], Loss: 0.3894\n",
      "Epoch [168/200], Loss: 0.3894\n",
      "Epoch [169/200], Loss: 0.3894\n",
      "Epoch [170/200], Loss: 0.3894\n",
      "Epoch [171/200], Loss: 0.3894\n",
      "Epoch [172/200], Loss: 0.3894\n",
      "Epoch [173/200], Loss: 0.3893\n",
      "Epoch [174/200], Loss: 0.3893\n",
      "Epoch [175/200], Loss: 0.3893\n",
      "Epoch [176/200], Loss: 0.3892\n",
      "Epoch [177/200], Loss: 0.3892\n",
      "Epoch [178/200], Loss: 0.3891\n",
      "Epoch [179/200], Loss: 0.3891\n",
      "Epoch [180/200], Loss: 0.3890\n",
      "Epoch [181/200], Loss: 0.3890\n",
      "Epoch [182/200], Loss: 0.3889\n",
      "Epoch [183/200], Loss: 0.3888\n",
      "Epoch [184/200], Loss: 0.3888\n",
      "Epoch [185/200], Loss: 0.3887\n",
      "Epoch [186/200], Loss: 0.3887\n",
      "Epoch [187/200], Loss: 0.3886\n",
      "Epoch [188/200], Loss: 0.3886\n",
      "Epoch [189/200], Loss: 0.3885\n",
      "Epoch [190/200], Loss: 0.3884\n",
      "Epoch [191/200], Loss: 0.3884\n",
      "Epoch [192/200], Loss: 0.3883\n",
      "Epoch [193/200], Loss: 0.3883\n",
      "Epoch [194/200], Loss: 0.3882\n",
      "Epoch [195/200], Loss: 0.3882\n",
      "Epoch [196/200], Loss: 0.3882\n",
      "Epoch [197/200], Loss: 0.3881\n",
      "Epoch [198/200], Loss: 0.3881\n",
      "Epoch [199/200], Loss: 0.3881\n",
      "Epoch [200/200], Loss: 0.3880\n",
      "Accuracy: 46.8254%\n",
      "Epoch [1/200], Loss: 0.1948\n",
      "Epoch [2/200], Loss: 0.1983\n",
      "Epoch [3/200], Loss: 0.2105\n",
      "Epoch [4/200], Loss: 0.2209\n",
      "Epoch [5/200], Loss: 0.2286\n",
      "Epoch [6/200], Loss: 0.2342\n",
      "Epoch [7/200], Loss: 0.2376\n",
      "Epoch [8/200], Loss: 0.2392\n",
      "Epoch [9/200], Loss: 0.2395\n",
      "Epoch [10/200], Loss: 0.2388\n",
      "Epoch [11/200], Loss: 0.2373\n",
      "Epoch [12/200], Loss: 0.2352\n",
      "Epoch [13/200], Loss: 0.2326\n",
      "Epoch [14/200], Loss: 0.2297\n",
      "Epoch [15/200], Loss: 0.2267\n",
      "Epoch [16/200], Loss: 0.2237\n",
      "Epoch [17/200], Loss: 0.2207\n",
      "Epoch [18/200], Loss: 0.2177\n",
      "Epoch [19/200], Loss: 0.2149\n",
      "Epoch [20/200], Loss: 0.2122\n",
      "Epoch [21/200], Loss: 0.2096\n",
      "Epoch [22/200], Loss: 0.2071\n",
      "Epoch [23/200], Loss: 0.2046\n",
      "Epoch [24/200], Loss: 0.2023\n",
      "Epoch [25/200], Loss: 0.2000\n",
      "Epoch [26/200], Loss: 0.1977\n",
      "Epoch [27/200], Loss: 0.1954\n",
      "Epoch [28/200], Loss: 0.1931\n",
      "Epoch [29/200], Loss: 0.1907\n",
      "Epoch [30/200], Loss: 0.1883\n",
      "Epoch [31/200], Loss: 0.1859\n",
      "Epoch [32/200], Loss: 0.1835\n",
      "Epoch [33/200], Loss: 0.1811\n",
      "Epoch [34/200], Loss: 0.1786\n",
      "Epoch [35/200], Loss: 0.1761\n",
      "Epoch [36/200], Loss: 0.1735\n",
      "Epoch [37/200], Loss: 0.1709\n",
      "Epoch [38/200], Loss: 0.1683\n",
      "Epoch [39/200], Loss: 0.1657\n",
      "Epoch [40/200], Loss: 0.1630\n",
      "Epoch [41/200], Loss: 0.1603\n",
      "Epoch [42/200], Loss: 0.1576\n",
      "Epoch [43/200], Loss: 0.1549\n",
      "Epoch [44/200], Loss: 0.1521\n",
      "Epoch [45/200], Loss: 0.1494\n",
      "Epoch [46/200], Loss: 0.1466\n",
      "Epoch [47/200], Loss: 0.1439\n",
      "Epoch [48/200], Loss: 0.1411\n",
      "Epoch [49/200], Loss: 0.1384\n",
      "Epoch [50/200], Loss: 0.1357\n",
      "Epoch [51/200], Loss: 0.1330\n",
      "Epoch [52/200], Loss: 0.1304\n",
      "Epoch [53/200], Loss: 0.1278\n",
      "Epoch [54/200], Loss: 0.1253\n",
      "Epoch [55/200], Loss: 0.1228\n",
      "Epoch [56/200], Loss: 0.1203\n",
      "Epoch [57/200], Loss: 0.1179\n",
      "Epoch [58/200], Loss: 0.1154\n",
      "Epoch [59/200], Loss: 0.1130\n",
      "Epoch [60/200], Loss: 0.1107\n",
      "Epoch [61/200], Loss: 0.1084\n",
      "Epoch [62/200], Loss: 0.1061\n",
      "Epoch [63/200], Loss: 0.1038\n",
      "Epoch [64/200], Loss: 0.1017\n",
      "Epoch [65/200], Loss: 0.0996\n",
      "Epoch [66/200], Loss: 0.0975\n",
      "Epoch [67/200], Loss: 0.0955\n",
      "Epoch [68/200], Loss: 0.0935\n",
      "Epoch [69/200], Loss: 0.0916\n",
      "Epoch [70/200], Loss: 0.0897\n",
      "Epoch [71/200], Loss: 0.0878\n",
      "Epoch [72/200], Loss: 0.0859\n",
      "Epoch [73/200], Loss: 0.0841\n",
      "Epoch [74/200], Loss: 0.0823\n",
      "Epoch [75/200], Loss: 0.0805\n",
      "Epoch [76/200], Loss: 0.0788\n",
      "Epoch [77/200], Loss: 0.0770\n",
      "Epoch [78/200], Loss: 0.0754\n",
      "Epoch [79/200], Loss: 0.0737\n",
      "Epoch [80/200], Loss: 0.0721\n",
      "Epoch [81/200], Loss: 0.0705\n",
      "Epoch [82/200], Loss: 0.0690\n",
      "Epoch [83/200], Loss: 0.0676\n",
      "Epoch [84/200], Loss: 0.0661\n",
      "Epoch [85/200], Loss: 0.0647\n",
      "Epoch [86/200], Loss: 0.0634\n",
      "Epoch [87/200], Loss: 0.0621\n",
      "Epoch [88/200], Loss: 0.0608\n",
      "Epoch [89/200], Loss: 0.0595\n",
      "Epoch [90/200], Loss: 0.0582\n",
      "Epoch [91/200], Loss: 0.0570\n",
      "Epoch [92/200], Loss: 0.0558\n",
      "Epoch [93/200], Loss: 0.0546\n",
      "Epoch [94/200], Loss: 0.0534\n",
      "Epoch [95/200], Loss: 0.0523\n",
      "Epoch [96/200], Loss: 0.0512\n",
      "Epoch [97/200], Loss: 0.0501\n",
      "Epoch [98/200], Loss: 0.0490\n",
      "Epoch [99/200], Loss: 0.0480\n",
      "Epoch [100/200], Loss: 0.0469\n",
      "Epoch [101/200], Loss: 0.0459\n",
      "Epoch [102/200], Loss: 0.0448\n",
      "Epoch [103/200], Loss: 0.0438\n",
      "Epoch [104/200], Loss: 0.0429\n",
      "Epoch [105/200], Loss: 0.0419\n",
      "Epoch [106/200], Loss: 0.0410\n",
      "Epoch [107/200], Loss: 0.0401\n",
      "Epoch [108/200], Loss: 0.0392\n",
      "Epoch [109/200], Loss: 0.0384\n",
      "Epoch [110/200], Loss: 0.0376\n",
      "Epoch [111/200], Loss: 0.0368\n",
      "Epoch [112/200], Loss: 0.0360\n",
      "Epoch [113/200], Loss: 0.0353\n",
      "Epoch [114/200], Loss: 0.0346\n",
      "Epoch [115/200], Loss: 0.0339\n",
      "Epoch [116/200], Loss: 0.0332\n",
      "Epoch [117/200], Loss: 0.0326\n",
      "Epoch [118/200], Loss: 0.0319\n",
      "Epoch [119/200], Loss: 0.0313\n",
      "Epoch [120/200], Loss: 0.0307\n",
      "Epoch [121/200], Loss: 0.0301\n",
      "Epoch [122/200], Loss: 0.0296\n",
      "Epoch [123/200], Loss: 0.0290\n",
      "Epoch [124/200], Loss: 0.0285\n",
      "Epoch [125/200], Loss: 0.0280\n",
      "Epoch [126/200], Loss: 0.0275\n",
      "Epoch [127/200], Loss: 0.0271\n",
      "Epoch [128/200], Loss: 0.0266\n",
      "Epoch [129/200], Loss: 0.0261\n",
      "Epoch [130/200], Loss: 0.0257\n",
      "Epoch [131/200], Loss: 0.0253\n",
      "Epoch [132/200], Loss: 0.0249\n",
      "Epoch [133/200], Loss: 0.0245\n",
      "Epoch [134/200], Loss: 0.0241\n",
      "Epoch [135/200], Loss: 0.0238\n",
      "Epoch [136/200], Loss: 0.0234\n",
      "Epoch [137/200], Loss: 0.0231\n",
      "Epoch [138/200], Loss: 0.0227\n",
      "Epoch [139/200], Loss: 0.0224\n",
      "Epoch [140/200], Loss: 0.0220\n",
      "Epoch [141/200], Loss: 0.0217\n",
      "Epoch [142/200], Loss: 0.0214\n",
      "Epoch [143/200], Loss: 0.0211\n",
      "Epoch [144/200], Loss: 0.0209\n",
      "Epoch [145/200], Loss: 0.0206\n",
      "Epoch [146/200], Loss: 0.0203\n",
      "Epoch [147/200], Loss: 0.0201\n",
      "Epoch [148/200], Loss: 0.0198\n",
      "Epoch [149/200], Loss: 0.0196\n",
      "Epoch [150/200], Loss: 0.0193\n",
      "Epoch [151/200], Loss: 0.0191\n",
      "Epoch [152/200], Loss: 0.0189\n",
      "Epoch [153/200], Loss: 0.0187\n",
      "Epoch [154/200], Loss: 0.0184\n",
      "Epoch [155/200], Loss: 0.0182\n",
      "Epoch [156/200], Loss: 0.0180\n",
      "Epoch [157/200], Loss: 0.0179\n",
      "Epoch [158/200], Loss: 0.0177\n",
      "Epoch [159/200], Loss: 0.0175\n",
      "Epoch [160/200], Loss: 0.0173\n",
      "Epoch [161/200], Loss: 0.0172\n",
      "Epoch [162/200], Loss: 0.0170\n",
      "Epoch [163/200], Loss: 0.0168\n",
      "Epoch [164/200], Loss: 0.0167\n",
      "Epoch [165/200], Loss: 0.0165\n",
      "Epoch [166/200], Loss: 0.0164\n",
      "Epoch [167/200], Loss: 0.0163\n",
      "Epoch [168/200], Loss: 0.0161\n",
      "Epoch [169/200], Loss: 0.0160\n",
      "Epoch [170/200], Loss: 0.0159\n",
      "Epoch [171/200], Loss: 0.0157\n",
      "Epoch [172/200], Loss: 0.0156\n",
      "Epoch [173/200], Loss: 0.0155\n",
      "Epoch [174/200], Loss: 0.0154\n",
      "Epoch [175/200], Loss: 0.0153\n",
      "Epoch [176/200], Loss: 0.0152\n",
      "Epoch [177/200], Loss: 0.0150\n",
      "Epoch [178/200], Loss: 0.0149\n",
      "Epoch [179/200], Loss: 0.0148\n",
      "Epoch [180/200], Loss: 0.0147\n",
      "Epoch [181/200], Loss: 0.0146\n",
      "Epoch [182/200], Loss: 0.0145\n",
      "Epoch [183/200], Loss: 0.0144\n",
      "Epoch [184/200], Loss: 0.0143\n",
      "Epoch [185/200], Loss: 0.0143\n",
      "Epoch [186/200], Loss: 0.0142\n",
      "Epoch [187/200], Loss: 0.0141\n",
      "Epoch [188/200], Loss: 0.0140\n",
      "Epoch [189/200], Loss: 0.0139\n",
      "Epoch [190/200], Loss: 0.0138\n",
      "Epoch [191/200], Loss: 0.0137\n",
      "Epoch [192/200], Loss: 0.0136\n",
      "Epoch [193/200], Loss: 0.0136\n",
      "Epoch [194/200], Loss: 0.0135\n",
      "Epoch [195/200], Loss: 0.0134\n",
      "Epoch [196/200], Loss: 0.0133\n",
      "Epoch [197/200], Loss: 0.0132\n",
      "Epoch [198/200], Loss: 0.0132\n",
      "Epoch [199/200], Loss: 0.0131\n",
      "Epoch [200/200], Loss: 0.0130\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/200], Loss: 0.0282\n",
      "Epoch [2/200], Loss: 0.0431\n",
      "Epoch [3/200], Loss: 0.0522\n",
      "Epoch [4/200], Loss: 0.0587\n",
      "Epoch [5/200], Loss: 0.0633\n",
      "Epoch [6/200], Loss: 0.0668\n",
      "Epoch [7/200], Loss: 0.0695\n",
      "Epoch [8/200], Loss: 0.0719\n",
      "Epoch [9/200], Loss: 0.0738\n",
      "Epoch [10/200], Loss: 0.0756\n",
      "Epoch [11/200], Loss: 0.0771\n",
      "Epoch [12/200], Loss: 0.0784\n",
      "Epoch [13/200], Loss: 0.0794\n",
      "Epoch [14/200], Loss: 0.0800\n",
      "Epoch [15/200], Loss: 0.0801\n",
      "Epoch [16/200], Loss: 0.0796\n",
      "Epoch [17/200], Loss: 0.0785\n",
      "Epoch [18/200], Loss: 0.0766\n",
      "Epoch [19/200], Loss: 0.0740\n",
      "Epoch [20/200], Loss: 0.0706\n",
      "Epoch [21/200], Loss: 0.0668\n",
      "Epoch [22/200], Loss: 0.0628\n",
      "Epoch [23/200], Loss: 0.0589\n",
      "Epoch [24/200], Loss: 0.0552\n",
      "Epoch [25/200], Loss: 0.0520\n",
      "Epoch [26/200], Loss: 0.0492\n",
      "Epoch [27/200], Loss: 0.0467\n",
      "Epoch [28/200], Loss: 0.0446\n",
      "Epoch [29/200], Loss: 0.0428\n",
      "Epoch [30/200], Loss: 0.0413\n",
      "Epoch [31/200], Loss: 0.0399\n",
      "Epoch [32/200], Loss: 0.0387\n",
      "Epoch [33/200], Loss: 0.0375\n",
      "Epoch [34/200], Loss: 0.0363\n",
      "Epoch [35/200], Loss: 0.0352\n",
      "Epoch [36/200], Loss: 0.0341\n",
      "Epoch [37/200], Loss: 0.0330\n",
      "Epoch [38/200], Loss: 0.0319\n",
      "Epoch [39/200], Loss: 0.0309\n",
      "Epoch [40/200], Loss: 0.0298\n",
      "Epoch [41/200], Loss: 0.0287\n",
      "Epoch [42/200], Loss: 0.0276\n",
      "Epoch [43/200], Loss: 0.0265\n",
      "Epoch [44/200], Loss: 0.0255\n",
      "Epoch [45/200], Loss: 0.0245\n",
      "Epoch [46/200], Loss: 0.0236\n",
      "Epoch [47/200], Loss: 0.0228\n",
      "Epoch [48/200], Loss: 0.0220\n",
      "Epoch [49/200], Loss: 0.0213\n",
      "Epoch [50/200], Loss: 0.0206\n",
      "Epoch [51/200], Loss: 0.0200\n",
      "Epoch [52/200], Loss: 0.0195\n",
      "Epoch [53/200], Loss: 0.0189\n",
      "Epoch [54/200], Loss: 0.0184\n",
      "Epoch [55/200], Loss: 0.0179\n",
      "Epoch [56/200], Loss: 0.0175\n",
      "Epoch [57/200], Loss: 0.0170\n",
      "Epoch [58/200], Loss: 0.0166\n",
      "Epoch [59/200], Loss: 0.0162\n",
      "Epoch [60/200], Loss: 0.0158\n",
      "Epoch [61/200], Loss: 0.0154\n",
      "Epoch [62/200], Loss: 0.0150\n",
      "Epoch [63/200], Loss: 0.0146\n",
      "Epoch [64/200], Loss: 0.0143\n",
      "Epoch [65/200], Loss: 0.0140\n",
      "Epoch [66/200], Loss: 0.0137\n",
      "Epoch [67/200], Loss: 0.0133\n",
      "Epoch [68/200], Loss: 0.0130\n",
      "Epoch [69/200], Loss: 0.0128\n",
      "Epoch [70/200], Loss: 0.0125\n",
      "Epoch [71/200], Loss: 0.0122\n",
      "Epoch [72/200], Loss: 0.0120\n",
      "Epoch [73/200], Loss: 0.0117\n",
      "Epoch [74/200], Loss: 0.0115\n",
      "Epoch [75/200], Loss: 0.0113\n",
      "Epoch [76/200], Loss: 0.0110\n",
      "Epoch [77/200], Loss: 0.0108\n",
      "Epoch [78/200], Loss: 0.0106\n",
      "Epoch [79/200], Loss: 0.0105\n",
      "Epoch [80/200], Loss: 0.0103\n",
      "Epoch [81/200], Loss: 0.0101\n",
      "Epoch [82/200], Loss: 0.0100\n",
      "Epoch [83/200], Loss: 0.0098\n",
      "Epoch [84/200], Loss: 0.0097\n",
      "Epoch [85/200], Loss: 0.0095\n",
      "Epoch [86/200], Loss: 0.0094\n",
      "Epoch [87/200], Loss: 0.0093\n",
      "Epoch [88/200], Loss: 0.0092\n",
      "Epoch [89/200], Loss: 0.0091\n",
      "Epoch [90/200], Loss: 0.0089\n",
      "Epoch [91/200], Loss: 0.0088\n",
      "Epoch [92/200], Loss: 0.0087\n",
      "Epoch [93/200], Loss: 0.0086\n",
      "Epoch [94/200], Loss: 0.0085\n",
      "Epoch [95/200], Loss: 0.0084\n",
      "Epoch [96/200], Loss: 0.0083\n",
      "Epoch [97/200], Loss: 0.0082\n",
      "Epoch [98/200], Loss: 0.0081\n",
      "Epoch [99/200], Loss: 0.0080\n",
      "Epoch [100/200], Loss: 0.0079\n",
      "Epoch [101/200], Loss: 0.0078\n",
      "Epoch [102/200], Loss: 0.0077\n",
      "Epoch [103/200], Loss: 0.0076\n",
      "Epoch [104/200], Loss: 0.0075\n",
      "Epoch [105/200], Loss: 0.0074\n",
      "Epoch [106/200], Loss: 0.0073\n",
      "Epoch [107/200], Loss: 0.0072\n",
      "Epoch [108/200], Loss: 0.0072\n",
      "Epoch [109/200], Loss: 0.0071\n",
      "Epoch [110/200], Loss: 0.0070\n",
      "Epoch [111/200], Loss: 0.0069\n",
      "Epoch [112/200], Loss: 0.0069\n",
      "Epoch [113/200], Loss: 0.0068\n",
      "Epoch [114/200], Loss: 0.0067\n",
      "Epoch [115/200], Loss: 0.0067\n",
      "Epoch [116/200], Loss: 0.0066\n",
      "Epoch [117/200], Loss: 0.0066\n",
      "Epoch [118/200], Loss: 0.0065\n",
      "Epoch [119/200], Loss: 0.0064\n",
      "Epoch [120/200], Loss: 0.0064\n",
      "Epoch [121/200], Loss: 0.0063\n",
      "Epoch [122/200], Loss: 0.0063\n",
      "Epoch [123/200], Loss: 0.0062\n",
      "Epoch [124/200], Loss: 0.0062\n",
      "Epoch [125/200], Loss: 0.0061\n",
      "Epoch [126/200], Loss: 0.0061\n",
      "Epoch [127/200], Loss: 0.0060\n",
      "Epoch [128/200], Loss: 0.0060\n",
      "Epoch [129/200], Loss: 0.0059\n",
      "Epoch [130/200], Loss: 0.0059\n",
      "Epoch [131/200], Loss: 0.0059\n",
      "Epoch [132/200], Loss: 0.0058\n",
      "Epoch [133/200], Loss: 0.0058\n",
      "Epoch [134/200], Loss: 0.0057\n",
      "Epoch [135/200], Loss: 0.0057\n",
      "Epoch [136/200], Loss: 0.0057\n",
      "Epoch [137/200], Loss: 0.0056\n",
      "Epoch [138/200], Loss: 0.0056\n",
      "Epoch [139/200], Loss: 0.0056\n",
      "Epoch [140/200], Loss: 0.0055\n",
      "Epoch [141/200], Loss: 0.0055\n",
      "Epoch [142/200], Loss: 0.0054\n",
      "Epoch [143/200], Loss: 0.0054\n",
      "Epoch [144/200], Loss: 0.0054\n",
      "Epoch [145/200], Loss: 0.0053\n",
      "Epoch [146/200], Loss: 0.0053\n",
      "Epoch [147/200], Loss: 0.0053\n",
      "Epoch [148/200], Loss: 0.0052\n",
      "Epoch [149/200], Loss: 0.0052\n",
      "Epoch [150/200], Loss: 0.0052\n",
      "Epoch [151/200], Loss: 0.0051\n",
      "Epoch [152/200], Loss: 0.0051\n",
      "Epoch [153/200], Loss: 0.0050\n",
      "Epoch [154/200], Loss: 0.0050\n",
      "Epoch [155/200], Loss: 0.0050\n",
      "Epoch [156/200], Loss: 0.0049\n",
      "Epoch [157/200], Loss: 0.0049\n",
      "Epoch [158/200], Loss: 0.0049\n",
      "Epoch [159/200], Loss: 0.0048\n",
      "Epoch [160/200], Loss: 0.0048\n",
      "Epoch [161/200], Loss: 0.0048\n",
      "Epoch [162/200], Loss: 0.0047\n",
      "Epoch [163/200], Loss: 0.0047\n",
      "Epoch [164/200], Loss: 0.0047\n",
      "Epoch [165/200], Loss: 0.0046\n",
      "Epoch [166/200], Loss: 0.0046\n",
      "Epoch [167/200], Loss: 0.0046\n",
      "Epoch [168/200], Loss: 0.0045\n",
      "Epoch [169/200], Loss: 0.0045\n",
      "Epoch [170/200], Loss: 0.0045\n",
      "Epoch [171/200], Loss: 0.0044\n",
      "Epoch [172/200], Loss: 0.0044\n",
      "Epoch [173/200], Loss: 0.0044\n",
      "Epoch [174/200], Loss: 0.0043\n",
      "Epoch [175/200], Loss: 0.0043\n",
      "Epoch [176/200], Loss: 0.0043\n",
      "Epoch [177/200], Loss: 0.0042\n",
      "Epoch [178/200], Loss: 0.0042\n",
      "Epoch [179/200], Loss: 0.0042\n",
      "Epoch [180/200], Loss: 0.0041\n",
      "Epoch [181/200], Loss: 0.0041\n",
      "Epoch [182/200], Loss: 0.0040\n",
      "Epoch [183/200], Loss: 0.0040\n",
      "Epoch [184/200], Loss: 0.0039\n",
      "Epoch [185/200], Loss: 0.0039\n",
      "Epoch [186/200], Loss: 0.0039\n",
      "Epoch [187/200], Loss: 0.0038\n",
      "Epoch [188/200], Loss: 0.0038\n",
      "Epoch [189/200], Loss: 0.0037\n",
      "Epoch [190/200], Loss: 0.0037\n",
      "Epoch [191/200], Loss: 0.0036\n",
      "Epoch [192/200], Loss: 0.0036\n",
      "Epoch [193/200], Loss: 0.0036\n",
      "Epoch [194/200], Loss: 0.0035\n",
      "Epoch [195/200], Loss: 0.0035\n",
      "Epoch [196/200], Loss: 0.0035\n",
      "Epoch [197/200], Loss: 0.0034\n",
      "Epoch [198/200], Loss: 0.0034\n",
      "Epoch [199/200], Loss: 0.0034\n",
      "Epoch [200/200], Loss: 0.0033\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/200], Loss: 0.0093\n",
      "Epoch [2/200], Loss: 0.0110\n",
      "Epoch [3/200], Loss: 0.0120\n",
      "Epoch [4/200], Loss: 0.0130\n",
      "Epoch [5/200], Loss: 0.0144\n",
      "Epoch [6/200], Loss: 0.0166\n",
      "Epoch [7/200], Loss: 0.0198\n",
      "Epoch [8/200], Loss: 0.0240\n",
      "Epoch [9/200], Loss: 0.0289\n",
      "Epoch [10/200], Loss: 0.0341\n",
      "Epoch [11/200], Loss: 0.0390\n",
      "Epoch [12/200], Loss: 0.0434\n",
      "Epoch [13/200], Loss: 0.0468\n",
      "Epoch [14/200], Loss: 0.0492\n",
      "Epoch [15/200], Loss: 0.0505\n",
      "Epoch [16/200], Loss: 0.0508\n",
      "Epoch [17/200], Loss: 0.0503\n",
      "Epoch [18/200], Loss: 0.0490\n",
      "Epoch [19/200], Loss: 0.0472\n",
      "Epoch [20/200], Loss: 0.0453\n",
      "Epoch [21/200], Loss: 0.0434\n",
      "Epoch [22/200], Loss: 0.0415\n",
      "Epoch [23/200], Loss: 0.0399\n",
      "Epoch [24/200], Loss: 0.0385\n",
      "Epoch [25/200], Loss: 0.0372\n",
      "Epoch [26/200], Loss: 0.0361\n",
      "Epoch [27/200], Loss: 0.0348\n",
      "Epoch [28/200], Loss: 0.0335\n",
      "Epoch [29/200], Loss: 0.0322\n",
      "Epoch [30/200], Loss: 0.0309\n",
      "Epoch [31/200], Loss: 0.0294\n",
      "Epoch [32/200], Loss: 0.0279\n",
      "Epoch [33/200], Loss: 0.0262\n",
      "Epoch [34/200], Loss: 0.0248\n",
      "Epoch [35/200], Loss: 0.0230\n",
      "Epoch [36/200], Loss: 0.0217\n",
      "Epoch [37/200], Loss: 0.0200\n",
      "Epoch [38/200], Loss: 0.0188\n",
      "Epoch [39/200], Loss: 0.0174\n",
      "Epoch [40/200], Loss: 0.0163\n",
      "Epoch [41/200], Loss: 0.0151\n",
      "Epoch [42/200], Loss: 0.0142\n",
      "Epoch [43/200], Loss: 0.0132\n",
      "Epoch [44/200], Loss: 0.0124\n",
      "Epoch [45/200], Loss: 0.0115\n",
      "Epoch [46/200], Loss: 0.0108\n",
      "Epoch [47/200], Loss: 0.0102\n",
      "Epoch [48/200], Loss: 0.0096\n",
      "Epoch [49/200], Loss: 0.0091\n",
      "Epoch [50/200], Loss: 0.0087\n",
      "Epoch [51/200], Loss: 0.0082\n",
      "Epoch [52/200], Loss: 0.0079\n",
      "Epoch [53/200], Loss: 0.0075\n",
      "Epoch [54/200], Loss: 0.0072\n",
      "Epoch [55/200], Loss: 0.0070\n",
      "Epoch [56/200], Loss: 0.0067\n",
      "Epoch [57/200], Loss: 0.0065\n",
      "Epoch [58/200], Loss: 0.0063\n",
      "Epoch [59/200], Loss: 0.0061\n",
      "Epoch [60/200], Loss: 0.0059\n",
      "Epoch [61/200], Loss: 0.0057\n",
      "Epoch [62/200], Loss: 0.0055\n",
      "Epoch [63/200], Loss: 0.0053\n",
      "Epoch [64/200], Loss: 0.0051\n",
      "Epoch [65/200], Loss: 0.0050\n",
      "Epoch [66/200], Loss: 0.0048\n",
      "Epoch [67/200], Loss: 0.0046\n",
      "Epoch [68/200], Loss: 0.0045\n",
      "Epoch [69/200], Loss: 0.0043\n",
      "Epoch [70/200], Loss: 0.0042\n",
      "Epoch [71/200], Loss: 0.0040\n",
      "Epoch [72/200], Loss: 0.0039\n",
      "Epoch [73/200], Loss: 0.0038\n",
      "Epoch [74/200], Loss: 0.0037\n",
      "Epoch [75/200], Loss: 0.0035\n",
      "Epoch [76/200], Loss: 0.0034\n",
      "Epoch [77/200], Loss: 0.0033\n",
      "Epoch [78/200], Loss: 0.0032\n",
      "Epoch [79/200], Loss: 0.0031\n",
      "Epoch [80/200], Loss: 0.0030\n",
      "Epoch [81/200], Loss: 0.0029\n",
      "Epoch [82/200], Loss: 0.0029\n",
      "Epoch [83/200], Loss: 0.0028\n",
      "Epoch [84/200], Loss: 0.0027\n",
      "Epoch [85/200], Loss: 0.0027\n",
      "Epoch [86/200], Loss: 0.0026\n",
      "Epoch [87/200], Loss: 0.0026\n",
      "Epoch [88/200], Loss: 0.0026\n",
      "Epoch [89/200], Loss: 0.0026\n",
      "Epoch [90/200], Loss: 0.0025\n",
      "Epoch [91/200], Loss: 0.0024\n",
      "Epoch [92/200], Loss: 0.0023\n",
      "Epoch [93/200], Loss: 0.0022\n",
      "Epoch [94/200], Loss: 0.0022\n",
      "Epoch [95/200], Loss: 0.0021\n",
      "Epoch [96/200], Loss: 0.0020\n",
      "Epoch [97/200], Loss: 0.0019\n",
      "Epoch [98/200], Loss: 0.0019\n",
      "Epoch [99/200], Loss: 0.0018\n",
      "Epoch [100/200], Loss: 0.0017\n",
      "Epoch [101/200], Loss: 0.0017\n",
      "Epoch [102/200], Loss: 0.0016\n",
      "Epoch [103/200], Loss: 0.0016\n",
      "Epoch [104/200], Loss: 0.0015\n",
      "Epoch [105/200], Loss: 0.0015\n",
      "Epoch [106/200], Loss: 0.0014\n",
      "Epoch [107/200], Loss: 0.0014\n",
      "Epoch [108/200], Loss: 0.0013\n",
      "Epoch [109/200], Loss: 0.0013\n",
      "Epoch [110/200], Loss: 0.0013\n",
      "Epoch [111/200], Loss: 0.0012\n",
      "Epoch [112/200], Loss: 0.0012\n",
      "Epoch [113/200], Loss: 0.0012\n",
      "Epoch [114/200], Loss: 0.0012\n",
      "Epoch [115/200], Loss: 0.0011\n",
      "Epoch [116/200], Loss: 0.0011\n",
      "Epoch [117/200], Loss: 0.0011\n",
      "Epoch [118/200], Loss: 0.0010\n",
      "Epoch [119/200], Loss: 0.0010\n",
      "Epoch [120/200], Loss: 0.0010\n",
      "Epoch [121/200], Loss: 0.0010\n",
      "Epoch [122/200], Loss: 0.0010\n",
      "Epoch [123/200], Loss: 0.0009\n",
      "Epoch [124/200], Loss: 0.0009\n",
      "Epoch [125/200], Loss: 0.0009\n",
      "Epoch [126/200], Loss: 0.0009\n",
      "Epoch [127/200], Loss: 0.0009\n",
      "Epoch [128/200], Loss: 0.0009\n",
      "Epoch [129/200], Loss: 0.0008\n",
      "Epoch [130/200], Loss: 0.0008\n",
      "Epoch [131/200], Loss: 0.0008\n",
      "Epoch [132/200], Loss: 0.0008\n",
      "Epoch [133/200], Loss: 0.0008\n",
      "Epoch [134/200], Loss: 0.0008\n",
      "Epoch [135/200], Loss: 0.0008\n",
      "Epoch [136/200], Loss: 0.0007\n",
      "Epoch [137/200], Loss: 0.0007\n",
      "Epoch [138/200], Loss: 0.0007\n",
      "Epoch [139/200], Loss: 0.0007\n",
      "Epoch [140/200], Loss: 0.0007\n",
      "Epoch [141/200], Loss: 0.0007\n",
      "Epoch [142/200], Loss: 0.0007\n",
      "Epoch [143/200], Loss: 0.0007\n",
      "Epoch [144/200], Loss: 0.0007\n",
      "Epoch [145/200], Loss: 0.0007\n",
      "Epoch [146/200], Loss: 0.0007\n",
      "Epoch [147/200], Loss: 0.0007\n",
      "Epoch [148/200], Loss: 0.0007\n",
      "Epoch [149/200], Loss: 0.0007\n",
      "Epoch [150/200], Loss: 0.0007\n",
      "Epoch [151/200], Loss: 0.0007\n",
      "Epoch [152/200], Loss: 0.0007\n",
      "Epoch [153/200], Loss: 0.0007\n",
      "Epoch [154/200], Loss: 0.0007\n",
      "Epoch [155/200], Loss: 0.0007\n",
      "Epoch [156/200], Loss: 0.0007\n",
      "Epoch [157/200], Loss: 0.0008\n",
      "Epoch [158/200], Loss: 0.0008\n",
      "Epoch [159/200], Loss: 0.0008\n",
      "Epoch [160/200], Loss: 0.0008\n",
      "Epoch [161/200], Loss: 0.0008\n",
      "Epoch [162/200], Loss: 0.0008\n",
      "Epoch [163/200], Loss: 0.0008\n",
      "Epoch [164/200], Loss: 0.0008\n",
      "Epoch [165/200], Loss: 0.0008\n",
      "Epoch [166/200], Loss: 0.0009\n",
      "Epoch [167/200], Loss: 0.0009\n",
      "Epoch [168/200], Loss: 0.0009\n",
      "Epoch [169/200], Loss: 0.0009\n",
      "Epoch [170/200], Loss: 0.0009\n",
      "Epoch [171/200], Loss: 0.0009\n",
      "Epoch [172/200], Loss: 0.0010\n",
      "Epoch [173/200], Loss: 0.0010\n",
      "Epoch [174/200], Loss: 0.0010\n",
      "Epoch [175/200], Loss: 0.0010\n",
      "Epoch [176/200], Loss: 0.0010\n",
      "Epoch [177/200], Loss: 0.0010\n",
      "Epoch [178/200], Loss: 0.0011\n",
      "Epoch [179/200], Loss: 0.0011\n",
      "Epoch [180/200], Loss: 0.0011\n",
      "Epoch [181/200], Loss: 0.0011\n",
      "Epoch [182/200], Loss: 0.0011\n",
      "Epoch [183/200], Loss: 0.0011\n",
      "Epoch [184/200], Loss: 0.0011\n",
      "Epoch [185/200], Loss: 0.0011\n",
      "Epoch [186/200], Loss: 0.0011\n",
      "Epoch [187/200], Loss: 0.0011\n",
      "Epoch [188/200], Loss: 0.0011\n",
      "Epoch [189/200], Loss: 0.0011\n",
      "Epoch [190/200], Loss: 0.0011\n",
      "Epoch [191/200], Loss: 0.0011\n",
      "Epoch [192/200], Loss: 0.0011\n",
      "Epoch [193/200], Loss: 0.0010\n",
      "Epoch [194/200], Loss: 0.0010\n",
      "Epoch [195/200], Loss: 0.0010\n",
      "Epoch [196/200], Loss: 0.0010\n",
      "Epoch [197/200], Loss: 0.0009\n",
      "Epoch [198/200], Loss: 0.0009\n",
      "Epoch [199/200], Loss: 0.0009\n",
      "Epoch [200/200], Loss: 0.0009\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/200], Loss: 0.0029\n",
      "Epoch [2/200], Loss: 0.0031\n",
      "Epoch [3/200], Loss: 0.0040\n",
      "Epoch [4/200], Loss: 0.0067\n",
      "Epoch [5/200], Loss: 0.0122\n",
      "Epoch [6/200], Loss: 0.0195\n",
      "Epoch [7/200], Loss: 0.0259\n",
      "Epoch [8/200], Loss: 0.0355\n",
      "Epoch [9/200], Loss: 0.0435\n",
      "Epoch [10/200], Loss: 0.0501\n",
      "Epoch [11/200], Loss: 0.0566\n",
      "Epoch [12/200], Loss: 0.0647\n",
      "Epoch [13/200], Loss: 0.0640\n",
      "Epoch [14/200], Loss: 0.0627\n",
      "Epoch [15/200], Loss: 0.0613\n",
      "Epoch [16/200], Loss: 0.0586\n",
      "Epoch [17/200], Loss: 0.0580\n",
      "Epoch [18/200], Loss: 0.0584\n",
      "Epoch [19/200], Loss: 0.0580\n",
      "Epoch [20/200], Loss: 0.0563\n",
      "Epoch [21/200], Loss: 0.0476\n",
      "Epoch [22/200], Loss: 0.0468\n",
      "Epoch [23/200], Loss: 0.0383\n",
      "Epoch [24/200], Loss: 0.0363\n",
      "Epoch [25/200], Loss: 0.0302\n",
      "Epoch [26/200], Loss: 0.0222\n",
      "Epoch [27/200], Loss: 0.0203\n",
      "Epoch [28/200], Loss: 0.0243\n",
      "Epoch [29/200], Loss: 0.0248\n",
      "Epoch [30/200], Loss: 0.0222\n",
      "Epoch [31/200], Loss: 0.0212\n",
      "Epoch [32/200], Loss: 0.0226\n",
      "Epoch [33/200], Loss: 0.0206\n",
      "Epoch [34/200], Loss: 0.0189\n",
      "Epoch [35/200], Loss: 0.0196\n",
      "Epoch [36/200], Loss: 0.0192\n",
      "Epoch [37/200], Loss: 0.0181\n",
      "Epoch [38/200], Loss: 0.0182\n",
      "Epoch [39/200], Loss: 0.0179\n",
      "Epoch [40/200], Loss: 0.0175\n",
      "Epoch [41/200], Loss: 0.0169\n",
      "Epoch [42/200], Loss: 0.0166\n",
      "Epoch [43/200], Loss: 0.0161\n",
      "Epoch [44/200], Loss: 0.0157\n",
      "Epoch [45/200], Loss: 0.0150\n",
      "Epoch [46/200], Loss: 0.0142\n",
      "Epoch [47/200], Loss: 0.0136\n",
      "Epoch [48/200], Loss: 0.0133\n",
      "Epoch [49/200], Loss: 0.0130\n",
      "Epoch [50/200], Loss: 0.0127\n",
      "Epoch [51/200], Loss: 0.0124\n",
      "Epoch [52/200], Loss: 0.0121\n",
      "Epoch [53/200], Loss: 0.0118\n",
      "Epoch [54/200], Loss: 0.0114\n",
      "Epoch [55/200], Loss: 0.0113\n",
      "Epoch [56/200], Loss: 0.0110\n",
      "Epoch [57/200], Loss: 0.0108\n",
      "Epoch [58/200], Loss: 0.0107\n",
      "Epoch [59/200], Loss: 0.0105\n",
      "Epoch [60/200], Loss: 0.0103\n",
      "Epoch [61/200], Loss: 0.0101\n",
      "Epoch [62/200], Loss: 0.0100\n",
      "Epoch [63/200], Loss: 0.0098\n",
      "Epoch [64/200], Loss: 0.0097\n",
      "Epoch [65/200], Loss: 0.0095\n",
      "Epoch [66/200], Loss: 0.0094\n",
      "Epoch [67/200], Loss: 0.0093\n",
      "Epoch [68/200], Loss: 0.0093\n",
      "Epoch [69/200], Loss: 0.0093\n",
      "Epoch [70/200], Loss: 0.0092\n",
      "Epoch [71/200], Loss: 0.0090\n",
      "Epoch [72/200], Loss: 0.0091\n",
      "Epoch [73/200], Loss: 0.0088\n",
      "Epoch [74/200], Loss: 0.0091\n",
      "Epoch [75/200], Loss: 0.0091\n",
      "Epoch [76/200], Loss: 0.0091\n",
      "Epoch [77/200], Loss: 0.0087\n",
      "Epoch [78/200], Loss: 0.0088\n",
      "Epoch [79/200], Loss: 0.0085\n",
      "Epoch [80/200], Loss: 0.0088\n",
      "Epoch [81/200], Loss: 0.0086\n",
      "Epoch [82/200], Loss: 0.0091\n",
      "Epoch [83/200], Loss: 0.0095\n",
      "Epoch [84/200], Loss: 0.0096\n",
      "Epoch [85/200], Loss: 0.0099\n",
      "Epoch [86/200], Loss: 0.0100\n",
      "Epoch [87/200], Loss: 0.0103\n",
      "Epoch [88/200], Loss: 0.0106\n",
      "Epoch [89/200], Loss: 0.0106\n",
      "Epoch [90/200], Loss: 0.0105\n",
      "Epoch [91/200], Loss: 0.0104\n",
      "Epoch [92/200], Loss: 0.0102\n",
      "Epoch [93/200], Loss: 0.0102\n",
      "Epoch [94/200], Loss: 0.0101\n",
      "Epoch [95/200], Loss: 0.0101\n",
      "Epoch [96/200], Loss: 0.0099\n",
      "Epoch [97/200], Loss: 0.0096\n",
      "Epoch [98/200], Loss: 0.0093\n",
      "Epoch [99/200], Loss: 0.0092\n",
      "Epoch [100/200], Loss: 0.0089\n",
      "Epoch [101/200], Loss: 0.0087\n",
      "Epoch [102/200], Loss: 0.0084\n",
      "Epoch [103/200], Loss: 0.0082\n",
      "Epoch [104/200], Loss: 0.0079\n",
      "Epoch [105/200], Loss: 0.0077\n",
      "Epoch [106/200], Loss: 0.0075\n",
      "Epoch [107/200], Loss: 0.0072\n",
      "Epoch [108/200], Loss: 0.0071\n",
      "Epoch [109/200], Loss: 0.0069\n",
      "Epoch [110/200], Loss: 0.0067\n",
      "Epoch [111/200], Loss: 0.0066\n",
      "Epoch [112/200], Loss: 0.0063\n",
      "Epoch [113/200], Loss: 0.0061\n",
      "Epoch [114/200], Loss: 0.0058\n",
      "Epoch [115/200], Loss: 0.0058\n",
      "Epoch [116/200], Loss: 0.0055\n",
      "Epoch [117/200], Loss: 0.0052\n",
      "Epoch [118/200], Loss: 0.0050\n",
      "Epoch [119/200], Loss: 0.0048\n",
      "Epoch [120/200], Loss: 0.0046\n",
      "Epoch [121/200], Loss: 0.0044\n",
      "Epoch [122/200], Loss: 0.0042\n",
      "Epoch [123/200], Loss: 0.0041\n",
      "Epoch [124/200], Loss: 0.0040\n",
      "Epoch [125/200], Loss: 0.0038\n",
      "Epoch [126/200], Loss: 0.0036\n",
      "Epoch [127/200], Loss: 0.0034\n",
      "Epoch [128/200], Loss: 0.0035\n",
      "Epoch [129/200], Loss: 0.0033\n",
      "Epoch [130/200], Loss: 0.0031\n",
      "Epoch [131/200], Loss: 0.0029\n",
      "Epoch [132/200], Loss: 0.0028\n",
      "Epoch [133/200], Loss: 0.0026\n",
      "Epoch [134/200], Loss: 0.0025\n",
      "Epoch [135/200], Loss: 0.0025\n",
      "Epoch [136/200], Loss: 0.0024\n",
      "Epoch [137/200], Loss: 0.0024\n",
      "Epoch [138/200], Loss: 0.0023\n",
      "Epoch [139/200], Loss: 0.0023\n",
      "Epoch [140/200], Loss: 0.0023\n",
      "Epoch [141/200], Loss: 0.0024\n",
      "Epoch [142/200], Loss: 0.0024\n",
      "Epoch [143/200], Loss: 0.0024\n",
      "Epoch [144/200], Loss: 0.0025\n",
      "Epoch [145/200], Loss: 0.0026\n",
      "Epoch [146/200], Loss: 0.0026\n",
      "Epoch [147/200], Loss: 0.0026\n",
      "Epoch [148/200], Loss: 0.0026\n",
      "Epoch [149/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0026\n",
      "Epoch [151/200], Loss: 0.0028\n",
      "Epoch [152/200], Loss: 0.0029\n",
      "Epoch [153/200], Loss: 0.0032\n",
      "Epoch [154/200], Loss: 0.0034\n",
      "Epoch [155/200], Loss: 0.0035\n",
      "Epoch [156/200], Loss: 0.0036\n",
      "Epoch [157/200], Loss: 0.0038\n",
      "Epoch [158/200], Loss: 0.0040\n",
      "Epoch [159/200], Loss: 0.0041\n",
      "Epoch [160/200], Loss: 0.0042\n",
      "Epoch [161/200], Loss: 0.0042\n",
      "Epoch [162/200], Loss: 0.0043\n",
      "Epoch [163/200], Loss: 0.0042\n",
      "Epoch [164/200], Loss: 0.0040\n",
      "Epoch [165/200], Loss: 0.0039\n",
      "Epoch [166/200], Loss: 0.0038\n",
      "Epoch [167/200], Loss: 0.0039\n",
      "Epoch [168/200], Loss: 0.0037\n",
      "Epoch [169/200], Loss: 0.0036\n",
      "Epoch [170/200], Loss: 0.0034\n",
      "Epoch [171/200], Loss: 0.0034\n",
      "Epoch [172/200], Loss: 0.0032\n",
      "Epoch [173/200], Loss: 0.0031\n",
      "Epoch [174/200], Loss: 0.0030\n",
      "Epoch [175/200], Loss: 0.0030\n",
      "Epoch [176/200], Loss: 0.0029\n",
      "Epoch [177/200], Loss: 0.0030\n",
      "Epoch [178/200], Loss: 0.0029\n",
      "Epoch [179/200], Loss: 0.0029\n",
      "Epoch [180/200], Loss: 0.0029\n",
      "Epoch [181/200], Loss: 0.0028\n",
      "Epoch [182/200], Loss: 0.0027\n",
      "Epoch [183/200], Loss: 0.0027\n",
      "Epoch [184/200], Loss: 0.0028\n",
      "Epoch [185/200], Loss: 0.0028\n",
      "Epoch [186/200], Loss: 0.0027\n",
      "Epoch [187/200], Loss: 0.0028\n",
      "Epoch [188/200], Loss: 0.0028\n",
      "Epoch [189/200], Loss: 0.0030\n",
      "Epoch [190/200], Loss: 0.0027\n",
      "Epoch [191/200], Loss: 0.0028\n",
      "Epoch [192/200], Loss: 0.0028\n",
      "Epoch [193/200], Loss: 0.0029\n",
      "Epoch [194/200], Loss: 0.0028\n",
      "Epoch [195/200], Loss: 0.0026\n",
      "Epoch [196/200], Loss: 0.0026\n",
      "Epoch [197/200], Loss: 0.0026\n",
      "Epoch [198/200], Loss: 0.0025\n",
      "Epoch [199/200], Loss: 0.0025\n",
      "Epoch [200/200], Loss: 0.0025\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/200], Loss: 0.0010\n",
      "Epoch [2/200], Loss: 0.0010\n",
      "Epoch [3/200], Loss: 0.0027\n",
      "Epoch [4/200], Loss: 0.0074\n",
      "Epoch [5/200], Loss: 0.0089\n",
      "Epoch [6/200], Loss: 0.0286\n",
      "Epoch [7/200], Loss: 0.0388\n",
      "Epoch [8/200], Loss: 0.0415\n",
      "Epoch [9/200], Loss: 0.0540\n",
      "Epoch [10/200], Loss: 0.0335\n",
      "Epoch [11/200], Loss: 0.0301\n",
      "Epoch [12/200], Loss: 0.0271\n",
      "Epoch [13/200], Loss: 0.0332\n",
      "Epoch [14/200], Loss: 0.0302\n",
      "Epoch [15/200], Loss: 0.0462\n",
      "Epoch [16/200], Loss: 0.0423\n",
      "Epoch [17/200], Loss: 0.0363\n",
      "Epoch [18/200], Loss: 0.0238\n",
      "Epoch [19/200], Loss: 0.0378\n",
      "Epoch [20/200], Loss: 0.1256\n",
      "Epoch [21/200], Loss: 0.0225\n",
      "Epoch [22/200], Loss: 0.0197\n",
      "Epoch [23/200], Loss: 0.0920\n",
      "Epoch [24/200], Loss: 0.0180\n",
      "Epoch [25/200], Loss: 0.0795\n",
      "Epoch [26/200], Loss: 0.0816\n",
      "Epoch [27/200], Loss: 0.0175\n",
      "Epoch [28/200], Loss: 0.0158\n",
      "Epoch [29/200], Loss: 0.0133\n",
      "Epoch [30/200], Loss: 0.0128\n",
      "Epoch [31/200], Loss: 0.0939\n",
      "Epoch [32/200], Loss: 0.0133\n",
      "Epoch [33/200], Loss: 0.0852\n",
      "Epoch [34/200], Loss: 0.0787\n",
      "Epoch [35/200], Loss: 0.0221\n",
      "Epoch [36/200], Loss: 0.0707\n",
      "Epoch [37/200], Loss: 0.0062\n",
      "Epoch [38/200], Loss: 0.0079\n",
      "Epoch [39/200], Loss: 0.0074\n",
      "Epoch [40/200], Loss: 0.0059\n",
      "Epoch [41/200], Loss: 0.0080\n",
      "Epoch [42/200], Loss: 0.0086\n",
      "Epoch [43/200], Loss: 0.0084\n",
      "Epoch [44/200], Loss: 0.0078\n",
      "Epoch [45/200], Loss: 0.0081\n",
      "Epoch [46/200], Loss: 0.0084\n",
      "Epoch [47/200], Loss: 0.0078\n",
      "Epoch [48/200], Loss: 0.0075\n",
      "Epoch [49/200], Loss: 0.0071\n",
      "Epoch [50/200], Loss: 0.0075\n",
      "Epoch [51/200], Loss: 0.0072\n",
      "Epoch [52/200], Loss: 0.0069\n",
      "Epoch [53/200], Loss: 0.0077\n",
      "Epoch [54/200], Loss: 0.0072\n",
      "Epoch [55/200], Loss: 0.0067\n",
      "Epoch [56/200], Loss: 0.0064\n",
      "Epoch [57/200], Loss: 0.0073\n",
      "Epoch [58/200], Loss: 0.0069\n",
      "Epoch [59/200], Loss: 0.0067\n",
      "Epoch [60/200], Loss: 0.0063\n",
      "Epoch [61/200], Loss: 0.0071\n",
      "Epoch [62/200], Loss: 0.0074\n",
      "Epoch [63/200], Loss: 0.0072\n",
      "Epoch [64/200], Loss: 0.0067\n",
      "Epoch [65/200], Loss: 0.0069\n",
      "Epoch [66/200], Loss: 0.0066\n",
      "Epoch [67/200], Loss: 0.0067\n",
      "Epoch [68/200], Loss: 0.0067\n",
      "Epoch [69/200], Loss: 0.0071\n",
      "Epoch [70/200], Loss: 0.0070\n",
      "Epoch [71/200], Loss: 0.0065\n",
      "Epoch [72/200], Loss: 0.0068\n",
      "Epoch [73/200], Loss: 0.0079\n",
      "Epoch [74/200], Loss: 0.0113\n",
      "Epoch [75/200], Loss: 0.0119\n",
      "Epoch [76/200], Loss: 0.0115\n",
      "Epoch [77/200], Loss: 0.0112\n",
      "Epoch [78/200], Loss: 0.0120\n",
      "Epoch [79/200], Loss: 0.0071\n",
      "Epoch [80/200], Loss: 0.0053\n",
      "Epoch [81/200], Loss: 0.0073\n",
      "Epoch [82/200], Loss: 0.0050\n",
      "Epoch [83/200], Loss: 0.0050\n",
      "Epoch [84/200], Loss: 0.0052\n",
      "Epoch [85/200], Loss: 0.0053\n",
      "Epoch [86/200], Loss: 0.0054\n",
      "Epoch [87/200], Loss: 0.0052\n",
      "Epoch [88/200], Loss: 0.0050\n",
      "Epoch [89/200], Loss: 0.0049\n",
      "Epoch [90/200], Loss: 0.0051\n",
      "Epoch [91/200], Loss: 0.0050\n",
      "Epoch [92/200], Loss: 0.0048\n",
      "Epoch [93/200], Loss: 0.0045\n",
      "Epoch [94/200], Loss: 0.0046\n",
      "Epoch [95/200], Loss: 0.0048\n",
      "Epoch [96/200], Loss: 0.0044\n",
      "Epoch [97/200], Loss: 0.0052\n",
      "Epoch [98/200], Loss: 0.0051\n",
      "Epoch [99/200], Loss: 0.0054\n",
      "Epoch [100/200], Loss: 0.0056\n",
      "Epoch [101/200], Loss: 0.0055\n",
      "Epoch [102/200], Loss: 0.0051\n",
      "Epoch [103/200], Loss: 0.0051\n",
      "Epoch [104/200], Loss: 0.0049\n",
      "Epoch [105/200], Loss: 0.0046\n",
      "Epoch [106/200], Loss: 0.0055\n",
      "Epoch [107/200], Loss: 0.0048\n",
      "Epoch [108/200], Loss: 0.0050\n",
      "Epoch [109/200], Loss: 0.0053\n",
      "Epoch [110/200], Loss: 0.0038\n",
      "Epoch [111/200], Loss: 0.0044\n",
      "Epoch [112/200], Loss: 0.0036\n",
      "Epoch [113/200], Loss: 0.0034\n",
      "Epoch [114/200], Loss: 0.0031\n",
      "Epoch [115/200], Loss: 0.0028\n",
      "Epoch [116/200], Loss: 0.0029\n",
      "Epoch [117/200], Loss: 0.0034\n",
      "Epoch [118/200], Loss: 0.0032\n",
      "Epoch [119/200], Loss: 0.0034\n",
      "Epoch [120/200], Loss: 0.0035\n",
      "Epoch [121/200], Loss: 0.0038\n",
      "Epoch [122/200], Loss: 0.0042\n",
      "Epoch [123/200], Loss: 0.0047\n",
      "Epoch [124/200], Loss: 0.0046\n",
      "Epoch [125/200], Loss: 0.0047\n",
      "Epoch [126/200], Loss: 0.0049\n",
      "Epoch [127/200], Loss: 0.0050\n",
      "Epoch [128/200], Loss: 0.0051\n",
      "Epoch [129/200], Loss: 0.0054\n",
      "Epoch [130/200], Loss: 0.0054\n",
      "Epoch [131/200], Loss: 0.0062\n",
      "Epoch [132/200], Loss: 0.0060\n",
      "Epoch [133/200], Loss: 0.0062\n",
      "Epoch [134/200], Loss: 0.0061\n",
      "Epoch [135/200], Loss: 0.0063\n",
      "Epoch [136/200], Loss: 0.0059\n",
      "Epoch [137/200], Loss: 0.0080\n",
      "Epoch [138/200], Loss: 0.0070\n",
      "Epoch [139/200], Loss: 0.0041\n",
      "Epoch [140/200], Loss: 0.0050\n",
      "Epoch [141/200], Loss: 0.0011\n",
      "Epoch [142/200], Loss: 0.0012\n",
      "Epoch [143/200], Loss: 0.0015\n",
      "Epoch [144/200], Loss: 0.0018\n",
      "Epoch [145/200], Loss: 0.0017\n",
      "Epoch [146/200], Loss: 0.0014\n",
      "Epoch [147/200], Loss: 0.0013\n",
      "Epoch [148/200], Loss: 0.0017\n",
      "Epoch [149/200], Loss: 0.0020\n",
      "Epoch [150/200], Loss: 0.0019\n",
      "Epoch [151/200], Loss: 0.0016\n",
      "Epoch [152/200], Loss: 0.0015\n",
      "Epoch [153/200], Loss: 0.0018\n",
      "Epoch [154/200], Loss: 0.0014\n",
      "Epoch [155/200], Loss: 0.0016\n",
      "Epoch [156/200], Loss: 0.0013\n",
      "Epoch [157/200], Loss: 0.0013\n",
      "Epoch [158/200], Loss: 0.0028\n",
      "Epoch [159/200], Loss: 0.0022\n",
      "Epoch [160/200], Loss: 0.0027\n",
      "Epoch [161/200], Loss: 0.0026\n",
      "Epoch [162/200], Loss: 0.0031\n",
      "Epoch [163/200], Loss: 0.0027\n",
      "Epoch [164/200], Loss: 0.0025\n",
      "Epoch [165/200], Loss: 0.0027\n",
      "Epoch [166/200], Loss: 0.0023\n",
      "Epoch [167/200], Loss: 0.0027\n",
      "Epoch [168/200], Loss: 0.0024\n",
      "Epoch [169/200], Loss: 0.0033\n",
      "Epoch [170/200], Loss: 0.0035\n",
      "Epoch [171/200], Loss: 0.0026\n",
      "Epoch [172/200], Loss: 0.0026\n",
      "Epoch [173/200], Loss: 0.0012\n",
      "Epoch [174/200], Loss: 0.0015\n",
      "Epoch [175/200], Loss: 0.0017\n",
      "Epoch [176/200], Loss: 0.0018\n",
      "Epoch [177/200], Loss: 0.0014\n",
      "Epoch [178/200], Loss: 0.0012\n",
      "Epoch [179/200], Loss: 0.0014\n",
      "Epoch [180/200], Loss: 0.0008\n",
      "Epoch [181/200], Loss: 0.0013\n",
      "Epoch [182/200], Loss: 0.0014\n",
      "Epoch [183/200], Loss: 0.0014\n",
      "Epoch [184/200], Loss: 0.0017\n",
      "Epoch [185/200], Loss: 0.0011\n",
      "Epoch [186/200], Loss: 0.0010\n",
      "Epoch [187/200], Loss: 0.0007\n",
      "Epoch [188/200], Loss: 0.0009\n",
      "Epoch [189/200], Loss: 0.0012\n",
      "Epoch [190/200], Loss: 0.0017\n",
      "Epoch [191/200], Loss: 0.0017\n",
      "Epoch [192/200], Loss: 0.0019\n",
      "Epoch [193/200], Loss: 0.0020\n",
      "Epoch [194/200], Loss: 0.0021\n",
      "Epoch [195/200], Loss: 0.0019\n",
      "Epoch [196/200], Loss: 0.0017\n",
      "Epoch [197/200], Loss: 0.0021\n",
      "Epoch [198/200], Loss: 0.0019\n",
      "Epoch [199/200], Loss: 0.0017\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "Accuracy: 50.7937%\n",
      "Epoch [1/200], Loss: 0.0005\n",
      "Epoch [2/200], Loss: 0.0007\n",
      "Epoch [3/200], Loss: 0.0019\n",
      "Epoch [4/200], Loss: 0.0039\n",
      "Epoch [5/200], Loss: 0.0062\n",
      "Epoch [6/200], Loss: 0.0137\n",
      "Epoch [7/200], Loss: 0.0076\n",
      "Epoch [8/200], Loss: 0.0094\n",
      "Epoch [9/200], Loss: 0.0118\n",
      "Epoch [10/200], Loss: 0.0122\n",
      "Epoch [11/200], Loss: 0.0137\n",
      "Epoch [12/200], Loss: 0.0123\n",
      "Epoch [13/200], Loss: 0.0070\n",
      "Epoch [14/200], Loss: 0.0114\n",
      "Epoch [15/200], Loss: 0.0141\n",
      "Epoch [16/200], Loss: 0.0149\n",
      "Epoch [17/200], Loss: 0.0090\n",
      "Epoch [18/200], Loss: 0.0050\n",
      "Epoch [19/200], Loss: 0.0064\n",
      "Epoch [20/200], Loss: 0.0090\n",
      "Epoch [21/200], Loss: 0.0050\n",
      "Epoch [22/200], Loss: 0.0125\n",
      "Epoch [23/200], Loss: 0.0009\n",
      "Epoch [24/200], Loss: 0.0007\n",
      "Epoch [25/200], Loss: 0.0001\n",
      "Epoch [26/200], Loss: 0.0005\n",
      "Epoch [27/200], Loss: 0.0012\n",
      "Epoch [28/200], Loss: 0.0029\n",
      "Epoch [29/200], Loss: 0.0010\n",
      "Epoch [30/200], Loss: 0.0032\n",
      "Epoch [31/200], Loss: 0.0036\n",
      "Epoch [32/200], Loss: 0.0018\n",
      "Epoch [33/200], Loss: 0.0082\n",
      "Epoch [34/200], Loss: 0.0015\n",
      "Epoch [35/200], Loss: 0.0007\n",
      "Epoch [36/200], Loss: 0.0050\n",
      "Epoch [37/200], Loss: 0.0049\n",
      "Epoch [38/200], Loss: 0.0003\n",
      "Epoch [39/200], Loss: 0.0022\n",
      "Epoch [40/200], Loss: 0.0019\n",
      "Epoch [41/200], Loss: 0.0002\n",
      "Epoch [42/200], Loss: 0.0061\n",
      "Epoch [43/200], Loss: 0.0004\n",
      "Epoch [44/200], Loss: 0.0002\n",
      "Epoch [45/200], Loss: 0.0009\n",
      "Epoch [46/200], Loss: 0.0035\n",
      "Epoch [47/200], Loss: 0.0001\n",
      "Epoch [48/200], Loss: 0.0004\n",
      "Epoch [49/200], Loss: 0.0007\n",
      "Epoch [50/200], Loss: 0.0005\n",
      "Epoch [51/200], Loss: 0.0007\n",
      "Epoch [52/200], Loss: 0.0015\n",
      "Epoch [53/200], Loss: 0.0030\n",
      "Epoch [54/200], Loss: 0.0021\n",
      "Epoch [55/200], Loss: 0.0030\n",
      "Epoch [56/200], Loss: 0.0014\n",
      "Epoch [57/200], Loss: 0.0023\n",
      "Epoch [58/200], Loss: 0.0032\n",
      "Epoch [59/200], Loss: 0.0024\n",
      "Epoch [60/200], Loss: 0.0016\n",
      "Epoch [61/200], Loss: 0.0012\n",
      "Epoch [62/200], Loss: 0.0001\n",
      "Epoch [63/200], Loss: 0.0010\n",
      "Epoch [64/200], Loss: 0.0009\n",
      "Epoch [65/200], Loss: 0.0012\n",
      "Epoch [66/200], Loss: 0.0013\n",
      "Epoch [67/200], Loss: 0.0015\n",
      "Epoch [68/200], Loss: 0.0014\n",
      "Epoch [69/200], Loss: 0.0016\n",
      "Epoch [70/200], Loss: 0.0015\n",
      "Epoch [71/200], Loss: 0.0017\n",
      "Epoch [72/200], Loss: 0.0011\n",
      "Epoch [73/200], Loss: 0.0010\n",
      "Epoch [74/200], Loss: 0.0008\n",
      "Epoch [75/200], Loss: 0.0009\n",
      "Epoch [76/200], Loss: 0.0012\n",
      "Epoch [77/200], Loss: 0.0002\n",
      "Epoch [78/200], Loss: 0.0005\n",
      "Epoch [79/200], Loss: 0.0005\n",
      "Epoch [80/200], Loss: 0.0006\n",
      "Epoch [81/200], Loss: 0.0007\n",
      "Epoch [82/200], Loss: 0.0008\n",
      "Epoch [83/200], Loss: 0.0007\n",
      "Epoch [84/200], Loss: 0.0008\n",
      "Epoch [85/200], Loss: 0.0009\n",
      "Epoch [86/200], Loss: 0.0009\n",
      "Epoch [87/200], Loss: 0.0010\n",
      "Epoch [88/200], Loss: 0.0010\n",
      "Epoch [89/200], Loss: 0.0011\n",
      "Epoch [90/200], Loss: 0.0011\n",
      "Epoch [91/200], Loss: 0.0012\n",
      "Epoch [92/200], Loss: 0.0013\n",
      "Epoch [93/200], Loss: 0.0014\n",
      "Epoch [94/200], Loss: 0.0015\n",
      "Epoch [95/200], Loss: 0.0016\n",
      "Epoch [96/200], Loss: 0.0017\n",
      "Epoch [97/200], Loss: 0.0017\n",
      "Epoch [98/200], Loss: 0.0017\n",
      "Epoch [99/200], Loss: 0.0018\n",
      "Epoch [100/200], Loss: 0.0017\n",
      "Epoch [101/200], Loss: 0.0017\n",
      "Epoch [102/200], Loss: 0.0017\n",
      "Epoch [103/200], Loss: 0.0016\n",
      "Epoch [104/200], Loss: 0.0016\n",
      "Epoch [105/200], Loss: 0.0016\n",
      "Epoch [106/200], Loss: 0.0016\n",
      "Epoch [107/200], Loss: 0.0016\n",
      "Epoch [108/200], Loss: 0.0016\n",
      "Epoch [109/200], Loss: 0.0016\n",
      "Epoch [110/200], Loss: 0.0013\n",
      "Epoch [111/200], Loss: 0.0014\n",
      "Epoch [112/200], Loss: 0.0013\n",
      "Epoch [113/200], Loss: 0.0013\n",
      "Epoch [114/200], Loss: 0.0013\n",
      "Epoch [115/200], Loss: 0.0014\n",
      "Epoch [116/200], Loss: 0.0014\n",
      "Epoch [117/200], Loss: 0.0014\n",
      "Epoch [118/200], Loss: 0.0015\n",
      "Epoch [119/200], Loss: 0.0015\n",
      "Epoch [120/200], Loss: 0.0016\n",
      "Epoch [121/200], Loss: 0.0017\n",
      "Epoch [122/200], Loss: 0.0018\n",
      "Epoch [123/200], Loss: 0.0019\n",
      "Epoch [124/200], Loss: 0.0019\n",
      "Epoch [125/200], Loss: 0.0020\n",
      "Epoch [126/200], Loss: 0.0021\n",
      "Epoch [127/200], Loss: 0.0023\n",
      "Epoch [128/200], Loss: 0.0023\n",
      "Epoch [129/200], Loss: 0.0024\n",
      "Epoch [130/200], Loss: 0.0025\n",
      "Epoch [131/200], Loss: 0.0026\n",
      "Epoch [132/200], Loss: 0.0027\n",
      "Epoch [133/200], Loss: 0.0029\n",
      "Epoch [134/200], Loss: 0.0030\n",
      "Epoch [135/200], Loss: 0.0032\n",
      "Epoch [136/200], Loss: 0.0035\n",
      "Epoch [137/200], Loss: 0.0037\n",
      "Epoch [138/200], Loss: 0.0037\n",
      "Epoch [139/200], Loss: 0.0038\n",
      "Epoch [140/200], Loss: 0.0038\n",
      "Epoch [141/200], Loss: 0.0035\n",
      "Epoch [142/200], Loss: 0.0035\n",
      "Epoch [143/200], Loss: 0.0034\n",
      "Epoch [144/200], Loss: 0.0033\n",
      "Epoch [145/200], Loss: 0.0035\n",
      "Epoch [146/200], Loss: 0.0034\n",
      "Epoch [147/200], Loss: 0.0035\n",
      "Epoch [148/200], Loss: 0.0036\n",
      "Epoch [149/200], Loss: 0.0030\n",
      "Epoch [150/200], Loss: 0.0034\n",
      "Epoch [151/200], Loss: 0.0027\n",
      "Epoch [152/200], Loss: 0.0027\n",
      "Epoch [153/200], Loss: 0.0026\n",
      "Epoch [154/200], Loss: 0.0027\n",
      "Epoch [155/200], Loss: 0.0023\n",
      "Epoch [156/200], Loss: 0.0020\n",
      "Epoch [157/200], Loss: 0.0019\n",
      "Epoch [158/200], Loss: 0.0015\n",
      "Epoch [159/200], Loss: 0.0014\n",
      "Epoch [160/200], Loss: 0.0013\n",
      "Epoch [161/200], Loss: 0.0011\n",
      "Epoch [162/200], Loss: 0.0010\n",
      "Epoch [163/200], Loss: 0.0007\n",
      "Epoch [164/200], Loss: 0.0007\n",
      "Epoch [165/200], Loss: 0.0006\n",
      "Epoch [166/200], Loss: 0.0006\n",
      "Epoch [167/200], Loss: 0.0006\n",
      "Epoch [168/200], Loss: 0.0006\n",
      "Epoch [169/200], Loss: 0.0005\n",
      "Epoch [170/200], Loss: 0.0005\n",
      "Epoch [171/200], Loss: 0.0005\n",
      "Epoch [172/200], Loss: 0.0005\n",
      "Epoch [173/200], Loss: 0.0005\n",
      "Epoch [174/200], Loss: 0.0005\n",
      "Epoch [175/200], Loss: 0.0005\n",
      "Epoch [176/200], Loss: 0.0005\n",
      "Epoch [177/200], Loss: 0.0005\n",
      "Epoch [178/200], Loss: 0.0005\n",
      "Epoch [179/200], Loss: 0.0004\n",
      "Epoch [180/200], Loss: 0.0004\n",
      "Epoch [181/200], Loss: 0.0004\n",
      "Epoch [182/200], Loss: 0.0004\n",
      "Epoch [183/200], Loss: 0.0004\n",
      "Epoch [184/200], Loss: 0.0004\n",
      "Epoch [185/200], Loss: 0.0004\n",
      "Epoch [186/200], Loss: 0.0004\n",
      "Epoch [187/200], Loss: 0.0004\n",
      "Epoch [188/200], Loss: 0.0004\n",
      "Epoch [189/200], Loss: 0.0003\n",
      "Epoch [190/200], Loss: 0.0004\n",
      "Epoch [191/200], Loss: 0.0004\n",
      "Epoch [192/200], Loss: 0.0003\n",
      "Epoch [193/200], Loss: 0.0003\n",
      "Epoch [194/200], Loss: 0.0003\n",
      "Epoch [195/200], Loss: 0.0004\n",
      "Epoch [196/200], Loss: 0.0004\n",
      "Epoch [197/200], Loss: 0.0005\n",
      "Epoch [198/200], Loss: 0.0005\n",
      "Epoch [199/200], Loss: 0.0005\n",
      "Epoch [200/200], Loss: 0.0006\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/400], Loss: 0.7423\n",
      "Epoch [2/400], Loss: 0.6559\n",
      "Epoch [3/400], Loss: 0.6098\n",
      "Epoch [4/400], Loss: 0.5791\n",
      "Epoch [5/400], Loss: 0.5567\n",
      "Epoch [6/400], Loss: 0.5401\n",
      "Epoch [7/400], Loss: 0.5281\n",
      "Epoch [8/400], Loss: 0.5193\n",
      "Epoch [9/400], Loss: 0.5130\n",
      "Epoch [10/400], Loss: 0.5083\n",
      "Epoch [11/400], Loss: 0.5049\n",
      "Epoch [12/400], Loss: 0.5024\n",
      "Epoch [13/400], Loss: 0.5007\n",
      "Epoch [14/400], Loss: 0.4995\n",
      "Epoch [15/400], Loss: 0.4986\n",
      "Epoch [16/400], Loss: 0.4979\n",
      "Epoch [17/400], Loss: 0.4972\n",
      "Epoch [18/400], Loss: 0.4964\n",
      "Epoch [19/400], Loss: 0.4953\n",
      "Epoch [20/400], Loss: 0.4938\n",
      "Epoch [21/400], Loss: 0.4918\n",
      "Epoch [22/400], Loss: 0.4892\n",
      "Epoch [23/400], Loss: 0.4859\n",
      "Epoch [24/400], Loss: 0.4818\n",
      "Epoch [25/400], Loss: 0.4768\n",
      "Epoch [26/400], Loss: 0.4710\n",
      "Epoch [27/400], Loss: 0.4644\n",
      "Epoch [28/400], Loss: 0.4568\n",
      "Epoch [29/400], Loss: 0.4485\n",
      "Epoch [30/400], Loss: 0.4394\n",
      "Epoch [31/400], Loss: 0.4296\n",
      "Epoch [32/400], Loss: 0.4193\n",
      "Epoch [33/400], Loss: 0.4086\n",
      "Epoch [34/400], Loss: 0.3976\n",
      "Epoch [35/400], Loss: 0.3863\n",
      "Epoch [36/400], Loss: 0.3750\n",
      "Epoch [37/400], Loss: 0.3637\n",
      "Epoch [38/400], Loss: 0.3526\n",
      "Epoch [39/400], Loss: 0.3417\n",
      "Epoch [40/400], Loss: 0.3310\n",
      "Epoch [41/400], Loss: 0.3207\n",
      "Epoch [42/400], Loss: 0.3106\n",
      "Epoch [43/400], Loss: 0.3008\n",
      "Epoch [44/400], Loss: 0.2912\n",
      "Epoch [45/400], Loss: 0.2820\n",
      "Epoch [46/400], Loss: 0.2732\n",
      "Epoch [47/400], Loss: 0.2646\n",
      "Epoch [48/400], Loss: 0.2564\n",
      "Epoch [49/400], Loss: 0.2485\n",
      "Epoch [50/400], Loss: 0.2409\n",
      "Epoch [51/400], Loss: 0.2335\n",
      "Epoch [52/400], Loss: 0.2262\n",
      "Epoch [53/400], Loss: 0.2191\n",
      "Epoch [54/400], Loss: 0.2123\n",
      "Epoch [55/400], Loss: 0.2057\n",
      "Epoch [56/400], Loss: 0.1992\n",
      "Epoch [57/400], Loss: 0.1930\n",
      "Epoch [58/400], Loss: 0.1870\n",
      "Epoch [59/400], Loss: 0.1812\n",
      "Epoch [60/400], Loss: 0.1755\n",
      "Epoch [61/400], Loss: 0.1701\n",
      "Epoch [62/400], Loss: 0.1649\n",
      "Epoch [63/400], Loss: 0.1598\n",
      "Epoch [64/400], Loss: 0.1549\n",
      "Epoch [65/400], Loss: 0.1502\n",
      "Epoch [66/400], Loss: 0.1456\n",
      "Epoch [67/400], Loss: 0.1412\n",
      "Epoch [68/400], Loss: 0.1369\n",
      "Epoch [69/400], Loss: 0.1327\n",
      "Epoch [70/400], Loss: 0.1288\n",
      "Epoch [71/400], Loss: 0.1249\n",
      "Epoch [72/400], Loss: 0.1212\n",
      "Epoch [73/400], Loss: 0.1177\n",
      "Epoch [74/400], Loss: 0.1143\n",
      "Epoch [75/400], Loss: 0.1110\n",
      "Epoch [76/400], Loss: 0.1079\n",
      "Epoch [77/400], Loss: 0.1048\n",
      "Epoch [78/400], Loss: 0.1019\n",
      "Epoch [79/400], Loss: 0.0990\n",
      "Epoch [80/400], Loss: 0.0963\n",
      "Epoch [81/400], Loss: 0.0936\n",
      "Epoch [82/400], Loss: 0.0910\n",
      "Epoch [83/400], Loss: 0.0885\n",
      "Epoch [84/400], Loss: 0.0861\n",
      "Epoch [85/400], Loss: 0.0838\n",
      "Epoch [86/400], Loss: 0.0815\n",
      "Epoch [87/400], Loss: 0.0793\n",
      "Epoch [88/400], Loss: 0.0771\n",
      "Epoch [89/400], Loss: 0.0750\n",
      "Epoch [90/400], Loss: 0.0730\n",
      "Epoch [91/400], Loss: 0.0709\n",
      "Epoch [92/400], Loss: 0.0689\n",
      "Epoch [93/400], Loss: 0.0670\n",
      "Epoch [94/400], Loss: 0.0651\n",
      "Epoch [95/400], Loss: 0.0632\n",
      "Epoch [96/400], Loss: 0.0614\n",
      "Epoch [97/400], Loss: 0.0597\n",
      "Epoch [98/400], Loss: 0.0580\n",
      "Epoch [99/400], Loss: 0.0563\n",
      "Epoch [100/400], Loss: 0.0547\n",
      "Epoch [101/400], Loss: 0.0531\n",
      "Epoch [102/400], Loss: 0.0516\n",
      "Epoch [103/400], Loss: 0.0501\n",
      "Epoch [104/400], Loss: 0.0486\n",
      "Epoch [105/400], Loss: 0.0472\n",
      "Epoch [106/400], Loss: 0.0458\n",
      "Epoch [107/400], Loss: 0.0445\n",
      "Epoch [108/400], Loss: 0.0432\n",
      "Epoch [109/400], Loss: 0.0420\n",
      "Epoch [110/400], Loss: 0.0407\n",
      "Epoch [111/400], Loss: 0.0395\n",
      "Epoch [112/400], Loss: 0.0384\n",
      "Epoch [113/400], Loss: 0.0373\n",
      "Epoch [114/400], Loss: 0.0362\n",
      "Epoch [115/400], Loss: 0.0352\n",
      "Epoch [116/400], Loss: 0.0342\n",
      "Epoch [117/400], Loss: 0.0332\n",
      "Epoch [118/400], Loss: 0.0323\n",
      "Epoch [119/400], Loss: 0.0314\n",
      "Epoch [120/400], Loss: 0.0305\n",
      "Epoch [121/400], Loss: 0.0296\n",
      "Epoch [122/400], Loss: 0.0288\n",
      "Epoch [123/400], Loss: 0.0280\n",
      "Epoch [124/400], Loss: 0.0272\n",
      "Epoch [125/400], Loss: 0.0264\n",
      "Epoch [126/400], Loss: 0.0257\n",
      "Epoch [127/400], Loss: 0.0250\n",
      "Epoch [128/400], Loss: 0.0244\n",
      "Epoch [129/400], Loss: 0.0237\n",
      "Epoch [130/400], Loss: 0.0231\n",
      "Epoch [131/400], Loss: 0.0225\n",
      "Epoch [132/400], Loss: 0.0219\n",
      "Epoch [133/400], Loss: 0.0213\n",
      "Epoch [134/400], Loss: 0.0207\n",
      "Epoch [135/400], Loss: 0.0202\n",
      "Epoch [136/400], Loss: 0.0197\n",
      "Epoch [137/400], Loss: 0.0191\n",
      "Epoch [138/400], Loss: 0.0186\n",
      "Epoch [139/400], Loss: 0.0182\n",
      "Epoch [140/400], Loss: 0.0177\n",
      "Epoch [141/400], Loss: 0.0172\n",
      "Epoch [142/400], Loss: 0.0168\n",
      "Epoch [143/400], Loss: 0.0164\n",
      "Epoch [144/400], Loss: 0.0160\n",
      "Epoch [145/400], Loss: 0.0155\n",
      "Epoch [146/400], Loss: 0.0152\n",
      "Epoch [147/400], Loss: 0.0148\n",
      "Epoch [148/400], Loss: 0.0144\n",
      "Epoch [149/400], Loss: 0.0140\n",
      "Epoch [150/400], Loss: 0.0137\n",
      "Epoch [151/400], Loss: 0.0134\n",
      "Epoch [152/400], Loss: 0.0130\n",
      "Epoch [153/400], Loss: 0.0127\n",
      "Epoch [154/400], Loss: 0.0124\n",
      "Epoch [155/400], Loss: 0.0121\n",
      "Epoch [156/400], Loss: 0.0118\n",
      "Epoch [157/400], Loss: 0.0115\n",
      "Epoch [158/400], Loss: 0.0113\n",
      "Epoch [159/400], Loss: 0.0110\n",
      "Epoch [160/400], Loss: 0.0107\n",
      "Epoch [161/400], Loss: 0.0105\n",
      "Epoch [162/400], Loss: 0.0102\n",
      "Epoch [163/400], Loss: 0.0100\n",
      "Epoch [164/400], Loss: 0.0098\n",
      "Epoch [165/400], Loss: 0.0095\n",
      "Epoch [166/400], Loss: 0.0093\n",
      "Epoch [167/400], Loss: 0.0091\n",
      "Epoch [168/400], Loss: 0.0089\n",
      "Epoch [169/400], Loss: 0.0087\n",
      "Epoch [170/400], Loss: 0.0085\n",
      "Epoch [171/400], Loss: 0.0083\n",
      "Epoch [172/400], Loss: 0.0081\n",
      "Epoch [173/400], Loss: 0.0080\n",
      "Epoch [174/400], Loss: 0.0078\n",
      "Epoch [175/400], Loss: 0.0076\n",
      "Epoch [176/400], Loss: 0.0075\n",
      "Epoch [177/400], Loss: 0.0073\n",
      "Epoch [178/400], Loss: 0.0072\n",
      "Epoch [179/400], Loss: 0.0070\n",
      "Epoch [180/400], Loss: 0.0069\n",
      "Epoch [181/400], Loss: 0.0068\n",
      "Epoch [182/400], Loss: 0.0066\n",
      "Epoch [183/400], Loss: 0.0065\n",
      "Epoch [184/400], Loss: 0.0064\n",
      "Epoch [185/400], Loss: 0.0063\n",
      "Epoch [186/400], Loss: 0.0062\n",
      "Epoch [187/400], Loss: 0.0061\n",
      "Epoch [188/400], Loss: 0.0059\n",
      "Epoch [189/400], Loss: 0.0058\n",
      "Epoch [190/400], Loss: 0.0057\n",
      "Epoch [191/400], Loss: 0.0056\n",
      "Epoch [192/400], Loss: 0.0055\n",
      "Epoch [193/400], Loss: 0.0054\n",
      "Epoch [194/400], Loss: 0.0053\n",
      "Epoch [195/400], Loss: 0.0053\n",
      "Epoch [196/400], Loss: 0.0052\n",
      "Epoch [197/400], Loss: 0.0051\n",
      "Epoch [198/400], Loss: 0.0050\n",
      "Epoch [199/400], Loss: 0.0049\n",
      "Epoch [200/400], Loss: 0.0048\n",
      "Epoch [201/400], Loss: 0.0048\n",
      "Epoch [202/400], Loss: 0.0047\n",
      "Epoch [203/400], Loss: 0.0046\n",
      "Epoch [204/400], Loss: 0.0046\n",
      "Epoch [205/400], Loss: 0.0045\n",
      "Epoch [206/400], Loss: 0.0044\n",
      "Epoch [207/400], Loss: 0.0044\n",
      "Epoch [208/400], Loss: 0.0043\n",
      "Epoch [209/400], Loss: 0.0042\n",
      "Epoch [210/400], Loss: 0.0042\n",
      "Epoch [211/400], Loss: 0.0041\n",
      "Epoch [212/400], Loss: 0.0041\n",
      "Epoch [213/400], Loss: 0.0040\n",
      "Epoch [214/400], Loss: 0.0039\n",
      "Epoch [215/400], Loss: 0.0039\n",
      "Epoch [216/400], Loss: 0.0038\n",
      "Epoch [217/400], Loss: 0.0038\n",
      "Epoch [218/400], Loss: 0.0038\n",
      "Epoch [219/400], Loss: 0.0037\n",
      "Epoch [220/400], Loss: 0.0037\n",
      "Epoch [221/400], Loss: 0.0036\n",
      "Epoch [222/400], Loss: 0.0036\n",
      "Epoch [223/400], Loss: 0.0035\n",
      "Epoch [224/400], Loss: 0.0035\n",
      "Epoch [225/400], Loss: 0.0035\n",
      "Epoch [226/400], Loss: 0.0034\n",
      "Epoch [227/400], Loss: 0.0034\n",
      "Epoch [228/400], Loss: 0.0034\n",
      "Epoch [229/400], Loss: 0.0033\n",
      "Epoch [230/400], Loss: 0.0033\n",
      "Epoch [231/400], Loss: 0.0033\n",
      "Epoch [232/400], Loss: 0.0032\n",
      "Epoch [233/400], Loss: 0.0032\n",
      "Epoch [234/400], Loss: 0.0032\n",
      "Epoch [235/400], Loss: 0.0031\n",
      "Epoch [236/400], Loss: 0.0031\n",
      "Epoch [237/400], Loss: 0.0031\n",
      "Epoch [238/400], Loss: 0.0031\n",
      "Epoch [239/400], Loss: 0.0030\n",
      "Epoch [240/400], Loss: 0.0030\n",
      "Epoch [241/400], Loss: 0.0030\n",
      "Epoch [242/400], Loss: 0.0030\n",
      "Epoch [243/400], Loss: 0.0029\n",
      "Epoch [244/400], Loss: 0.0029\n",
      "Epoch [245/400], Loss: 0.0029\n",
      "Epoch [246/400], Loss: 0.0029\n",
      "Epoch [247/400], Loss: 0.0028\n",
      "Epoch [248/400], Loss: 0.0028\n",
      "Epoch [249/400], Loss: 0.0028\n",
      "Epoch [250/400], Loss: 0.0028\n",
      "Epoch [251/400], Loss: 0.0028\n",
      "Epoch [252/400], Loss: 0.0027\n",
      "Epoch [253/400], Loss: 0.0027\n",
      "Epoch [254/400], Loss: 0.0027\n",
      "Epoch [255/400], Loss: 0.0027\n",
      "Epoch [256/400], Loss: 0.0027\n",
      "Epoch [257/400], Loss: 0.0026\n",
      "Epoch [258/400], Loss: 0.0026\n",
      "Epoch [259/400], Loss: 0.0026\n",
      "Epoch [260/400], Loss: 0.0026\n",
      "Epoch [261/400], Loss: 0.0026\n",
      "Epoch [262/400], Loss: 0.0026\n",
      "Epoch [263/400], Loss: 0.0026\n",
      "Epoch [264/400], Loss: 0.0026\n",
      "Epoch [265/400], Loss: 0.0025\n",
      "Epoch [266/400], Loss: 0.0025\n",
      "Epoch [267/400], Loss: 0.0025\n",
      "Epoch [268/400], Loss: 0.0025\n",
      "Epoch [269/400], Loss: 0.0025\n",
      "Epoch [270/400], Loss: 0.0025\n",
      "Epoch [271/400], Loss: 0.0025\n",
      "Epoch [272/400], Loss: 0.0025\n",
      "Epoch [273/400], Loss: 0.0025\n",
      "Epoch [274/400], Loss: 0.0025\n",
      "Epoch [275/400], Loss: 0.0025\n",
      "Epoch [276/400], Loss: 0.0025\n",
      "Epoch [277/400], Loss: 0.0024\n",
      "Epoch [278/400], Loss: 0.0024\n",
      "Epoch [279/400], Loss: 0.0024\n",
      "Epoch [280/400], Loss: 0.0024\n",
      "Epoch [281/400], Loss: 0.0024\n",
      "Epoch [282/400], Loss: 0.0024\n",
      "Epoch [283/400], Loss: 0.0024\n",
      "Epoch [284/400], Loss: 0.0024\n",
      "Epoch [285/400], Loss: 0.0024\n",
      "Epoch [286/400], Loss: 0.0024\n",
      "Epoch [287/400], Loss: 0.0024\n",
      "Epoch [288/400], Loss: 0.0024\n",
      "Epoch [289/400], Loss: 0.0024\n",
      "Epoch [290/400], Loss: 0.0024\n",
      "Epoch [291/400], Loss: 0.0024\n",
      "Epoch [292/400], Loss: 0.0024\n",
      "Epoch [293/400], Loss: 0.0024\n",
      "Epoch [294/400], Loss: 0.0024\n",
      "Epoch [295/400], Loss: 0.0024\n",
      "Epoch [296/400], Loss: 0.0024\n",
      "Epoch [297/400], Loss: 0.0024\n",
      "Epoch [298/400], Loss: 0.0024\n",
      "Epoch [299/400], Loss: 0.0024\n",
      "Epoch [300/400], Loss: 0.0024\n",
      "Epoch [301/400], Loss: 0.0025\n",
      "Epoch [302/400], Loss: 0.0025\n",
      "Epoch [303/400], Loss: 0.0025\n",
      "Epoch [304/400], Loss: 0.0025\n",
      "Epoch [305/400], Loss: 0.0025\n",
      "Epoch [306/400], Loss: 0.0025\n",
      "Epoch [307/400], Loss: 0.0025\n",
      "Epoch [308/400], Loss: 0.0025\n",
      "Epoch [309/400], Loss: 0.0025\n",
      "Epoch [310/400], Loss: 0.0026\n",
      "Epoch [311/400], Loss: 0.0026\n",
      "Epoch [312/400], Loss: 0.0026\n",
      "Epoch [313/400], Loss: 0.0026\n",
      "Epoch [314/400], Loss: 0.0026\n",
      "Epoch [315/400], Loss: 0.0026\n",
      "Epoch [316/400], Loss: 0.0027\n",
      "Epoch [317/400], Loss: 0.0027\n",
      "Epoch [318/400], Loss: 0.0027\n",
      "Epoch [319/400], Loss: 0.0027\n",
      "Epoch [320/400], Loss: 0.0027\n",
      "Epoch [321/400], Loss: 0.0028\n",
      "Epoch [322/400], Loss: 0.0028\n",
      "Epoch [323/400], Loss: 0.0028\n",
      "Epoch [324/400], Loss: 0.0028\n",
      "Epoch [325/400], Loss: 0.0029\n",
      "Epoch [326/400], Loss: 0.0029\n",
      "Epoch [327/400], Loss: 0.0029\n",
      "Epoch [328/400], Loss: 0.0029\n",
      "Epoch [329/400], Loss: 0.0030\n",
      "Epoch [330/400], Loss: 0.0030\n",
      "Epoch [331/400], Loss: 0.0030\n",
      "Epoch [332/400], Loss: 0.0030\n",
      "Epoch [333/400], Loss: 0.0031\n",
      "Epoch [334/400], Loss: 0.0031\n",
      "Epoch [335/400], Loss: 0.0031\n",
      "Epoch [336/400], Loss: 0.0032\n",
      "Epoch [337/400], Loss: 0.0032\n",
      "Epoch [338/400], Loss: 0.0032\n",
      "Epoch [339/400], Loss: 0.0032\n",
      "Epoch [340/400], Loss: 0.0033\n",
      "Epoch [341/400], Loss: 0.0033\n",
      "Epoch [342/400], Loss: 0.0033\n",
      "Epoch [343/400], Loss: 0.0034\n",
      "Epoch [344/400], Loss: 0.0034\n",
      "Epoch [345/400], Loss: 0.0034\n",
      "Epoch [346/400], Loss: 0.0034\n",
      "Epoch [347/400], Loss: 0.0035\n",
      "Epoch [348/400], Loss: 0.0035\n",
      "Epoch [349/400], Loss: 0.0035\n",
      "Epoch [350/400], Loss: 0.0036\n",
      "Epoch [351/400], Loss: 0.0036\n",
      "Epoch [352/400], Loss: 0.0036\n",
      "Epoch [353/400], Loss: 0.0037\n",
      "Epoch [354/400], Loss: 0.0037\n",
      "Epoch [355/400], Loss: 0.0037\n",
      "Epoch [356/400], Loss: 0.0037\n",
      "Epoch [357/400], Loss: 0.0038\n",
      "Epoch [358/400], Loss: 0.0038\n",
      "Epoch [359/400], Loss: 0.0038\n",
      "Epoch [360/400], Loss: 0.0039\n",
      "Epoch [361/400], Loss: 0.0039\n",
      "Epoch [362/400], Loss: 0.0039\n",
      "Epoch [363/400], Loss: 0.0040\n",
      "Epoch [364/400], Loss: 0.0040\n",
      "Epoch [365/400], Loss: 0.0040\n",
      "Epoch [366/400], Loss: 0.0041\n",
      "Epoch [367/400], Loss: 0.0041\n",
      "Epoch [368/400], Loss: 0.0041\n",
      "Epoch [369/400], Loss: 0.0042\n",
      "Epoch [370/400], Loss: 0.0042\n",
      "Epoch [371/400], Loss: 0.0042\n",
      "Epoch [372/400], Loss: 0.0043\n",
      "Epoch [373/400], Loss: 0.0043\n",
      "Epoch [374/400], Loss: 0.0043\n",
      "Epoch [375/400], Loss: 0.0044\n",
      "Epoch [376/400], Loss: 0.0044\n",
      "Epoch [377/400], Loss: 0.0044\n",
      "Epoch [378/400], Loss: 0.0045\n",
      "Epoch [379/400], Loss: 0.0045\n",
      "Epoch [380/400], Loss: 0.0045\n",
      "Epoch [381/400], Loss: 0.0046\n",
      "Epoch [382/400], Loss: 0.0046\n",
      "Epoch [383/400], Loss: 0.0046\n",
      "Epoch [384/400], Loss: 0.0047\n",
      "Epoch [385/400], Loss: 0.0047\n",
      "Epoch [386/400], Loss: 0.0047\n",
      "Epoch [387/400], Loss: 0.0048\n",
      "Epoch [388/400], Loss: 0.0048\n",
      "Epoch [389/400], Loss: 0.0048\n",
      "Epoch [390/400], Loss: 0.0049\n",
      "Epoch [391/400], Loss: 0.0049\n",
      "Epoch [392/400], Loss: 0.0049\n",
      "Epoch [393/400], Loss: 0.0050\n",
      "Epoch [394/400], Loss: 0.0050\n",
      "Epoch [395/400], Loss: 0.0050\n",
      "Epoch [396/400], Loss: 0.0050\n",
      "Epoch [397/400], Loss: 0.0051\n",
      "Epoch [398/400], Loss: 0.0051\n",
      "Epoch [399/400], Loss: 0.0051\n",
      "Epoch [400/400], Loss: 0.0051\n",
      "Accuracy: 49.6032%\n",
      "Epoch [1/400], Loss: 0.2868\n",
      "Epoch [2/400], Loss: 0.2794\n",
      "Epoch [3/400], Loss: 0.2819\n",
      "Epoch [4/400], Loss: 0.2835\n",
      "Epoch [5/400], Loss: 0.2840\n",
      "Epoch [6/400], Loss: 0.2840\n",
      "Epoch [7/400], Loss: 0.2838\n",
      "Epoch [8/400], Loss: 0.2837\n",
      "Epoch [9/400], Loss: 0.2836\n",
      "Epoch [10/400], Loss: 0.2837\n",
      "Epoch [11/400], Loss: 0.2838\n",
      "Epoch [12/400], Loss: 0.2839\n",
      "Epoch [13/400], Loss: 0.2839\n",
      "Epoch [14/400], Loss: 0.2839\n",
      "Epoch [15/400], Loss: 0.2837\n",
      "Epoch [16/400], Loss: 0.2832\n",
      "Epoch [17/400], Loss: 0.2825\n",
      "Epoch [18/400], Loss: 0.2814\n",
      "Epoch [19/400], Loss: 0.2798\n",
      "Epoch [20/400], Loss: 0.2778\n",
      "Epoch [21/400], Loss: 0.2752\n",
      "Epoch [22/400], Loss: 0.2721\n",
      "Epoch [23/400], Loss: 0.2684\n",
      "Epoch [24/400], Loss: 0.2640\n",
      "Epoch [25/400], Loss: 0.2591\n",
      "Epoch [26/400], Loss: 0.2536\n",
      "Epoch [27/400], Loss: 0.2476\n",
      "Epoch [28/400], Loss: 0.2413\n",
      "Epoch [29/400], Loss: 0.2346\n",
      "Epoch [30/400], Loss: 0.2276\n",
      "Epoch [31/400], Loss: 0.2205\n",
      "Epoch [32/400], Loss: 0.2134\n",
      "Epoch [33/400], Loss: 0.2063\n",
      "Epoch [34/400], Loss: 0.1992\n",
      "Epoch [35/400], Loss: 0.1921\n",
      "Epoch [36/400], Loss: 0.1852\n",
      "Epoch [37/400], Loss: 0.1784\n",
      "Epoch [38/400], Loss: 0.1718\n",
      "Epoch [39/400], Loss: 0.1654\n",
      "Epoch [40/400], Loss: 0.1591\n",
      "Epoch [41/400], Loss: 0.1530\n",
      "Epoch [42/400], Loss: 0.1472\n",
      "Epoch [43/400], Loss: 0.1416\n",
      "Epoch [44/400], Loss: 0.1361\n",
      "Epoch [45/400], Loss: 0.1309\n",
      "Epoch [46/400], Loss: 0.1259\n",
      "Epoch [47/400], Loss: 0.1209\n",
      "Epoch [48/400], Loss: 0.1162\n",
      "Epoch [49/400], Loss: 0.1115\n",
      "Epoch [50/400], Loss: 0.1070\n",
      "Epoch [51/400], Loss: 0.1026\n",
      "Epoch [52/400], Loss: 0.0983\n",
      "Epoch [53/400], Loss: 0.0941\n",
      "Epoch [54/400], Loss: 0.0900\n",
      "Epoch [55/400], Loss: 0.0860\n",
      "Epoch [56/400], Loss: 0.0821\n",
      "Epoch [57/400], Loss: 0.0784\n",
      "Epoch [58/400], Loss: 0.0747\n",
      "Epoch [59/400], Loss: 0.0712\n",
      "Epoch [60/400], Loss: 0.0678\n",
      "Epoch [61/400], Loss: 0.0645\n",
      "Epoch [62/400], Loss: 0.0612\n",
      "Epoch [63/400], Loss: 0.0581\n",
      "Epoch [64/400], Loss: 0.0552\n",
      "Epoch [65/400], Loss: 0.0523\n",
      "Epoch [66/400], Loss: 0.0496\n",
      "Epoch [67/400], Loss: 0.0470\n",
      "Epoch [68/400], Loss: 0.0445\n",
      "Epoch [69/400], Loss: 0.0421\n",
      "Epoch [70/400], Loss: 0.0398\n",
      "Epoch [71/400], Loss: 0.0377\n",
      "Epoch [72/400], Loss: 0.0356\n",
      "Epoch [73/400], Loss: 0.0337\n",
      "Epoch [74/400], Loss: 0.0318\n",
      "Epoch [75/400], Loss: 0.0301\n",
      "Epoch [76/400], Loss: 0.0284\n",
      "Epoch [77/400], Loss: 0.0268\n",
      "Epoch [78/400], Loss: 0.0253\n",
      "Epoch [79/400], Loss: 0.0239\n",
      "Epoch [80/400], Loss: 0.0226\n",
      "Epoch [81/400], Loss: 0.0213\n",
      "Epoch [82/400], Loss: 0.0201\n",
      "Epoch [83/400], Loss: 0.0190\n",
      "Epoch [84/400], Loss: 0.0179\n",
      "Epoch [85/400], Loss: 0.0169\n",
      "Epoch [86/400], Loss: 0.0160\n",
      "Epoch [87/400], Loss: 0.0151\n",
      "Epoch [88/400], Loss: 0.0143\n",
      "Epoch [89/400], Loss: 0.0135\n",
      "Epoch [90/400], Loss: 0.0128\n",
      "Epoch [91/400], Loss: 0.0122\n",
      "Epoch [92/400], Loss: 0.0115\n",
      "Epoch [93/400], Loss: 0.0109\n",
      "Epoch [94/400], Loss: 0.0104\n",
      "Epoch [95/400], Loss: 0.0098\n",
      "Epoch [96/400], Loss: 0.0094\n",
      "Epoch [97/400], Loss: 0.0089\n",
      "Epoch [98/400], Loss: 0.0085\n",
      "Epoch [99/400], Loss: 0.0081\n",
      "Epoch [100/400], Loss: 0.0077\n",
      "Epoch [101/400], Loss: 0.0073\n",
      "Epoch [102/400], Loss: 0.0070\n",
      "Epoch [103/400], Loss: 0.0067\n",
      "Epoch [104/400], Loss: 0.0064\n",
      "Epoch [105/400], Loss: 0.0061\n",
      "Epoch [106/400], Loss: 0.0059\n",
      "Epoch [107/400], Loss: 0.0056\n",
      "Epoch [108/400], Loss: 0.0054\n",
      "Epoch [109/400], Loss: 0.0052\n",
      "Epoch [110/400], Loss: 0.0050\n",
      "Epoch [111/400], Loss: 0.0048\n",
      "Epoch [112/400], Loss: 0.0046\n",
      "Epoch [113/400], Loss: 0.0045\n",
      "Epoch [114/400], Loss: 0.0043\n",
      "Epoch [115/400], Loss: 0.0042\n",
      "Epoch [116/400], Loss: 0.0040\n",
      "Epoch [117/400], Loss: 0.0039\n",
      "Epoch [118/400], Loss: 0.0038\n",
      "Epoch [119/400], Loss: 0.0037\n",
      "Epoch [120/400], Loss: 0.0035\n",
      "Epoch [121/400], Loss: 0.0034\n",
      "Epoch [122/400], Loss: 0.0033\n",
      "Epoch [123/400], Loss: 0.0032\n",
      "Epoch [124/400], Loss: 0.0031\n",
      "Epoch [125/400], Loss: 0.0030\n",
      "Epoch [126/400], Loss: 0.0030\n",
      "Epoch [127/400], Loss: 0.0029\n",
      "Epoch [128/400], Loss: 0.0028\n",
      "Epoch [129/400], Loss: 0.0027\n",
      "Epoch [130/400], Loss: 0.0026\n",
      "Epoch [131/400], Loss: 0.0026\n",
      "Epoch [132/400], Loss: 0.0025\n",
      "Epoch [133/400], Loss: 0.0024\n",
      "Epoch [134/400], Loss: 0.0024\n",
      "Epoch [135/400], Loss: 0.0023\n",
      "Epoch [136/400], Loss: 0.0023\n",
      "Epoch [137/400], Loss: 0.0022\n",
      "Epoch [138/400], Loss: 0.0021\n",
      "Epoch [139/400], Loss: 0.0021\n",
      "Epoch [140/400], Loss: 0.0020\n",
      "Epoch [141/400], Loss: 0.0020\n",
      "Epoch [142/400], Loss: 0.0020\n",
      "Epoch [143/400], Loss: 0.0019\n",
      "Epoch [144/400], Loss: 0.0019\n",
      "Epoch [145/400], Loss: 0.0018\n",
      "Epoch [146/400], Loss: 0.0018\n",
      "Epoch [147/400], Loss: 0.0018\n",
      "Epoch [148/400], Loss: 0.0017\n",
      "Epoch [149/400], Loss: 0.0017\n",
      "Epoch [150/400], Loss: 0.0017\n",
      "Epoch [151/400], Loss: 0.0016\n",
      "Epoch [152/400], Loss: 0.0016\n",
      "Epoch [153/400], Loss: 0.0015\n",
      "Epoch [154/400], Loss: 0.0015\n",
      "Epoch [155/400], Loss: 0.0015\n",
      "Epoch [156/400], Loss: 0.0015\n",
      "Epoch [157/400], Loss: 0.0014\n",
      "Epoch [158/400], Loss: 0.0014\n",
      "Epoch [159/400], Loss: 0.0014\n",
      "Epoch [160/400], Loss: 0.0014\n",
      "Epoch [161/400], Loss: 0.0013\n",
      "Epoch [162/400], Loss: 0.0013\n",
      "Epoch [163/400], Loss: 0.0013\n",
      "Epoch [164/400], Loss: 0.0013\n",
      "Epoch [165/400], Loss: 0.0013\n",
      "Epoch [166/400], Loss: 0.0012\n",
      "Epoch [167/400], Loss: 0.0012\n",
      "Epoch [168/400], Loss: 0.0012\n",
      "Epoch [169/400], Loss: 0.0012\n",
      "Epoch [170/400], Loss: 0.0012\n",
      "Epoch [171/400], Loss: 0.0012\n",
      "Epoch [172/400], Loss: 0.0011\n",
      "Epoch [173/400], Loss: 0.0011\n",
      "Epoch [174/400], Loss: 0.0011\n",
      "Epoch [175/400], Loss: 0.0011\n",
      "Epoch [176/400], Loss: 0.0011\n",
      "Epoch [177/400], Loss: 0.0011\n",
      "Epoch [178/400], Loss: 0.0011\n",
      "Epoch [179/400], Loss: 0.0011\n",
      "Epoch [180/400], Loss: 0.0011\n",
      "Epoch [181/400], Loss: 0.0011\n",
      "Epoch [182/400], Loss: 0.0011\n",
      "Epoch [183/400], Loss: 0.0011\n",
      "Epoch [184/400], Loss: 0.0010\n",
      "Epoch [185/400], Loss: 0.0010\n",
      "Epoch [186/400], Loss: 0.0010\n",
      "Epoch [187/400], Loss: 0.0010\n",
      "Epoch [188/400], Loss: 0.0010\n",
      "Epoch [189/400], Loss: 0.0010\n",
      "Epoch [190/400], Loss: 0.0010\n",
      "Epoch [191/400], Loss: 0.0010\n",
      "Epoch [192/400], Loss: 0.0010\n",
      "Epoch [193/400], Loss: 0.0010\n",
      "Epoch [194/400], Loss: 0.0010\n",
      "Epoch [195/400], Loss: 0.0010\n",
      "Epoch [196/400], Loss: 0.0010\n",
      "Epoch [197/400], Loss: 0.0010\n",
      "Epoch [198/400], Loss: 0.0010\n",
      "Epoch [199/400], Loss: 0.0010\n",
      "Epoch [200/400], Loss: 0.0010\n",
      "Epoch [201/400], Loss: 0.0010\n",
      "Epoch [202/400], Loss: 0.0010\n",
      "Epoch [203/400], Loss: 0.0010\n",
      "Epoch [204/400], Loss: 0.0010\n",
      "Epoch [205/400], Loss: 0.0010\n",
      "Epoch [206/400], Loss: 0.0010\n",
      "Epoch [207/400], Loss: 0.0010\n",
      "Epoch [208/400], Loss: 0.0010\n",
      "Epoch [209/400], Loss: 0.0010\n",
      "Epoch [210/400], Loss: 0.0010\n",
      "Epoch [211/400], Loss: 0.0010\n",
      "Epoch [212/400], Loss: 0.0010\n",
      "Epoch [213/400], Loss: 0.0010\n",
      "Epoch [214/400], Loss: 0.0010\n",
      "Epoch [215/400], Loss: 0.0010\n",
      "Epoch [216/400], Loss: 0.0010\n",
      "Epoch [217/400], Loss: 0.0010\n",
      "Epoch [218/400], Loss: 0.0010\n",
      "Epoch [219/400], Loss: 0.0010\n",
      "Epoch [220/400], Loss: 0.0010\n",
      "Epoch [221/400], Loss: 0.0010\n",
      "Epoch [222/400], Loss: 0.0010\n",
      "Epoch [223/400], Loss: 0.0010\n",
      "Epoch [224/400], Loss: 0.0010\n",
      "Epoch [225/400], Loss: 0.0010\n",
      "Epoch [226/400], Loss: 0.0010\n",
      "Epoch [227/400], Loss: 0.0010\n",
      "Epoch [228/400], Loss: 0.0010\n",
      "Epoch [229/400], Loss: 0.0010\n",
      "Epoch [230/400], Loss: 0.0010\n",
      "Epoch [231/400], Loss: 0.0010\n",
      "Epoch [232/400], Loss: 0.0010\n",
      "Epoch [233/400], Loss: 0.0010\n",
      "Epoch [234/400], Loss: 0.0010\n",
      "Epoch [235/400], Loss: 0.0010\n",
      "Epoch [236/400], Loss: 0.0010\n",
      "Epoch [237/400], Loss: 0.0010\n",
      "Epoch [238/400], Loss: 0.0010\n",
      "Epoch [239/400], Loss: 0.0010\n",
      "Epoch [240/400], Loss: 0.0010\n",
      "Epoch [241/400], Loss: 0.0010\n",
      "Epoch [242/400], Loss: 0.0010\n",
      "Epoch [243/400], Loss: 0.0010\n",
      "Epoch [244/400], Loss: 0.0010\n",
      "Epoch [245/400], Loss: 0.0010\n",
      "Epoch [246/400], Loss: 0.0010\n",
      "Epoch [247/400], Loss: 0.0010\n",
      "Epoch [248/400], Loss: 0.0010\n",
      "Epoch [249/400], Loss: 0.0010\n",
      "Epoch [250/400], Loss: 0.0010\n",
      "Epoch [251/400], Loss: 0.0010\n",
      "Epoch [252/400], Loss: 0.0010\n",
      "Epoch [253/400], Loss: 0.0011\n",
      "Epoch [254/400], Loss: 0.0011\n",
      "Epoch [255/400], Loss: 0.0011\n",
      "Epoch [256/400], Loss: 0.0011\n",
      "Epoch [257/400], Loss: 0.0011\n",
      "Epoch [258/400], Loss: 0.0011\n",
      "Epoch [259/400], Loss: 0.0011\n",
      "Epoch [260/400], Loss: 0.0011\n",
      "Epoch [261/400], Loss: 0.0011\n",
      "Epoch [262/400], Loss: 0.0011\n",
      "Epoch [263/400], Loss: 0.0011\n",
      "Epoch [264/400], Loss: 0.0012\n",
      "Epoch [265/400], Loss: 0.0012\n",
      "Epoch [266/400], Loss: 0.0012\n",
      "Epoch [267/400], Loss: 0.0012\n",
      "Epoch [268/400], Loss: 0.0012\n",
      "Epoch [269/400], Loss: 0.0012\n",
      "Epoch [270/400], Loss: 0.0012\n",
      "Epoch [271/400], Loss: 0.0013\n",
      "Epoch [272/400], Loss: 0.0013\n",
      "Epoch [273/400], Loss: 0.0013\n",
      "Epoch [274/400], Loss: 0.0013\n",
      "Epoch [275/400], Loss: 0.0013\n",
      "Epoch [276/400], Loss: 0.0013\n",
      "Epoch [277/400], Loss: 0.0013\n",
      "Epoch [278/400], Loss: 0.0014\n",
      "Epoch [279/400], Loss: 0.0014\n",
      "Epoch [280/400], Loss: 0.0014\n",
      "Epoch [281/400], Loss: 0.0014\n",
      "Epoch [282/400], Loss: 0.0014\n",
      "Epoch [283/400], Loss: 0.0014\n",
      "Epoch [284/400], Loss: 0.0014\n",
      "Epoch [285/400], Loss: 0.0015\n",
      "Epoch [286/400], Loss: 0.0015\n",
      "Epoch [287/400], Loss: 0.0015\n",
      "Epoch [288/400], Loss: 0.0015\n",
      "Epoch [289/400], Loss: 0.0015\n",
      "Epoch [290/400], Loss: 0.0015\n",
      "Epoch [291/400], Loss: 0.0015\n",
      "Epoch [292/400], Loss: 0.0016\n",
      "Epoch [293/400], Loss: 0.0016\n",
      "Epoch [294/400], Loss: 0.0016\n",
      "Epoch [295/400], Loss: 0.0016\n",
      "Epoch [296/400], Loss: 0.0016\n",
      "Epoch [297/400], Loss: 0.0016\n",
      "Epoch [298/400], Loss: 0.0016\n",
      "Epoch [299/400], Loss: 0.0016\n",
      "Epoch [300/400], Loss: 0.0017\n",
      "Epoch [301/400], Loss: 0.0017\n",
      "Epoch [302/400], Loss: 0.0017\n",
      "Epoch [303/400], Loss: 0.0017\n",
      "Epoch [304/400], Loss: 0.0017\n",
      "Epoch [305/400], Loss: 0.0017\n",
      "Epoch [306/400], Loss: 0.0017\n",
      "Epoch [307/400], Loss: 0.0017\n",
      "Epoch [308/400], Loss: 0.0017\n",
      "Epoch [309/400], Loss: 0.0018\n",
      "Epoch [310/400], Loss: 0.0018\n",
      "Epoch [311/400], Loss: 0.0018\n",
      "Epoch [312/400], Loss: 0.0018\n",
      "Epoch [313/400], Loss: 0.0018\n",
      "Epoch [314/400], Loss: 0.0018\n",
      "Epoch [315/400], Loss: 0.0018\n",
      "Epoch [316/400], Loss: 0.0018\n",
      "Epoch [317/400], Loss: 0.0018\n",
      "Epoch [318/400], Loss: 0.0018\n",
      "Epoch [319/400], Loss: 0.0019\n",
      "Epoch [320/400], Loss: 0.0019\n",
      "Epoch [321/400], Loss: 0.0019\n",
      "Epoch [322/400], Loss: 0.0019\n",
      "Epoch [323/400], Loss: 0.0019\n",
      "Epoch [324/400], Loss: 0.0019\n",
      "Epoch [325/400], Loss: 0.0019\n",
      "Epoch [326/400], Loss: 0.0019\n",
      "Epoch [327/400], Loss: 0.0019\n",
      "Epoch [328/400], Loss: 0.0019\n",
      "Epoch [329/400], Loss: 0.0019\n",
      "Epoch [330/400], Loss: 0.0019\n",
      "Epoch [331/400], Loss: 0.0020\n",
      "Epoch [332/400], Loss: 0.0020\n",
      "Epoch [333/400], Loss: 0.0020\n",
      "Epoch [334/400], Loss: 0.0020\n",
      "Epoch [335/400], Loss: 0.0020\n",
      "Epoch [336/400], Loss: 0.0020\n",
      "Epoch [337/400], Loss: 0.0020\n",
      "Epoch [338/400], Loss: 0.0020\n",
      "Epoch [339/400], Loss: 0.0020\n",
      "Epoch [340/400], Loss: 0.0020\n",
      "Epoch [341/400], Loss: 0.0020\n",
      "Epoch [342/400], Loss: 0.0020\n",
      "Epoch [343/400], Loss: 0.0020\n",
      "Epoch [344/400], Loss: 0.0020\n",
      "Epoch [345/400], Loss: 0.0020\n",
      "Epoch [346/400], Loss: 0.0020\n",
      "Epoch [347/400], Loss: 0.0021\n",
      "Epoch [348/400], Loss: 0.0021\n",
      "Epoch [349/400], Loss: 0.0021\n",
      "Epoch [350/400], Loss: 0.0021\n",
      "Epoch [351/400], Loss: 0.0021\n",
      "Epoch [352/400], Loss: 0.0021\n",
      "Epoch [353/400], Loss: 0.0021\n",
      "Epoch [354/400], Loss: 0.0021\n",
      "Epoch [355/400], Loss: 0.0021\n",
      "Epoch [356/400], Loss: 0.0021\n",
      "Epoch [357/400], Loss: 0.0021\n",
      "Epoch [358/400], Loss: 0.0021\n",
      "Epoch [359/400], Loss: 0.0021\n",
      "Epoch [360/400], Loss: 0.0021\n",
      "Epoch [361/400], Loss: 0.0021\n",
      "Epoch [362/400], Loss: 0.0021\n",
      "Epoch [363/400], Loss: 0.0021\n",
      "Epoch [364/400], Loss: 0.0021\n",
      "Epoch [365/400], Loss: 0.0021\n",
      "Epoch [366/400], Loss: 0.0021\n",
      "Epoch [367/400], Loss: 0.0021\n",
      "Epoch [368/400], Loss: 0.0022\n",
      "Epoch [369/400], Loss: 0.0022\n",
      "Epoch [370/400], Loss: 0.0022\n",
      "Epoch [371/400], Loss: 0.0022\n",
      "Epoch [372/400], Loss: 0.0022\n",
      "Epoch [373/400], Loss: 0.0022\n",
      "Epoch [374/400], Loss: 0.0022\n",
      "Epoch [375/400], Loss: 0.0022\n",
      "Epoch [376/400], Loss: 0.0022\n",
      "Epoch [377/400], Loss: 0.0022\n",
      "Epoch [378/400], Loss: 0.0022\n",
      "Epoch [379/400], Loss: 0.0022\n",
      "Epoch [380/400], Loss: 0.0022\n",
      "Epoch [381/400], Loss: 0.0022\n",
      "Epoch [382/400], Loss: 0.0022\n",
      "Epoch [383/400], Loss: 0.0022\n",
      "Epoch [384/400], Loss: 0.0022\n",
      "Epoch [385/400], Loss: 0.0022\n",
      "Epoch [386/400], Loss: 0.0022\n",
      "Epoch [387/400], Loss: 0.0022\n",
      "Epoch [388/400], Loss: 0.0022\n",
      "Epoch [389/400], Loss: 0.0022\n",
      "Epoch [390/400], Loss: 0.0022\n",
      "Epoch [391/400], Loss: 0.0022\n",
      "Epoch [392/400], Loss: 0.0022\n",
      "Epoch [393/400], Loss: 0.0022\n",
      "Epoch [394/400], Loss: 0.0022\n",
      "Epoch [395/400], Loss: 0.0022\n",
      "Epoch [396/400], Loss: 0.0022\n",
      "Epoch [397/400], Loss: 0.0023\n",
      "Epoch [398/400], Loss: 0.0023\n",
      "Epoch [399/400], Loss: 0.0023\n",
      "Epoch [400/400], Loss: 0.0023\n",
      "Accuracy: 49.2063%\n",
      "Epoch [1/400], Loss: 0.0502\n",
      "Epoch [2/400], Loss: 0.0589\n",
      "Epoch [3/400], Loss: 0.0643\n",
      "Epoch [4/400], Loss: 0.0682\n",
      "Epoch [5/400], Loss: 0.0714\n",
      "Epoch [6/400], Loss: 0.0742\n",
      "Epoch [7/400], Loss: 0.0767\n",
      "Epoch [8/400], Loss: 0.0790\n",
      "Epoch [9/400], Loss: 0.0812\n",
      "Epoch [10/400], Loss: 0.0832\n",
      "Epoch [11/400], Loss: 0.0850\n",
      "Epoch [12/400], Loss: 0.0867\n",
      "Epoch [13/400], Loss: 0.0881\n",
      "Epoch [14/400], Loss: 0.0892\n",
      "Epoch [15/400], Loss: 0.0901\n",
      "Epoch [16/400], Loss: 0.0906\n",
      "Epoch [17/400], Loss: 0.0908\n",
      "Epoch [18/400], Loss: 0.0906\n",
      "Epoch [19/400], Loss: 0.0899\n",
      "Epoch [20/400], Loss: 0.0887\n",
      "Epoch [21/400], Loss: 0.0869\n",
      "Epoch [22/400], Loss: 0.0843\n",
      "Epoch [23/400], Loss: 0.0809\n",
      "Epoch [24/400], Loss: 0.0769\n",
      "Epoch [25/400], Loss: 0.0723\n",
      "Epoch [26/400], Loss: 0.0675\n",
      "Epoch [27/400], Loss: 0.0628\n",
      "Epoch [28/400], Loss: 0.0582\n",
      "Epoch [29/400], Loss: 0.0541\n",
      "Epoch [30/400], Loss: 0.0504\n",
      "Epoch [31/400], Loss: 0.0472\n",
      "Epoch [32/400], Loss: 0.0446\n",
      "Epoch [33/400], Loss: 0.0422\n",
      "Epoch [34/400], Loss: 0.0402\n",
      "Epoch [35/400], Loss: 0.0384\n",
      "Epoch [36/400], Loss: 0.0368\n",
      "Epoch [37/400], Loss: 0.0353\n",
      "Epoch [38/400], Loss: 0.0339\n",
      "Epoch [39/400], Loss: 0.0326\n",
      "Epoch [40/400], Loss: 0.0314\n",
      "Epoch [41/400], Loss: 0.0302\n",
      "Epoch [42/400], Loss: 0.0291\n",
      "Epoch [43/400], Loss: 0.0281\n",
      "Epoch [44/400], Loss: 0.0271\n",
      "Epoch [45/400], Loss: 0.0261\n",
      "Epoch [46/400], Loss: 0.0251\n",
      "Epoch [47/400], Loss: 0.0242\n",
      "Epoch [48/400], Loss: 0.0232\n",
      "Epoch [49/400], Loss: 0.0223\n",
      "Epoch [50/400], Loss: 0.0215\n",
      "Epoch [51/400], Loss: 0.0206\n",
      "Epoch [52/400], Loss: 0.0198\n",
      "Epoch [53/400], Loss: 0.0190\n",
      "Epoch [54/400], Loss: 0.0183\n",
      "Epoch [55/400], Loss: 0.0176\n",
      "Epoch [56/400], Loss: 0.0169\n",
      "Epoch [57/400], Loss: 0.0162\n",
      "Epoch [58/400], Loss: 0.0156\n",
      "Epoch [59/400], Loss: 0.0150\n",
      "Epoch [60/400], Loss: 0.0145\n",
      "Epoch [61/400], Loss: 0.0139\n",
      "Epoch [62/400], Loss: 0.0134\n",
      "Epoch [63/400], Loss: 0.0129\n",
      "Epoch [64/400], Loss: 0.0125\n",
      "Epoch [65/400], Loss: 0.0120\n",
      "Epoch [66/400], Loss: 0.0116\n",
      "Epoch [67/400], Loss: 0.0112\n",
      "Epoch [68/400], Loss: 0.0108\n",
      "Epoch [69/400], Loss: 0.0104\n",
      "Epoch [70/400], Loss: 0.0101\n",
      "Epoch [71/400], Loss: 0.0098\n",
      "Epoch [72/400], Loss: 0.0095\n",
      "Epoch [73/400], Loss: 0.0092\n",
      "Epoch [74/400], Loss: 0.0089\n",
      "Epoch [75/400], Loss: 0.0087\n",
      "Epoch [76/400], Loss: 0.0084\n",
      "Epoch [77/400], Loss: 0.0082\n",
      "Epoch [78/400], Loss: 0.0080\n",
      "Epoch [79/400], Loss: 0.0078\n",
      "Epoch [80/400], Loss: 0.0076\n",
      "Epoch [81/400], Loss: 0.0074\n",
      "Epoch [82/400], Loss: 0.0072\n",
      "Epoch [83/400], Loss: 0.0071\n",
      "Epoch [84/400], Loss: 0.0069\n",
      "Epoch [85/400], Loss: 0.0068\n",
      "Epoch [86/400], Loss: 0.0067\n",
      "Epoch [87/400], Loss: 0.0065\n",
      "Epoch [88/400], Loss: 0.0064\n",
      "Epoch [89/400], Loss: 0.0063\n",
      "Epoch [90/400], Loss: 0.0062\n",
      "Epoch [91/400], Loss: 0.0061\n",
      "Epoch [92/400], Loss: 0.0060\n",
      "Epoch [93/400], Loss: 0.0059\n",
      "Epoch [94/400], Loss: 0.0058\n",
      "Epoch [95/400], Loss: 0.0057\n",
      "Epoch [96/400], Loss: 0.0056\n",
      "Epoch [97/400], Loss: 0.0055\n",
      "Epoch [98/400], Loss: 0.0054\n",
      "Epoch [99/400], Loss: 0.0053\n",
      "Epoch [100/400], Loss: 0.0053\n",
      "Epoch [101/400], Loss: 0.0052\n",
      "Epoch [102/400], Loss: 0.0051\n",
      "Epoch [103/400], Loss: 0.0051\n",
      "Epoch [104/400], Loss: 0.0050\n",
      "Epoch [105/400], Loss: 0.0049\n",
      "Epoch [106/400], Loss: 0.0049\n",
      "Epoch [107/400], Loss: 0.0048\n",
      "Epoch [108/400], Loss: 0.0048\n",
      "Epoch [109/400], Loss: 0.0047\n",
      "Epoch [110/400], Loss: 0.0047\n",
      "Epoch [111/400], Loss: 0.0046\n",
      "Epoch [112/400], Loss: 0.0046\n",
      "Epoch [113/400], Loss: 0.0045\n",
      "Epoch [114/400], Loss: 0.0045\n",
      "Epoch [115/400], Loss: 0.0045\n",
      "Epoch [116/400], Loss: 0.0044\n",
      "Epoch [117/400], Loss: 0.0044\n",
      "Epoch [118/400], Loss: 0.0044\n",
      "Epoch [119/400], Loss: 0.0043\n",
      "Epoch [120/400], Loss: 0.0043\n",
      "Epoch [121/400], Loss: 0.0043\n",
      "Epoch [122/400], Loss: 0.0042\n",
      "Epoch [123/400], Loss: 0.0042\n",
      "Epoch [124/400], Loss: 0.0042\n",
      "Epoch [125/400], Loss: 0.0041\n",
      "Epoch [126/400], Loss: 0.0041\n",
      "Epoch [127/400], Loss: 0.0041\n",
      "Epoch [128/400], Loss: 0.0041\n",
      "Epoch [129/400], Loss: 0.0040\n",
      "Epoch [130/400], Loss: 0.0040\n",
      "Epoch [131/400], Loss: 0.0040\n",
      "Epoch [132/400], Loss: 0.0040\n",
      "Epoch [133/400], Loss: 0.0039\n",
      "Epoch [134/400], Loss: 0.0039\n",
      "Epoch [135/400], Loss: 0.0039\n",
      "Epoch [136/400], Loss: 0.0039\n",
      "Epoch [137/400], Loss: 0.0038\n",
      "Epoch [138/400], Loss: 0.0038\n",
      "Epoch [139/400], Loss: 0.0038\n",
      "Epoch [140/400], Loss: 0.0038\n",
      "Epoch [141/400], Loss: 0.0038\n",
      "Epoch [142/400], Loss: 0.0038\n",
      "Epoch [143/400], Loss: 0.0037\n",
      "Epoch [144/400], Loss: 0.0037\n",
      "Epoch [145/400], Loss: 0.0037\n",
      "Epoch [146/400], Loss: 0.0037\n",
      "Epoch [147/400], Loss: 0.0037\n",
      "Epoch [148/400], Loss: 0.0036\n",
      "Epoch [149/400], Loss: 0.0036\n",
      "Epoch [150/400], Loss: 0.0036\n",
      "Epoch [151/400], Loss: 0.0036\n",
      "Epoch [152/400], Loss: 0.0036\n",
      "Epoch [153/400], Loss: 0.0036\n",
      "Epoch [154/400], Loss: 0.0035\n",
      "Epoch [155/400], Loss: 0.0035\n",
      "Epoch [156/400], Loss: 0.0035\n",
      "Epoch [157/400], Loss: 0.0035\n",
      "Epoch [158/400], Loss: 0.0035\n",
      "Epoch [159/400], Loss: 0.0035\n",
      "Epoch [160/400], Loss: 0.0035\n",
      "Epoch [161/400], Loss: 0.0034\n",
      "Epoch [162/400], Loss: 0.0034\n",
      "Epoch [163/400], Loss: 0.0034\n",
      "Epoch [164/400], Loss: 0.0034\n",
      "Epoch [165/400], Loss: 0.0034\n",
      "Epoch [166/400], Loss: 0.0034\n",
      "Epoch [167/400], Loss: 0.0034\n",
      "Epoch [168/400], Loss: 0.0033\n",
      "Epoch [169/400], Loss: 0.0033\n",
      "Epoch [170/400], Loss: 0.0033\n",
      "Epoch [171/400], Loss: 0.0033\n",
      "Epoch [172/400], Loss: 0.0033\n",
      "Epoch [173/400], Loss: 0.0033\n",
      "Epoch [174/400], Loss: 0.0033\n",
      "Epoch [175/400], Loss: 0.0033\n",
      "Epoch [176/400], Loss: 0.0032\n",
      "Epoch [177/400], Loss: 0.0032\n",
      "Epoch [178/400], Loss: 0.0032\n",
      "Epoch [179/400], Loss: 0.0032\n",
      "Epoch [180/400], Loss: 0.0032\n",
      "Epoch [181/400], Loss: 0.0032\n",
      "Epoch [182/400], Loss: 0.0032\n",
      "Epoch [183/400], Loss: 0.0032\n",
      "Epoch [184/400], Loss: 0.0032\n",
      "Epoch [185/400], Loss: 0.0032\n",
      "Epoch [186/400], Loss: 0.0032\n",
      "Epoch [187/400], Loss: 0.0032\n",
      "Epoch [188/400], Loss: 0.0032\n",
      "Epoch [189/400], Loss: 0.0032\n",
      "Epoch [190/400], Loss: 0.0031\n",
      "Epoch [191/400], Loss: 0.0031\n",
      "Epoch [192/400], Loss: 0.0031\n",
      "Epoch [193/400], Loss: 0.0031\n",
      "Epoch [194/400], Loss: 0.0031\n",
      "Epoch [195/400], Loss: 0.0031\n",
      "Epoch [196/400], Loss: 0.0031\n",
      "Epoch [197/400], Loss: 0.0031\n",
      "Epoch [198/400], Loss: 0.0031\n",
      "Epoch [199/400], Loss: 0.0031\n",
      "Epoch [200/400], Loss: 0.0031\n",
      "Epoch [201/400], Loss: 0.0032\n",
      "Epoch [202/400], Loss: 0.0032\n",
      "Epoch [203/400], Loss: 0.0032\n",
      "Epoch [204/400], Loss: 0.0032\n",
      "Epoch [205/400], Loss: 0.0032\n",
      "Epoch [206/400], Loss: 0.0032\n",
      "Epoch [207/400], Loss: 0.0032\n",
      "Epoch [208/400], Loss: 0.0032\n",
      "Epoch [209/400], Loss: 0.0032\n",
      "Epoch [210/400], Loss: 0.0032\n",
      "Epoch [211/400], Loss: 0.0032\n",
      "Epoch [212/400], Loss: 0.0032\n",
      "Epoch [213/400], Loss: 0.0032\n",
      "Epoch [214/400], Loss: 0.0032\n",
      "Epoch [215/400], Loss: 0.0031\n",
      "Epoch [216/400], Loss: 0.0031\n",
      "Epoch [217/400], Loss: 0.0031\n",
      "Epoch [218/400], Loss: 0.0031\n",
      "Epoch [219/400], Loss: 0.0031\n",
      "Epoch [220/400], Loss: 0.0031\n",
      "Epoch [221/400], Loss: 0.0031\n",
      "Epoch [222/400], Loss: 0.0031\n",
      "Epoch [223/400], Loss: 0.0031\n",
      "Epoch [224/400], Loss: 0.0031\n",
      "Epoch [225/400], Loss: 0.0031\n",
      "Epoch [226/400], Loss: 0.0031\n",
      "Epoch [227/400], Loss: 0.0031\n",
      "Epoch [228/400], Loss: 0.0031\n",
      "Epoch [229/400], Loss: 0.0031\n",
      "Epoch [230/400], Loss: 0.0031\n",
      "Epoch [231/400], Loss: 0.0031\n",
      "Epoch [232/400], Loss: 0.0031\n",
      "Epoch [233/400], Loss: 0.0031\n",
      "Epoch [234/400], Loss: 0.0031\n",
      "Epoch [235/400], Loss: 0.0031\n",
      "Epoch [236/400], Loss: 0.0031\n",
      "Epoch [237/400], Loss: 0.0031\n",
      "Epoch [238/400], Loss: 0.0031\n",
      "Epoch [239/400], Loss: 0.0031\n",
      "Epoch [240/400], Loss: 0.0031\n",
      "Epoch [241/400], Loss: 0.0031\n",
      "Epoch [242/400], Loss: 0.0031\n",
      "Epoch [243/400], Loss: 0.0031\n",
      "Epoch [244/400], Loss: 0.0031\n",
      "Epoch [245/400], Loss: 0.0031\n",
      "Epoch [246/400], Loss: 0.0031\n",
      "Epoch [247/400], Loss: 0.0031\n",
      "Epoch [248/400], Loss: 0.0031\n",
      "Epoch [249/400], Loss: 0.0031\n",
      "Epoch [250/400], Loss: 0.0031\n",
      "Epoch [251/400], Loss: 0.0031\n",
      "Epoch [252/400], Loss: 0.0031\n",
      "Epoch [253/400], Loss: 0.0031\n",
      "Epoch [254/400], Loss: 0.0031\n",
      "Epoch [255/400], Loss: 0.0031\n",
      "Epoch [256/400], Loss: 0.0031\n",
      "Epoch [257/400], Loss: 0.0031\n",
      "Epoch [258/400], Loss: 0.0031\n",
      "Epoch [259/400], Loss: 0.0031\n",
      "Epoch [260/400], Loss: 0.0031\n",
      "Epoch [261/400], Loss: 0.0031\n",
      "Epoch [262/400], Loss: 0.0031\n",
      "Epoch [263/400], Loss: 0.0031\n",
      "Epoch [264/400], Loss: 0.0032\n",
      "Epoch [265/400], Loss: 0.0032\n",
      "Epoch [266/400], Loss: 0.0032\n",
      "Epoch [267/400], Loss: 0.0032\n",
      "Epoch [268/400], Loss: 0.0032\n",
      "Epoch [269/400], Loss: 0.0032\n",
      "Epoch [270/400], Loss: 0.0032\n",
      "Epoch [271/400], Loss: 0.0032\n",
      "Epoch [272/400], Loss: 0.0032\n",
      "Epoch [273/400], Loss: 0.0032\n",
      "Epoch [274/400], Loss: 0.0033\n",
      "Epoch [275/400], Loss: 0.0033\n",
      "Epoch [276/400], Loss: 0.0033\n",
      "Epoch [277/400], Loss: 0.0033\n",
      "Epoch [278/400], Loss: 0.0033\n",
      "Epoch [279/400], Loss: 0.0033\n",
      "Epoch [280/400], Loss: 0.0033\n",
      "Epoch [281/400], Loss: 0.0033\n",
      "Epoch [282/400], Loss: 0.0033\n",
      "Epoch [283/400], Loss: 0.0033\n",
      "Epoch [284/400], Loss: 0.0034\n",
      "Epoch [285/400], Loss: 0.0034\n",
      "Epoch [286/400], Loss: 0.0034\n",
      "Epoch [287/400], Loss: 0.0034\n",
      "Epoch [288/400], Loss: 0.0034\n",
      "Epoch [289/400], Loss: 0.0034\n",
      "Epoch [290/400], Loss: 0.0034\n",
      "Epoch [291/400], Loss: 0.0034\n",
      "Epoch [292/400], Loss: 0.0034\n",
      "Epoch [293/400], Loss: 0.0034\n",
      "Epoch [294/400], Loss: 0.0034\n",
      "Epoch [295/400], Loss: 0.0034\n",
      "Epoch [296/400], Loss: 0.0034\n",
      "Epoch [297/400], Loss: 0.0034\n",
      "Epoch [298/400], Loss: 0.0034\n",
      "Epoch [299/400], Loss: 0.0034\n",
      "Epoch [300/400], Loss: 0.0034\n",
      "Epoch [301/400], Loss: 0.0034\n",
      "Epoch [302/400], Loss: 0.0034\n",
      "Epoch [303/400], Loss: 0.0034\n",
      "Epoch [304/400], Loss: 0.0034\n",
      "Epoch [305/400], Loss: 0.0034\n",
      "Epoch [306/400], Loss: 0.0034\n",
      "Epoch [307/400], Loss: 0.0034\n",
      "Epoch [308/400], Loss: 0.0034\n",
      "Epoch [309/400], Loss: 0.0034\n",
      "Epoch [310/400], Loss: 0.0034\n",
      "Epoch [311/400], Loss: 0.0034\n",
      "Epoch [312/400], Loss: 0.0035\n",
      "Epoch [313/400], Loss: 0.0035\n",
      "Epoch [314/400], Loss: 0.0035\n",
      "Epoch [315/400], Loss: 0.0035\n",
      "Epoch [316/400], Loss: 0.0035\n",
      "Epoch [317/400], Loss: 0.0035\n",
      "Epoch [318/400], Loss: 0.0035\n",
      "Epoch [319/400], Loss: 0.0035\n",
      "Epoch [320/400], Loss: 0.0035\n",
      "Epoch [321/400], Loss: 0.0035\n",
      "Epoch [322/400], Loss: 0.0035\n",
      "Epoch [323/400], Loss: 0.0035\n",
      "Epoch [324/400], Loss: 0.0035\n",
      "Epoch [325/400], Loss: 0.0035\n",
      "Epoch [326/400], Loss: 0.0035\n",
      "Epoch [327/400], Loss: 0.0035\n",
      "Epoch [328/400], Loss: 0.0035\n",
      "Epoch [329/400], Loss: 0.0035\n",
      "Epoch [330/400], Loss: 0.0035\n",
      "Epoch [331/400], Loss: 0.0035\n",
      "Epoch [332/400], Loss: 0.0035\n",
      "Epoch [333/400], Loss: 0.0035\n",
      "Epoch [334/400], Loss: 0.0035\n",
      "Epoch [335/400], Loss: 0.0035\n",
      "Epoch [336/400], Loss: 0.0035\n",
      "Epoch [337/400], Loss: 0.0035\n",
      "Epoch [338/400], Loss: 0.0035\n",
      "Epoch [339/400], Loss: 0.0035\n",
      "Epoch [340/400], Loss: 0.0035\n",
      "Epoch [341/400], Loss: 0.0035\n",
      "Epoch [342/400], Loss: 0.0035\n",
      "Epoch [343/400], Loss: 0.0035\n",
      "Epoch [344/400], Loss: 0.0035\n",
      "Epoch [345/400], Loss: 0.0035\n",
      "Epoch [346/400], Loss: 0.0035\n",
      "Epoch [347/400], Loss: 0.0035\n",
      "Epoch [348/400], Loss: 0.0035\n",
      "Epoch [349/400], Loss: 0.0035\n",
      "Epoch [350/400], Loss: 0.0034\n",
      "Epoch [351/400], Loss: 0.0034\n",
      "Epoch [352/400], Loss: 0.0034\n",
      "Epoch [353/400], Loss: 0.0034\n",
      "Epoch [354/400], Loss: 0.0034\n",
      "Epoch [355/400], Loss: 0.0034\n",
      "Epoch [356/400], Loss: 0.0034\n",
      "Epoch [357/400], Loss: 0.0034\n",
      "Epoch [358/400], Loss: 0.0034\n",
      "Epoch [359/400], Loss: 0.0034\n",
      "Epoch [360/400], Loss: 0.0034\n",
      "Epoch [361/400], Loss: 0.0034\n",
      "Epoch [362/400], Loss: 0.0034\n",
      "Epoch [363/400], Loss: 0.0034\n",
      "Epoch [364/400], Loss: 0.0034\n",
      "Epoch [365/400], Loss: 0.0034\n",
      "Epoch [366/400], Loss: 0.0034\n",
      "Epoch [367/400], Loss: 0.0033\n",
      "Epoch [368/400], Loss: 0.0033\n",
      "Epoch [369/400], Loss: 0.0033\n",
      "Epoch [370/400], Loss: 0.0033\n",
      "Epoch [371/400], Loss: 0.0033\n",
      "Epoch [372/400], Loss: 0.0033\n",
      "Epoch [373/400], Loss: 0.0033\n",
      "Epoch [374/400], Loss: 0.0033\n",
      "Epoch [375/400], Loss: 0.0033\n",
      "Epoch [376/400], Loss: 0.0033\n",
      "Epoch [377/400], Loss: 0.0033\n",
      "Epoch [378/400], Loss: 0.0033\n",
      "Epoch [379/400], Loss: 0.0033\n",
      "Epoch [380/400], Loss: 0.0033\n",
      "Epoch [381/400], Loss: 0.0033\n",
      "Epoch [382/400], Loss: 0.0032\n",
      "Epoch [383/400], Loss: 0.0032\n",
      "Epoch [384/400], Loss: 0.0032\n",
      "Epoch [385/400], Loss: 0.0032\n",
      "Epoch [386/400], Loss: 0.0032\n",
      "Epoch [387/400], Loss: 0.0032\n",
      "Epoch [388/400], Loss: 0.0032\n",
      "Epoch [389/400], Loss: 0.0032\n",
      "Epoch [390/400], Loss: 0.0032\n",
      "Epoch [391/400], Loss: 0.0032\n",
      "Epoch [392/400], Loss: 0.0031\n",
      "Epoch [393/400], Loss: 0.0031\n",
      "Epoch [394/400], Loss: 0.0031\n",
      "Epoch [395/400], Loss: 0.0031\n",
      "Epoch [396/400], Loss: 0.0031\n",
      "Epoch [397/400], Loss: 0.0031\n",
      "Epoch [398/400], Loss: 0.0031\n",
      "Epoch [399/400], Loss: 0.0031\n",
      "Epoch [400/400], Loss: 0.0031\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/400], Loss: 0.0155\n",
      "Epoch [2/400], Loss: 0.0157\n",
      "Epoch [3/400], Loss: 0.0156\n",
      "Epoch [4/400], Loss: 0.0162\n",
      "Epoch [5/400], Loss: 0.0191\n",
      "Epoch [6/400], Loss: 0.0268\n",
      "Epoch [7/400], Loss: 0.0401\n",
      "Epoch [8/400], Loss: 0.0565\n",
      "Epoch [9/400], Loss: 0.0732\n",
      "Epoch [10/400], Loss: 0.0887\n",
      "Epoch [11/400], Loss: 0.1023\n",
      "Epoch [12/400], Loss: 0.1140\n",
      "Epoch [13/400], Loss: 0.1235\n",
      "Epoch [14/400], Loss: 0.1311\n",
      "Epoch [15/400], Loss: 0.1368\n",
      "Epoch [16/400], Loss: 0.1407\n",
      "Epoch [17/400], Loss: 0.1430\n",
      "Epoch [18/400], Loss: 0.1437\n",
      "Epoch [19/400], Loss: 0.1424\n",
      "Epoch [20/400], Loss: 0.1390\n",
      "Epoch [21/400], Loss: 0.1333\n",
      "Epoch [22/400], Loss: 0.1257\n",
      "Epoch [23/400], Loss: 0.1165\n",
      "Epoch [24/400], Loss: 0.1066\n",
      "Epoch [25/400], Loss: 0.0967\n",
      "Epoch [26/400], Loss: 0.0875\n",
      "Epoch [27/400], Loss: 0.0791\n",
      "Epoch [28/400], Loss: 0.0712\n",
      "Epoch [29/400], Loss: 0.0639\n",
      "Epoch [30/400], Loss: 0.0571\n",
      "Epoch [31/400], Loss: 0.0511\n",
      "Epoch [32/400], Loss: 0.0455\n",
      "Epoch [33/400], Loss: 0.0407\n",
      "Epoch [34/400], Loss: 0.0363\n",
      "Epoch [35/400], Loss: 0.0326\n",
      "Epoch [36/400], Loss: 0.0294\n",
      "Epoch [37/400], Loss: 0.0265\n",
      "Epoch [38/400], Loss: 0.0240\n",
      "Epoch [39/400], Loss: 0.0218\n",
      "Epoch [40/400], Loss: 0.0199\n",
      "Epoch [41/400], Loss: 0.0183\n",
      "Epoch [42/400], Loss: 0.0168\n",
      "Epoch [43/400], Loss: 0.0155\n",
      "Epoch [44/400], Loss: 0.0144\n",
      "Epoch [45/400], Loss: 0.0134\n",
      "Epoch [46/400], Loss: 0.0126\n",
      "Epoch [47/400], Loss: 0.0118\n",
      "Epoch [48/400], Loss: 0.0113\n",
      "Epoch [49/400], Loss: 0.0107\n",
      "Epoch [50/400], Loss: 0.0103\n",
      "Epoch [51/400], Loss: 0.0099\n",
      "Epoch [52/400], Loss: 0.0096\n",
      "Epoch [53/400], Loss: 0.0093\n",
      "Epoch [54/400], Loss: 0.0091\n",
      "Epoch [55/400], Loss: 0.0089\n",
      "Epoch [56/400], Loss: 0.0086\n",
      "Epoch [57/400], Loss: 0.0084\n",
      "Epoch [58/400], Loss: 0.0083\n",
      "Epoch [59/400], Loss: 0.0081\n",
      "Epoch [60/400], Loss: 0.0079\n",
      "Epoch [61/400], Loss: 0.0077\n",
      "Epoch [62/400], Loss: 0.0075\n",
      "Epoch [63/400], Loss: 0.0074\n",
      "Epoch [64/400], Loss: 0.0072\n",
      "Epoch [65/400], Loss: 0.0070\n",
      "Epoch [66/400], Loss: 0.0068\n",
      "Epoch [67/400], Loss: 0.0067\n",
      "Epoch [68/400], Loss: 0.0065\n",
      "Epoch [69/400], Loss: 0.0063\n",
      "Epoch [70/400], Loss: 0.0061\n",
      "Epoch [71/400], Loss: 0.0060\n",
      "Epoch [72/400], Loss: 0.0058\n",
      "Epoch [73/400], Loss: 0.0056\n",
      "Epoch [74/400], Loss: 0.0054\n",
      "Epoch [75/400], Loss: 0.0053\n",
      "Epoch [76/400], Loss: 0.0051\n",
      "Epoch [77/400], Loss: 0.0050\n",
      "Epoch [78/400], Loss: 0.0048\n",
      "Epoch [79/400], Loss: 0.0047\n",
      "Epoch [80/400], Loss: 0.0045\n",
      "Epoch [81/400], Loss: 0.0044\n",
      "Epoch [82/400], Loss: 0.0042\n",
      "Epoch [83/400], Loss: 0.0041\n",
      "Epoch [84/400], Loss: 0.0039\n",
      "Epoch [85/400], Loss: 0.0038\n",
      "Epoch [86/400], Loss: 0.0036\n",
      "Epoch [87/400], Loss: 0.0035\n",
      "Epoch [88/400], Loss: 0.0034\n",
      "Epoch [89/400], Loss: 0.0032\n",
      "Epoch [90/400], Loss: 0.0031\n",
      "Epoch [91/400], Loss: 0.0030\n",
      "Epoch [92/400], Loss: 0.0029\n",
      "Epoch [93/400], Loss: 0.0028\n",
      "Epoch [94/400], Loss: 0.0027\n",
      "Epoch [95/400], Loss: 0.0026\n",
      "Epoch [96/400], Loss: 0.0025\n",
      "Epoch [97/400], Loss: 0.0024\n",
      "Epoch [98/400], Loss: 0.0023\n",
      "Epoch [99/400], Loss: 0.0022\n",
      "Epoch [100/400], Loss: 0.0021\n",
      "Epoch [101/400], Loss: 0.0020\n",
      "Epoch [102/400], Loss: 0.0020\n",
      "Epoch [103/400], Loss: 0.0019\n",
      "Epoch [104/400], Loss: 0.0018\n",
      "Epoch [105/400], Loss: 0.0018\n",
      "Epoch [106/400], Loss: 0.0017\n",
      "Epoch [107/400], Loss: 0.0017\n",
      "Epoch [108/400], Loss: 0.0016\n",
      "Epoch [109/400], Loss: 0.0016\n",
      "Epoch [110/400], Loss: 0.0016\n",
      "Epoch [111/400], Loss: 0.0016\n",
      "Epoch [112/400], Loss: 0.0016\n",
      "Epoch [113/400], Loss: 0.0015\n",
      "Epoch [114/400], Loss: 0.0015\n",
      "Epoch [115/400], Loss: 0.0015\n",
      "Epoch [116/400], Loss: 0.0014\n",
      "Epoch [117/400], Loss: 0.0014\n",
      "Epoch [118/400], Loss: 0.0013\n",
      "Epoch [119/400], Loss: 0.0013\n",
      "Epoch [120/400], Loss: 0.0013\n",
      "Epoch [121/400], Loss: 0.0012\n",
      "Epoch [122/400], Loss: 0.0012\n",
      "Epoch [123/400], Loss: 0.0012\n",
      "Epoch [124/400], Loss: 0.0011\n",
      "Epoch [125/400], Loss: 0.0011\n",
      "Epoch [126/400], Loss: 0.0011\n",
      "Epoch [127/400], Loss: 0.0011\n",
      "Epoch [128/400], Loss: 0.0010\n",
      "Epoch [129/400], Loss: 0.0010\n",
      "Epoch [130/400], Loss: 0.0010\n",
      "Epoch [131/400], Loss: 0.0009\n",
      "Epoch [132/400], Loss: 0.0009\n",
      "Epoch [133/400], Loss: 0.0009\n",
      "Epoch [134/400], Loss: 0.0009\n",
      "Epoch [135/400], Loss: 0.0008\n",
      "Epoch [136/400], Loss: 0.0008\n",
      "Epoch [137/400], Loss: 0.0008\n",
      "Epoch [138/400], Loss: 0.0008\n",
      "Epoch [139/400], Loss: 0.0007\n",
      "Epoch [140/400], Loss: 0.0007\n",
      "Epoch [141/400], Loss: 0.0007\n",
      "Epoch [142/400], Loss: 0.0007\n",
      "Epoch [143/400], Loss: 0.0006\n",
      "Epoch [144/400], Loss: 0.0006\n",
      "Epoch [145/400], Loss: 0.0006\n",
      "Epoch [146/400], Loss: 0.0006\n",
      "Epoch [147/400], Loss: 0.0006\n",
      "Epoch [148/400], Loss: 0.0005\n",
      "Epoch [149/400], Loss: 0.0005\n",
      "Epoch [150/400], Loss: 0.0005\n",
      "Epoch [151/400], Loss: 0.0005\n",
      "Epoch [152/400], Loss: 0.0005\n",
      "Epoch [153/400], Loss: 0.0005\n",
      "Epoch [154/400], Loss: 0.0004\n",
      "Epoch [155/400], Loss: 0.0004\n",
      "Epoch [156/400], Loss: 0.0004\n",
      "Epoch [157/400], Loss: 0.0004\n",
      "Epoch [158/400], Loss: 0.0004\n",
      "Epoch [159/400], Loss: 0.0004\n",
      "Epoch [160/400], Loss: 0.0004\n",
      "Epoch [161/400], Loss: 0.0004\n",
      "Epoch [162/400], Loss: 0.0004\n",
      "Epoch [163/400], Loss: 0.0004\n",
      "Epoch [164/400], Loss: 0.0004\n",
      "Epoch [165/400], Loss: 0.0004\n",
      "Epoch [166/400], Loss: 0.0004\n",
      "Epoch [167/400], Loss: 0.0004\n",
      "Epoch [168/400], Loss: 0.0003\n",
      "Epoch [169/400], Loss: 0.0003\n",
      "Epoch [170/400], Loss: 0.0003\n",
      "Epoch [171/400], Loss: 0.0003\n",
      "Epoch [172/400], Loss: 0.0003\n",
      "Epoch [173/400], Loss: 0.0003\n",
      "Epoch [174/400], Loss: 0.0003\n",
      "Epoch [175/400], Loss: 0.0003\n",
      "Epoch [176/400], Loss: 0.0004\n",
      "Epoch [177/400], Loss: 0.0004\n",
      "Epoch [178/400], Loss: 0.0004\n",
      "Epoch [179/400], Loss: 0.0004\n",
      "Epoch [180/400], Loss: 0.0004\n",
      "Epoch [181/400], Loss: 0.0004\n",
      "Epoch [182/400], Loss: 0.0004\n",
      "Epoch [183/400], Loss: 0.0004\n",
      "Epoch [184/400], Loss: 0.0004\n",
      "Epoch [185/400], Loss: 0.0004\n",
      "Epoch [186/400], Loss: 0.0004\n",
      "Epoch [187/400], Loss: 0.0004\n",
      "Epoch [188/400], Loss: 0.0004\n",
      "Epoch [189/400], Loss: 0.0004\n",
      "Epoch [190/400], Loss: 0.0004\n",
      "Epoch [191/400], Loss: 0.0004\n",
      "Epoch [192/400], Loss: 0.0004\n",
      "Epoch [193/400], Loss: 0.0004\n",
      "Epoch [194/400], Loss: 0.0004\n",
      "Epoch [195/400], Loss: 0.0004\n",
      "Epoch [196/400], Loss: 0.0004\n",
      "Epoch [197/400], Loss: 0.0004\n",
      "Epoch [198/400], Loss: 0.0004\n",
      "Epoch [199/400], Loss: 0.0004\n",
      "Epoch [200/400], Loss: 0.0005\n",
      "Epoch [201/400], Loss: 0.0005\n",
      "Epoch [202/400], Loss: 0.0005\n",
      "Epoch [203/400], Loss: 0.0005\n",
      "Epoch [204/400], Loss: 0.0005\n",
      "Epoch [205/400], Loss: 0.0005\n",
      "Epoch [206/400], Loss: 0.0005\n",
      "Epoch [207/400], Loss: 0.0005\n",
      "Epoch [208/400], Loss: 0.0005\n",
      "Epoch [209/400], Loss: 0.0005\n",
      "Epoch [210/400], Loss: 0.0006\n",
      "Epoch [211/400], Loss: 0.0006\n",
      "Epoch [212/400], Loss: 0.0006\n",
      "Epoch [213/400], Loss: 0.0006\n",
      "Epoch [214/400], Loss: 0.0006\n",
      "Epoch [215/400], Loss: 0.0006\n",
      "Epoch [216/400], Loss: 0.0007\n",
      "Epoch [217/400], Loss: 0.0007\n",
      "Epoch [218/400], Loss: 0.0007\n",
      "Epoch [219/400], Loss: 0.0008\n",
      "Epoch [220/400], Loss: 0.0008\n",
      "Epoch [221/400], Loss: 0.0009\n",
      "Epoch [222/400], Loss: 0.0009\n",
      "Epoch [223/400], Loss: 0.0010\n",
      "Epoch [224/400], Loss: 0.0010\n",
      "Epoch [225/400], Loss: 0.0011\n",
      "Epoch [226/400], Loss: 0.0011\n",
      "Epoch [227/400], Loss: 0.0012\n",
      "Epoch [228/400], Loss: 0.0012\n",
      "Epoch [229/400], Loss: 0.0013\n",
      "Epoch [230/400], Loss: 0.0013\n",
      "Epoch [231/400], Loss: 0.0014\n",
      "Epoch [232/400], Loss: 0.0014\n",
      "Epoch [233/400], Loss: 0.0014\n",
      "Epoch [234/400], Loss: 0.0015\n",
      "Epoch [235/400], Loss: 0.0015\n",
      "Epoch [236/400], Loss: 0.0015\n",
      "Epoch [237/400], Loss: 0.0015\n",
      "Epoch [238/400], Loss: 0.0016\n",
      "Epoch [239/400], Loss: 0.0017\n",
      "Epoch [240/400], Loss: 0.0017\n",
      "Epoch [241/400], Loss: 0.0017\n",
      "Epoch [242/400], Loss: 0.0017\n",
      "Epoch [243/400], Loss: 0.0017\n",
      "Epoch [244/400], Loss: 0.0017\n",
      "Epoch [245/400], Loss: 0.0017\n",
      "Epoch [246/400], Loss: 0.0018\n",
      "Epoch [247/400], Loss: 0.0018\n",
      "Epoch [248/400], Loss: 0.0018\n",
      "Epoch [249/400], Loss: 0.0019\n",
      "Epoch [250/400], Loss: 0.0019\n",
      "Epoch [251/400], Loss: 0.0018\n",
      "Epoch [252/400], Loss: 0.0019\n",
      "Epoch [253/400], Loss: 0.0019\n",
      "Epoch [254/400], Loss: 0.0020\n",
      "Epoch [255/400], Loss: 0.0020\n",
      "Epoch [256/400], Loss: 0.0019\n",
      "Epoch [257/400], Loss: 0.0020\n",
      "Epoch [258/400], Loss: 0.0020\n",
      "Epoch [259/400], Loss: 0.0020\n",
      "Epoch [260/400], Loss: 0.0020\n",
      "Epoch [261/400], Loss: 0.0019\n",
      "Epoch [262/400], Loss: 0.0020\n",
      "Epoch [263/400], Loss: 0.0022\n",
      "Epoch [264/400], Loss: 0.0021\n",
      "Epoch [265/400], Loss: 0.0020\n",
      "Epoch [266/400], Loss: 0.0020\n",
      "Epoch [267/400], Loss: 0.0019\n",
      "Epoch [268/400], Loss: 0.0020\n",
      "Epoch [269/400], Loss: 0.0019\n",
      "Epoch [270/400], Loss: 0.0019\n",
      "Epoch [271/400], Loss: 0.0020\n",
      "Epoch [272/400], Loss: 0.0022\n",
      "Epoch [273/400], Loss: 0.0021\n",
      "Epoch [274/400], Loss: 0.0020\n",
      "Epoch [275/400], Loss: 0.0020\n",
      "Epoch [276/400], Loss: 0.0018\n",
      "Epoch [277/400], Loss: 0.0019\n",
      "Epoch [278/400], Loss: 0.0022\n",
      "Epoch [279/400], Loss: 0.0021\n",
      "Epoch [280/400], Loss: 0.0020\n",
      "Epoch [281/400], Loss: 0.0019\n",
      "Epoch [282/400], Loss: 0.0019\n",
      "Epoch [283/400], Loss: 0.0019\n",
      "Epoch [284/400], Loss: 0.0017\n",
      "Epoch [285/400], Loss: 0.0020\n",
      "Epoch [286/400], Loss: 0.0021\n",
      "Epoch [287/400], Loss: 0.0024\n",
      "Epoch [288/400], Loss: 0.0022\n",
      "Epoch [289/400], Loss: 0.0022\n",
      "Epoch [290/400], Loss: 0.0020\n",
      "Epoch [291/400], Loss: 0.0023\n",
      "Epoch [292/400], Loss: 0.0023\n",
      "Epoch [293/400], Loss: 0.0026\n",
      "Epoch [294/400], Loss: 0.0025\n",
      "Epoch [295/400], Loss: 0.0024\n",
      "Epoch [296/400], Loss: 0.0023\n",
      "Epoch [297/400], Loss: 0.0021\n",
      "Epoch [298/400], Loss: 0.0025\n",
      "Epoch [299/400], Loss: 0.0026\n",
      "Epoch [300/400], Loss: 0.0027\n",
      "Epoch [301/400], Loss: 0.0027\n",
      "Epoch [302/400], Loss: 0.0027\n",
      "Epoch [303/400], Loss: 0.0027\n",
      "Epoch [304/400], Loss: 0.0027\n",
      "Epoch [305/400], Loss: 0.0030\n",
      "Epoch [306/400], Loss: 0.0028\n",
      "Epoch [307/400], Loss: 0.0031\n",
      "Epoch [308/400], Loss: 0.0027\n",
      "Epoch [309/400], Loss: 0.0031\n",
      "Epoch [310/400], Loss: 0.0031\n",
      "Epoch [311/400], Loss: 0.0030\n",
      "Epoch [312/400], Loss: 0.0030\n",
      "Epoch [313/400], Loss: 0.0033\n",
      "Epoch [314/400], Loss: 0.0029\n",
      "Epoch [315/400], Loss: 0.0034\n",
      "Epoch [316/400], Loss: 0.0031\n",
      "Epoch [317/400], Loss: 0.0036\n",
      "Epoch [318/400], Loss: 0.0033\n",
      "Epoch [319/400], Loss: 0.0035\n",
      "Epoch [320/400], Loss: 0.0036\n",
      "Epoch [321/400], Loss: 0.0041\n",
      "Epoch [322/400], Loss: 0.0037\n",
      "Epoch [323/400], Loss: 0.0039\n",
      "Epoch [324/400], Loss: 0.0040\n",
      "Epoch [325/400], Loss: 0.0044\n",
      "Epoch [326/400], Loss: 0.0042\n",
      "Epoch [327/400], Loss: 0.0044\n",
      "Epoch [328/400], Loss: 0.0044\n",
      "Epoch [329/400], Loss: 0.0046\n",
      "Epoch [330/400], Loss: 0.0044\n",
      "Epoch [331/400], Loss: 0.0045\n",
      "Epoch [332/400], Loss: 0.0045\n",
      "Epoch [333/400], Loss: 0.0042\n",
      "Epoch [334/400], Loss: 0.0052\n",
      "Epoch [335/400], Loss: 0.0052\n",
      "Epoch [336/400], Loss: 0.0051\n",
      "Epoch [337/400], Loss: 0.0051\n",
      "Epoch [338/400], Loss: 0.0051\n",
      "Epoch [339/400], Loss: 0.0052\n",
      "Epoch [340/400], Loss: 0.0057\n",
      "Epoch [341/400], Loss: 0.0057\n",
      "Epoch [342/400], Loss: 0.0058\n",
      "Epoch [343/400], Loss: 0.0063\n",
      "Epoch [344/400], Loss: 0.0064\n",
      "Epoch [345/400], Loss: 0.0065\n",
      "Epoch [346/400], Loss: 0.0067\n",
      "Epoch [347/400], Loss: 0.0068\n",
      "Epoch [348/400], Loss: 0.0070\n",
      "Epoch [349/400], Loss: 0.0069\n",
      "Epoch [350/400], Loss: 0.0067\n",
      "Epoch [351/400], Loss: 0.0072\n",
      "Epoch [352/400], Loss: 0.0074\n",
      "Epoch [353/400], Loss: 0.0070\n",
      "Epoch [354/400], Loss: 0.0067\n",
      "Epoch [355/400], Loss: 0.0063\n",
      "Epoch [356/400], Loss: 0.0065\n",
      "Epoch [357/400], Loss: 0.0064\n",
      "Epoch [358/400], Loss: 0.0061\n",
      "Epoch [359/400], Loss: 0.0067\n",
      "Epoch [360/400], Loss: 0.0064\n",
      "Epoch [361/400], Loss: 0.0062\n",
      "Epoch [362/400], Loss: 0.0061\n",
      "Epoch [363/400], Loss: 0.0060\n",
      "Epoch [364/400], Loss: 0.0066\n",
      "Epoch [365/400], Loss: 0.0064\n",
      "Epoch [366/400], Loss: 0.0062\n",
      "Epoch [367/400], Loss: 0.0060\n",
      "Epoch [368/400], Loss: 0.0060\n",
      "Epoch [369/400], Loss: 0.0059\n",
      "Epoch [370/400], Loss: 0.0061\n",
      "Epoch [371/400], Loss: 0.0057\n",
      "Epoch [372/400], Loss: 0.0062\n",
      "Epoch [373/400], Loss: 0.0061\n",
      "Epoch [374/400], Loss: 0.0062\n",
      "Epoch [375/400], Loss: 0.0060\n",
      "Epoch [376/400], Loss: 0.0061\n",
      "Epoch [377/400], Loss: 0.0064\n",
      "Epoch [378/400], Loss: 0.0064\n",
      "Epoch [379/400], Loss: 0.0062\n",
      "Epoch [380/400], Loss: 0.0063\n",
      "Epoch [381/400], Loss: 0.0062\n",
      "Epoch [382/400], Loss: 0.0065\n",
      "Epoch [383/400], Loss: 0.0064\n",
      "Epoch [384/400], Loss: 0.0064\n",
      "Epoch [385/400], Loss: 0.0066\n",
      "Epoch [386/400], Loss: 0.0061\n",
      "Epoch [387/400], Loss: 0.0067\n",
      "Epoch [388/400], Loss: 0.0066\n",
      "Epoch [389/400], Loss: 0.0064\n",
      "Epoch [390/400], Loss: 0.0065\n",
      "Epoch [391/400], Loss: 0.0068\n",
      "Epoch [392/400], Loss: 0.0068\n",
      "Epoch [393/400], Loss: 0.0070\n",
      "Epoch [394/400], Loss: 0.0070\n",
      "Epoch [395/400], Loss: 0.0069\n",
      "Epoch [396/400], Loss: 0.0070\n",
      "Epoch [397/400], Loss: 0.0071\n",
      "Epoch [398/400], Loss: 0.0071\n",
      "Epoch [399/400], Loss: 0.0072\n",
      "Epoch [400/400], Loss: 0.0072\n",
      "Accuracy: 51.1905%\n",
      "Epoch [1/400], Loss: 0.0024\n",
      "Epoch [2/400], Loss: 0.0027\n",
      "Epoch [3/400], Loss: 0.0036\n",
      "Epoch [4/400], Loss: 0.0063\n",
      "Epoch [5/400], Loss: 0.0107\n",
      "Epoch [6/400], Loss: 0.0170\n",
      "Epoch [7/400], Loss: 0.0256\n",
      "Epoch [8/400], Loss: 0.0352\n",
      "Epoch [9/400], Loss: 0.0433\n",
      "Epoch [10/400], Loss: 0.0601\n",
      "Epoch [11/400], Loss: 0.0640\n",
      "Epoch [12/400], Loss: 0.0681\n",
      "Epoch [13/400], Loss: 0.0702\n",
      "Epoch [14/400], Loss: 0.0726\n",
      "Epoch [15/400], Loss: 0.0743\n",
      "Epoch [16/400], Loss: 0.0724\n",
      "Epoch [17/400], Loss: 0.0704\n",
      "Epoch [18/400], Loss: 0.0658\n",
      "Epoch [19/400], Loss: 0.0647\n",
      "Epoch [20/400], Loss: 0.0529\n",
      "Epoch [21/400], Loss: 0.0278\n",
      "Epoch [22/400], Loss: 0.0386\n",
      "Epoch [23/400], Loss: 0.0193\n",
      "Epoch [24/400], Loss: 0.0299\n",
      "Epoch [25/400], Loss: 0.0328\n",
      "Epoch [26/400], Loss: 0.0258\n",
      "Epoch [27/400], Loss: 0.0281\n",
      "Epoch [28/400], Loss: 0.0113\n",
      "Epoch [29/400], Loss: 0.0224\n",
      "Epoch [30/400], Loss: 0.0239\n",
      "Epoch [31/400], Loss: 0.0228\n",
      "Epoch [32/400], Loss: 0.0110\n",
      "Epoch [33/400], Loss: 0.0145\n",
      "Epoch [34/400], Loss: 0.0134\n",
      "Epoch [35/400], Loss: 0.0097\n",
      "Epoch [36/400], Loss: 0.0161\n",
      "Epoch [37/400], Loss: 0.0147\n",
      "Epoch [38/400], Loss: 0.0174\n",
      "Epoch [39/400], Loss: 0.0198\n",
      "Epoch [40/400], Loss: 0.0172\n",
      "Epoch [41/400], Loss: 0.0082\n",
      "Epoch [42/400], Loss: 0.0110\n",
      "Epoch [43/400], Loss: 0.0080\n",
      "Epoch [44/400], Loss: 0.0092\n",
      "Epoch [45/400], Loss: 0.0096\n",
      "Epoch [46/400], Loss: 0.0102\n",
      "Epoch [47/400], Loss: 0.0080\n",
      "Epoch [48/400], Loss: 0.0109\n",
      "Epoch [49/400], Loss: 0.0119\n",
      "Epoch [50/400], Loss: 0.0119\n",
      "Epoch [51/400], Loss: 0.0141\n",
      "Epoch [52/400], Loss: 0.0151\n",
      "Epoch [53/400], Loss: 0.0164\n",
      "Epoch [54/400], Loss: 0.0165\n",
      "Epoch [55/400], Loss: 0.0169\n",
      "Epoch [56/400], Loss: 0.0172\n",
      "Epoch [57/400], Loss: 0.0174\n",
      "Epoch [58/400], Loss: 0.0174\n",
      "Epoch [59/400], Loss: 0.0173\n",
      "Epoch [60/400], Loss: 0.0171\n",
      "Epoch [61/400], Loss: 0.0168\n",
      "Epoch [62/400], Loss: 0.0165\n",
      "Epoch [63/400], Loss: 0.0161\n",
      "Epoch [64/400], Loss: 0.0158\n",
      "Epoch [65/400], Loss: 0.0155\n",
      "Epoch [66/400], Loss: 0.0152\n",
      "Epoch [67/400], Loss: 0.0149\n",
      "Epoch [68/400], Loss: 0.0145\n",
      "Epoch [69/400], Loss: 0.0142\n",
      "Epoch [70/400], Loss: 0.0139\n",
      "Epoch [71/400], Loss: 0.0135\n",
      "Epoch [72/400], Loss: 0.0131\n",
      "Epoch [73/400], Loss: 0.0127\n",
      "Epoch [74/400], Loss: 0.0123\n",
      "Epoch [75/400], Loss: 0.0119\n",
      "Epoch [76/400], Loss: 0.0114\n",
      "Epoch [77/400], Loss: 0.0108\n",
      "Epoch [78/400], Loss: 0.0102\n",
      "Epoch [79/400], Loss: 0.0095\n",
      "Epoch [80/400], Loss: 0.0088\n",
      "Epoch [81/400], Loss: 0.0080\n",
      "Epoch [82/400], Loss: 0.0074\n",
      "Epoch [83/400], Loss: 0.0070\n",
      "Epoch [84/400], Loss: 0.0067\n",
      "Epoch [85/400], Loss: 0.0064\n",
      "Epoch [86/400], Loss: 0.0062\n",
      "Epoch [87/400], Loss: 0.0060\n",
      "Epoch [88/400], Loss: 0.0058\n",
      "Epoch [89/400], Loss: 0.0057\n",
      "Epoch [90/400], Loss: 0.0055\n",
      "Epoch [91/400], Loss: 0.0054\n",
      "Epoch [92/400], Loss: 0.0053\n",
      "Epoch [93/400], Loss: 0.0051\n",
      "Epoch [94/400], Loss: 0.0050\n",
      "Epoch [95/400], Loss: 0.0049\n",
      "Epoch [96/400], Loss: 0.0047\n",
      "Epoch [97/400], Loss: 0.0046\n",
      "Epoch [98/400], Loss: 0.0044\n",
      "Epoch [99/400], Loss: 0.0043\n",
      "Epoch [100/400], Loss: 0.0042\n",
      "Epoch [101/400], Loss: 0.0040\n",
      "Epoch [102/400], Loss: 0.0038\n",
      "Epoch [103/400], Loss: 0.0037\n",
      "Epoch [104/400], Loss: 0.0035\n",
      "Epoch [105/400], Loss: 0.0034\n",
      "Epoch [106/400], Loss: 0.0033\n",
      "Epoch [107/400], Loss: 0.0032\n",
      "Epoch [108/400], Loss: 0.0031\n",
      "Epoch [109/400], Loss: 0.0030\n",
      "Epoch [110/400], Loss: 0.0030\n",
      "Epoch [111/400], Loss: 0.0029\n",
      "Epoch [112/400], Loss: 0.0028\n",
      "Epoch [113/400], Loss: 0.0027\n",
      "Epoch [114/400], Loss: 0.0026\n",
      "Epoch [115/400], Loss: 0.0025\n",
      "Epoch [116/400], Loss: 0.0024\n",
      "Epoch [117/400], Loss: 0.0023\n",
      "Epoch [118/400], Loss: 0.0023\n",
      "Epoch [119/400], Loss: 0.0022\n",
      "Epoch [120/400], Loss: 0.0021\n",
      "Epoch [121/400], Loss: 0.0021\n",
      "Epoch [122/400], Loss: 0.0020\n",
      "Epoch [123/400], Loss: 0.0019\n",
      "Epoch [124/400], Loss: 0.0019\n",
      "Epoch [125/400], Loss: 0.0019\n",
      "Epoch [126/400], Loss: 0.0018\n",
      "Epoch [127/400], Loss: 0.0018\n",
      "Epoch [128/400], Loss: 0.0017\n",
      "Epoch [129/400], Loss: 0.0017\n",
      "Epoch [130/400], Loss: 0.0017\n",
      "Epoch [131/400], Loss: 0.0016\n",
      "Epoch [132/400], Loss: 0.0017\n",
      "Epoch [133/400], Loss: 0.0016\n",
      "Epoch [134/400], Loss: 0.0016\n",
      "Epoch [135/400], Loss: 0.0016\n",
      "Epoch [136/400], Loss: 0.0016\n",
      "Epoch [137/400], Loss: 0.0016\n",
      "Epoch [138/400], Loss: 0.0015\n",
      "Epoch [139/400], Loss: 0.0015\n",
      "Epoch [140/400], Loss: 0.0015\n",
      "Epoch [141/400], Loss: 0.0015\n",
      "Epoch [142/400], Loss: 0.0015\n",
      "Epoch [143/400], Loss: 0.0015\n",
      "Epoch [144/400], Loss: 0.0015\n",
      "Epoch [145/400], Loss: 0.0015\n",
      "Epoch [146/400], Loss: 0.0015\n",
      "Epoch [147/400], Loss: 0.0015\n",
      "Epoch [148/400], Loss: 0.0015\n",
      "Epoch [149/400], Loss: 0.0014\n",
      "Epoch [150/400], Loss: 0.0010\n",
      "Epoch [151/400], Loss: 0.0012\n",
      "Epoch [152/400], Loss: 0.0009\n",
      "Epoch [153/400], Loss: 0.0007\n",
      "Epoch [154/400], Loss: 0.0006\n",
      "Epoch [155/400], Loss: 0.0005\n",
      "Epoch [156/400], Loss: 0.0005\n",
      "Epoch [157/400], Loss: 0.0005\n",
      "Epoch [158/400], Loss: 0.0005\n",
      "Epoch [159/400], Loss: 0.0008\n",
      "Epoch [160/400], Loss: 0.0015\n",
      "Epoch [161/400], Loss: 0.0019\n",
      "Epoch [162/400], Loss: 0.0020\n",
      "Epoch [163/400], Loss: 0.0024\n",
      "Epoch [164/400], Loss: 0.0024\n",
      "Epoch [165/400], Loss: 0.0019\n",
      "Epoch [166/400], Loss: 0.0018\n",
      "Epoch [167/400], Loss: 0.0017\n",
      "Epoch [168/400], Loss: 0.0016\n",
      "Epoch [169/400], Loss: 0.0015\n",
      "Epoch [170/400], Loss: 0.0018\n",
      "Epoch [171/400], Loss: 0.0019\n",
      "Epoch [172/400], Loss: 0.0019\n",
      "Epoch [173/400], Loss: 0.0014\n",
      "Epoch [174/400], Loss: 0.0013\n",
      "Epoch [175/400], Loss: 0.0013\n",
      "Epoch [176/400], Loss: 0.0013\n",
      "Epoch [177/400], Loss: 0.0014\n",
      "Epoch [178/400], Loss: 0.0018\n",
      "Epoch [179/400], Loss: 0.0017\n",
      "Epoch [180/400], Loss: 0.0018\n",
      "Epoch [181/400], Loss: 0.0017\n",
      "Epoch [182/400], Loss: 0.0018\n",
      "Epoch [183/400], Loss: 0.0018\n",
      "Epoch [184/400], Loss: 0.0019\n",
      "Epoch [185/400], Loss: 0.0020\n",
      "Epoch [186/400], Loss: 0.0020\n",
      "Epoch [187/400], Loss: 0.0021\n",
      "Epoch [188/400], Loss: 0.0021\n",
      "Epoch [189/400], Loss: 0.0021\n",
      "Epoch [190/400], Loss: 0.0021\n",
      "Epoch [191/400], Loss: 0.0021\n",
      "Epoch [192/400], Loss: 0.0020\n",
      "Epoch [193/400], Loss: 0.0020\n",
      "Epoch [194/400], Loss: 0.0019\n",
      "Epoch [195/400], Loss: 0.0019\n",
      "Epoch [196/400], Loss: 0.0019\n",
      "Epoch [197/400], Loss: 0.0018\n",
      "Epoch [198/400], Loss: 0.0017\n",
      "Epoch [199/400], Loss: 0.0019\n",
      "Epoch [200/400], Loss: 0.0017\n",
      "Epoch [201/400], Loss: 0.0014\n",
      "Epoch [202/400], Loss: 0.0014\n",
      "Epoch [203/400], Loss: 0.0013\n",
      "Epoch [204/400], Loss: 0.0013\n",
      "Epoch [205/400], Loss: 0.0012\n",
      "Epoch [206/400], Loss: 0.0014\n",
      "Epoch [207/400], Loss: 0.0012\n",
      "Epoch [208/400], Loss: 0.0010\n",
      "Epoch [209/400], Loss: 0.0009\n",
      "Epoch [210/400], Loss: 0.0010\n",
      "Epoch [211/400], Loss: 0.0008\n",
      "Epoch [212/400], Loss: 0.0010\n",
      "Epoch [213/400], Loss: 0.0010\n",
      "Epoch [214/400], Loss: 0.0010\n",
      "Epoch [215/400], Loss: 0.0009\n",
      "Epoch [216/400], Loss: 0.0009\n",
      "Epoch [217/400], Loss: 0.0009\n",
      "Epoch [218/400], Loss: 0.0009\n",
      "Epoch [219/400], Loss: 0.0009\n",
      "Epoch [220/400], Loss: 0.0008\n",
      "Epoch [221/400], Loss: 0.0008\n",
      "Epoch [222/400], Loss: 0.0008\n",
      "Epoch [223/400], Loss: 0.0007\n",
      "Epoch [224/400], Loss: 0.0008\n",
      "Epoch [225/400], Loss: 0.0008\n",
      "Epoch [226/400], Loss: 0.0008\n",
      "Epoch [227/400], Loss: 0.0008\n",
      "Epoch [228/400], Loss: 0.0007\n",
      "Epoch [229/400], Loss: 0.0007\n",
      "Epoch [230/400], Loss: 0.0007\n",
      "Epoch [231/400], Loss: 0.0008\n",
      "Epoch [232/400], Loss: 0.0008\n",
      "Epoch [233/400], Loss: 0.0008\n",
      "Epoch [234/400], Loss: 0.0008\n",
      "Epoch [235/400], Loss: 0.0008\n",
      "Epoch [236/400], Loss: 0.0008\n",
      "Epoch [237/400], Loss: 0.0008\n",
      "Epoch [238/400], Loss: 0.0008\n",
      "Epoch [239/400], Loss: 0.0007\n",
      "Epoch [240/400], Loss: 0.0007\n",
      "Epoch [241/400], Loss: 0.0007\n",
      "Epoch [242/400], Loss: 0.0006\n",
      "Epoch [243/400], Loss: 0.0006\n",
      "Epoch [244/400], Loss: 0.0006\n",
      "Epoch [245/400], Loss: 0.0006\n",
      "Epoch [246/400], Loss: 0.0006\n",
      "Epoch [247/400], Loss: 0.0005\n",
      "Epoch [248/400], Loss: 0.0004\n",
      "Epoch [249/400], Loss: 0.0004\n",
      "Epoch [250/400], Loss: 0.0003\n",
      "Epoch [251/400], Loss: 0.0004\n",
      "Epoch [252/400], Loss: 0.0004\n",
      "Epoch [253/400], Loss: 0.0004\n",
      "Epoch [254/400], Loss: 0.0003\n",
      "Epoch [255/400], Loss: 0.0004\n",
      "Epoch [256/400], Loss: 0.0004\n",
      "Epoch [257/400], Loss: 0.0004\n",
      "Epoch [258/400], Loss: 0.0003\n",
      "Epoch [259/400], Loss: 0.0002\n",
      "Epoch [260/400], Loss: 0.0003\n",
      "Epoch [261/400], Loss: 0.0002\n",
      "Epoch [262/400], Loss: 0.0002\n",
      "Epoch [263/400], Loss: 0.0002\n",
      "Epoch [264/400], Loss: 0.0001\n",
      "Epoch [265/400], Loss: 0.0002\n",
      "Epoch [266/400], Loss: 0.0003\n",
      "Epoch [267/400], Loss: 0.0003\n",
      "Epoch [268/400], Loss: 0.0003\n",
      "Epoch [269/400], Loss: 0.0003\n",
      "Epoch [270/400], Loss: 0.0003\n",
      "Epoch [271/400], Loss: 0.0003\n",
      "Epoch [272/400], Loss: 0.0003\n",
      "Epoch [273/400], Loss: 0.0002\n",
      "Epoch [274/400], Loss: 0.0002\n",
      "Epoch [275/400], Loss: 0.0002\n",
      "Epoch [276/400], Loss: 0.0001\n",
      "Epoch [277/400], Loss: 0.0002\n",
      "Epoch [278/400], Loss: 0.0000\n",
      "Epoch [279/400], Loss: 0.0001\n",
      "Epoch [280/400], Loss: 0.0001\n",
      "Epoch [281/400], Loss: 0.0001\n",
      "Epoch [282/400], Loss: 0.0001\n",
      "Epoch [283/400], Loss: 0.0001\n",
      "Epoch [284/400], Loss: 0.0001\n",
      "Epoch [285/400], Loss: 0.0001\n",
      "Epoch [286/400], Loss: 0.0001\n",
      "Epoch [287/400], Loss: 0.0001\n",
      "Epoch [288/400], Loss: 0.0002\n",
      "Epoch [289/400], Loss: 0.0002\n",
      "Epoch [290/400], Loss: 0.0002\n",
      "Epoch [291/400], Loss: 0.0002\n",
      "Epoch [292/400], Loss: 0.0002\n",
      "Epoch [293/400], Loss: 0.0002\n",
      "Epoch [294/400], Loss: 0.0002\n",
      "Epoch [295/400], Loss: 0.0001\n",
      "Epoch [296/400], Loss: 0.0001\n",
      "Epoch [297/400], Loss: 0.0002\n",
      "Epoch [298/400], Loss: 0.0001\n",
      "Epoch [299/400], Loss: 0.0001\n",
      "Epoch [300/400], Loss: 0.0001\n",
      "Epoch [301/400], Loss: 0.0002\n",
      "Epoch [302/400], Loss: 0.0002\n",
      "Epoch [303/400], Loss: 0.0002\n",
      "Epoch [304/400], Loss: 0.0003\n",
      "Epoch [305/400], Loss: 0.0002\n",
      "Epoch [306/400], Loss: 0.0002\n",
      "Epoch [307/400], Loss: 0.0002\n",
      "Epoch [308/400], Loss: 0.0002\n",
      "Epoch [309/400], Loss: 0.0002\n",
      "Epoch [310/400], Loss: 0.0001\n",
      "Epoch [311/400], Loss: 0.0001\n",
      "Epoch [312/400], Loss: 0.0001\n",
      "Epoch [313/400], Loss: 0.0001\n",
      "Epoch [314/400], Loss: 0.0001\n",
      "Epoch [315/400], Loss: 0.0000\n",
      "Epoch [316/400], Loss: 0.0000\n",
      "Epoch [317/400], Loss: 0.0000\n",
      "Epoch [318/400], Loss: 0.0000\n",
      "Epoch [319/400], Loss: 0.0000\n",
      "Epoch [320/400], Loss: 0.0000\n",
      "Epoch [321/400], Loss: 0.0000\n",
      "Epoch [322/400], Loss: 0.0000\n",
      "Epoch [323/400], Loss: 0.0001\n",
      "Epoch [324/400], Loss: 0.0001\n",
      "Epoch [325/400], Loss: 0.0001\n",
      "Epoch [326/400], Loss: 0.0001\n",
      "Epoch [327/400], Loss: 0.0001\n",
      "Epoch [328/400], Loss: 0.0001\n",
      "Epoch [329/400], Loss: 0.0001\n",
      "Epoch [330/400], Loss: 0.0001\n",
      "Epoch [331/400], Loss: 0.0001\n",
      "Epoch [332/400], Loss: 0.0001\n",
      "Epoch [333/400], Loss: 0.0001\n",
      "Epoch [334/400], Loss: 0.0002\n",
      "Epoch [335/400], Loss: 0.0001\n",
      "Epoch [336/400], Loss: 0.0001\n",
      "Epoch [337/400], Loss: 0.0002\n",
      "Epoch [338/400], Loss: 0.0002\n",
      "Epoch [339/400], Loss: 0.0003\n",
      "Epoch [340/400], Loss: 0.0002\n",
      "Epoch [341/400], Loss: 0.0003\n",
      "Epoch [342/400], Loss: 0.0001\n",
      "Epoch [343/400], Loss: 0.0002\n",
      "Epoch [344/400], Loss: 0.0002\n",
      "Epoch [345/400], Loss: 0.0002\n",
      "Epoch [346/400], Loss: 0.0002\n",
      "Epoch [347/400], Loss: 0.0003\n",
      "Epoch [348/400], Loss: 0.0003\n",
      "Epoch [349/400], Loss: 0.0002\n",
      "Epoch [350/400], Loss: 0.0003\n",
      "Epoch [351/400], Loss: 0.0003\n",
      "Epoch [352/400], Loss: 0.0002\n",
      "Epoch [353/400], Loss: 0.0002\n",
      "Epoch [354/400], Loss: 0.0002\n",
      "Epoch [355/400], Loss: 0.0003\n",
      "Epoch [356/400], Loss: 0.0003\n",
      "Epoch [357/400], Loss: 0.0002\n",
      "Epoch [358/400], Loss: 0.0002\n",
      "Epoch [359/400], Loss: 0.0002\n",
      "Epoch [360/400], Loss: 0.0003\n",
      "Epoch [361/400], Loss: 0.0002\n",
      "Epoch [362/400], Loss: 0.0003\n",
      "Epoch [363/400], Loss: 0.0003\n",
      "Epoch [364/400], Loss: 0.0003\n",
      "Epoch [365/400], Loss: 0.0003\n",
      "Epoch [366/400], Loss: 0.0003\n",
      "Epoch [367/400], Loss: 0.0002\n",
      "Epoch [368/400], Loss: 0.0002\n",
      "Epoch [369/400], Loss: 0.0004\n",
      "Epoch [370/400], Loss: 0.0003\n",
      "Epoch [371/400], Loss: 0.0002\n",
      "Epoch [372/400], Loss: 0.0002\n",
      "Epoch [373/400], Loss: 0.0002\n",
      "Epoch [374/400], Loss: 0.0002\n",
      "Epoch [375/400], Loss: 0.0001\n",
      "Epoch [376/400], Loss: 0.0002\n",
      "Epoch [377/400], Loss: 0.0002\n",
      "Epoch [378/400], Loss: 0.0002\n",
      "Epoch [379/400], Loss: 0.0002\n",
      "Epoch [380/400], Loss: 0.0002\n",
      "Epoch [381/400], Loss: 0.0002\n",
      "Epoch [382/400], Loss: 0.0002\n",
      "Epoch [383/400], Loss: 0.0001\n",
      "Epoch [384/400], Loss: 0.0002\n",
      "Epoch [385/400], Loss: 0.0001\n",
      "Epoch [386/400], Loss: 0.0001\n",
      "Epoch [387/400], Loss: 0.0001\n",
      "Epoch [388/400], Loss: 0.0001\n",
      "Epoch [389/400], Loss: 0.0002\n",
      "Epoch [390/400], Loss: 0.0002\n",
      "Epoch [391/400], Loss: 0.0002\n",
      "Epoch [392/400], Loss: 0.0001\n",
      "Epoch [393/400], Loss: 0.0001\n",
      "Epoch [394/400], Loss: 0.0002\n",
      "Epoch [395/400], Loss: 0.0001\n",
      "Epoch [396/400], Loss: 0.0001\n",
      "Epoch [397/400], Loss: 0.0001\n",
      "Epoch [398/400], Loss: 0.0001\n",
      "Epoch [399/400], Loss: 0.0002\n",
      "Epoch [400/400], Loss: 0.0001\n",
      "Accuracy: 52.3810%\n",
      "Epoch [1/400], Loss: 0.0012\n",
      "Epoch [2/400], Loss: 0.0012\n",
      "Epoch [3/400], Loss: 0.0027\n",
      "Epoch [4/400], Loss: 0.0052\n",
      "Epoch [5/400], Loss: 0.0078\n",
      "Epoch [6/400], Loss: 0.0113\n",
      "Epoch [7/400], Loss: 0.0340\n",
      "Epoch [8/400], Loss: 0.0216\n",
      "Epoch [9/400], Loss: 0.0325\n",
      "Epoch [10/400], Loss: 0.0273\n",
      "Epoch [11/400], Loss: 0.0380\n",
      "Epoch [12/400], Loss: 0.0251\n",
      "Epoch [13/400], Loss: 0.0246\n",
      "Epoch [14/400], Loss: 0.0246\n",
      "Epoch [15/400], Loss: 0.0245\n",
      "Epoch [16/400], Loss: 0.0188\n",
      "Epoch [17/400], Loss: 0.0234\n",
      "Epoch [18/400], Loss: 0.0308\n",
      "Epoch [19/400], Loss: 0.0221\n",
      "Epoch [20/400], Loss: 0.0197\n",
      "Epoch [21/400], Loss: 0.0190\n",
      "Epoch [22/400], Loss: 0.0191\n",
      "Epoch [23/400], Loss: 0.0200\n",
      "Epoch [24/400], Loss: 0.0172\n",
      "Epoch [25/400], Loss: 0.0119\n",
      "Epoch [26/400], Loss: 0.0168\n",
      "Epoch [27/400], Loss: 0.0157\n",
      "Epoch [28/400], Loss: 0.0227\n",
      "Epoch [29/400], Loss: 0.0225\n",
      "Epoch [30/400], Loss: 0.0206\n",
      "Epoch [31/400], Loss: 0.0188\n",
      "Epoch [32/400], Loss: 0.0188\n",
      "Epoch [33/400], Loss: 0.0155\n",
      "Epoch [34/400], Loss: 0.0140\n",
      "Epoch [35/400], Loss: 0.0163\n",
      "Epoch [36/400], Loss: 0.0091\n",
      "Epoch [37/400], Loss: 0.0166\n",
      "Epoch [38/400], Loss: 0.0123\n",
      "Epoch [39/400], Loss: 0.0131\n",
      "Epoch [40/400], Loss: 0.0134\n",
      "Epoch [41/400], Loss: 0.0131\n",
      "Epoch [42/400], Loss: 0.0135\n",
      "Epoch [43/400], Loss: 0.0126\n",
      "Epoch [44/400], Loss: 0.0077\n",
      "Epoch [45/400], Loss: 0.0051\n",
      "Epoch [46/400], Loss: 0.0047\n",
      "Epoch [47/400], Loss: 0.0062\n",
      "Epoch [48/400], Loss: 0.0087\n",
      "Epoch [49/400], Loss: 0.0049\n",
      "Epoch [50/400], Loss: 0.0093\n",
      "Epoch [51/400], Loss: 0.0087\n",
      "Epoch [52/400], Loss: 0.0087\n",
      "Epoch [53/400], Loss: 0.0091\n",
      "Epoch [54/400], Loss: 0.0092\n",
      "Epoch [55/400], Loss: 0.0094\n",
      "Epoch [56/400], Loss: 0.0092\n",
      "Epoch [57/400], Loss: 0.0094\n",
      "Epoch [58/400], Loss: 0.0089\n",
      "Epoch [59/400], Loss: 0.0093\n",
      "Epoch [60/400], Loss: 0.0088\n",
      "Epoch [61/400], Loss: 0.0090\n",
      "Epoch [62/400], Loss: 0.0087\n",
      "Epoch [63/400], Loss: 0.0087\n",
      "Epoch [64/400], Loss: 0.0085\n",
      "Epoch [65/400], Loss: 0.0084\n",
      "Epoch [66/400], Loss: 0.0082\n",
      "Epoch [67/400], Loss: 0.0028\n",
      "Epoch [68/400], Loss: 0.0057\n",
      "Epoch [69/400], Loss: 0.0057\n",
      "Epoch [70/400], Loss: 0.0058\n",
      "Epoch [71/400], Loss: 0.0059\n",
      "Epoch [72/400], Loss: 0.0060\n",
      "Epoch [73/400], Loss: 0.0061\n",
      "Epoch [74/400], Loss: 0.0061\n",
      "Epoch [75/400], Loss: 0.0062\n",
      "Epoch [76/400], Loss: 0.0062\n",
      "Epoch [77/400], Loss: 0.0062\n",
      "Epoch [78/400], Loss: 0.0062\n",
      "Epoch [79/400], Loss: 0.0062\n",
      "Epoch [80/400], Loss: 0.0062\n",
      "Epoch [81/400], Loss: 0.0062\n",
      "Epoch [82/400], Loss: 0.0062\n",
      "Epoch [83/400], Loss: 0.0062\n",
      "Epoch [84/400], Loss: 0.0061\n",
      "Epoch [85/400], Loss: 0.0061\n",
      "Epoch [86/400], Loss: 0.0060\n",
      "Epoch [87/400], Loss: 0.0060\n",
      "Epoch [88/400], Loss: 0.0060\n",
      "Epoch [89/400], Loss: 0.0059\n",
      "Epoch [90/400], Loss: 0.0059\n",
      "Epoch [91/400], Loss: 0.0058\n",
      "Epoch [92/400], Loss: 0.0058\n",
      "Epoch [93/400], Loss: 0.0058\n",
      "Epoch [94/400], Loss: 0.0058\n",
      "Epoch [95/400], Loss: 0.0058\n",
      "Epoch [96/400], Loss: 0.0058\n",
      "Epoch [97/400], Loss: 0.0058\n",
      "Epoch [98/400], Loss: 0.0058\n",
      "Epoch [99/400], Loss: 0.0059\n",
      "Epoch [100/400], Loss: 0.0059\n",
      "Epoch [101/400], Loss: 0.0060\n",
      "Epoch [102/400], Loss: 0.0060\n",
      "Epoch [103/400], Loss: 0.0060\n",
      "Epoch [104/400], Loss: 0.0061\n",
      "Epoch [105/400], Loss: 0.0061\n",
      "Epoch [106/400], Loss: 0.0061\n",
      "Epoch [107/400], Loss: 0.0061\n",
      "Epoch [108/400], Loss: 0.0061\n",
      "Epoch [109/400], Loss: 0.0061\n",
      "Epoch [110/400], Loss: 0.0061\n",
      "Epoch [111/400], Loss: 0.0060\n",
      "Epoch [112/400], Loss: 0.0061\n",
      "Epoch [113/400], Loss: 0.0062\n",
      "Epoch [114/400], Loss: 0.0062\n",
      "Epoch [115/400], Loss: 0.0063\n",
      "Epoch [116/400], Loss: 0.0063\n",
      "Epoch [117/400], Loss: 0.0064\n",
      "Epoch [118/400], Loss: 0.0065\n",
      "Epoch [119/400], Loss: 0.0065\n",
      "Epoch [120/400], Loss: 0.0066\n",
      "Epoch [121/400], Loss: 0.0067\n",
      "Epoch [122/400], Loss: 0.0067\n",
      "Epoch [123/400], Loss: 0.0067\n",
      "Epoch [124/400], Loss: 0.0068\n",
      "Epoch [125/400], Loss: 0.0068\n",
      "Epoch [126/400], Loss: 0.0068\n",
      "Epoch [127/400], Loss: 0.0068\n",
      "Epoch [128/400], Loss: 0.0067\n",
      "Epoch [129/400], Loss: 0.0067\n",
      "Epoch [130/400], Loss: 0.0067\n",
      "Epoch [131/400], Loss: 0.0066\n",
      "Epoch [132/400], Loss: 0.0066\n",
      "Epoch [133/400], Loss: 0.0065\n",
      "Epoch [134/400], Loss: 0.0064\n",
      "Epoch [135/400], Loss: 0.0064\n",
      "Epoch [136/400], Loss: 0.0063\n",
      "Epoch [137/400], Loss: 0.0062\n",
      "Epoch [138/400], Loss: 0.0061\n",
      "Epoch [139/400], Loss: 0.0060\n",
      "Epoch [140/400], Loss: 0.0059\n",
      "Epoch [141/400], Loss: 0.0058\n",
      "Epoch [142/400], Loss: 0.0058\n",
      "Epoch [143/400], Loss: 0.0057\n",
      "Epoch [144/400], Loss: 0.0056\n",
      "Epoch [145/400], Loss: 0.0055\n",
      "Epoch [146/400], Loss: 0.0054\n",
      "Epoch [147/400], Loss: 0.0053\n",
      "Epoch [148/400], Loss: 0.0052\n",
      "Epoch [149/400], Loss: 0.0051\n",
      "Epoch [150/400], Loss: 0.0050\n",
      "Epoch [151/400], Loss: 0.0049\n",
      "Epoch [152/400], Loss: 0.0048\n",
      "Epoch [153/400], Loss: 0.0048\n",
      "Epoch [154/400], Loss: 0.0047\n",
      "Epoch [155/400], Loss: 0.0046\n",
      "Epoch [156/400], Loss: 0.0045\n",
      "Epoch [157/400], Loss: 0.0044\n",
      "Epoch [158/400], Loss: 0.0044\n",
      "Epoch [159/400], Loss: 0.0043\n",
      "Epoch [160/400], Loss: 0.0042\n",
      "Epoch [161/400], Loss: 0.0041\n",
      "Epoch [162/400], Loss: 0.0041\n",
      "Epoch [163/400], Loss: 0.0040\n",
      "Epoch [164/400], Loss: 0.0039\n",
      "Epoch [165/400], Loss: 0.0039\n",
      "Epoch [166/400], Loss: 0.0038\n",
      "Epoch [167/400], Loss: 0.0037\n",
      "Epoch [168/400], Loss: 0.0036\n",
      "Epoch [169/400], Loss: 0.0035\n",
      "Epoch [170/400], Loss: 0.0034\n",
      "Epoch [171/400], Loss: 0.0033\n",
      "Epoch [172/400], Loss: 0.0033\n",
      "Epoch [173/400], Loss: 0.0032\n",
      "Epoch [174/400], Loss: 0.0032\n",
      "Epoch [175/400], Loss: 0.0031\n",
      "Epoch [176/400], Loss: 0.0030\n",
      "Epoch [177/400], Loss: 0.0030\n",
      "Epoch [178/400], Loss: 0.0030\n",
      "Epoch [179/400], Loss: 0.0030\n",
      "Epoch [180/400], Loss: 0.0029\n",
      "Epoch [181/400], Loss: 0.0029\n",
      "Epoch [182/400], Loss: 0.0030\n",
      "Epoch [183/400], Loss: 0.0030\n",
      "Epoch [184/400], Loss: 0.0030\n",
      "Epoch [185/400], Loss: 0.0030\n",
      "Epoch [186/400], Loss: 0.0031\n",
      "Epoch [187/400], Loss: 0.0031\n",
      "Epoch [188/400], Loss: 0.0032\n",
      "Epoch [189/400], Loss: 0.0032\n",
      "Epoch [190/400], Loss: 0.0032\n",
      "Epoch [191/400], Loss: 0.0033\n",
      "Epoch [192/400], Loss: 0.0033\n",
      "Epoch [193/400], Loss: 0.0034\n",
      "Epoch [194/400], Loss: 0.0034\n",
      "Epoch [195/400], Loss: 0.0035\n",
      "Epoch [196/400], Loss: 0.0035\n",
      "Epoch [197/400], Loss: 0.0036\n",
      "Epoch [198/400], Loss: 0.0036\n",
      "Epoch [199/400], Loss: 0.0037\n",
      "Epoch [200/400], Loss: 0.0037\n",
      "Epoch [201/400], Loss: 0.0038\n",
      "Epoch [202/400], Loss: 0.0038\n",
      "Epoch [203/400], Loss: 0.0039\n",
      "Epoch [204/400], Loss: 0.0040\n",
      "Epoch [205/400], Loss: 0.0041\n",
      "Epoch [206/400], Loss: 0.0041\n",
      "Epoch [207/400], Loss: 0.0042\n",
      "Epoch [208/400], Loss: 0.0043\n",
      "Epoch [209/400], Loss: 0.0044\n",
      "Epoch [210/400], Loss: 0.0045\n",
      "Epoch [211/400], Loss: 0.0046\n",
      "Epoch [212/400], Loss: 0.0047\n",
      "Epoch [213/400], Loss: 0.0048\n",
      "Epoch [214/400], Loss: 0.0048\n",
      "Epoch [215/400], Loss: 0.0049\n",
      "Epoch [216/400], Loss: 0.0050\n",
      "Epoch [217/400], Loss: 0.0051\n",
      "Epoch [218/400], Loss: 0.0052\n",
      "Epoch [219/400], Loss: 0.0052\n",
      "Epoch [220/400], Loss: 0.0053\n",
      "Epoch [221/400], Loss: 0.0053\n",
      "Epoch [222/400], Loss: 0.0053\n",
      "Epoch [223/400], Loss: 0.0052\n",
      "Epoch [224/400], Loss: 0.0049\n",
      "Epoch [225/400], Loss: 0.0047\n",
      "Epoch [226/400], Loss: 0.0047\n",
      "Epoch [227/400], Loss: 0.0046\n",
      "Epoch [228/400], Loss: 0.0047\n",
      "Epoch [229/400], Loss: 0.0046\n",
      "Epoch [230/400], Loss: 0.0047\n",
      "Epoch [231/400], Loss: 0.0045\n",
      "Epoch [232/400], Loss: 0.0045\n",
      "Epoch [233/400], Loss: 0.0048\n",
      "Epoch [234/400], Loss: 0.0048\n",
      "Epoch [235/400], Loss: 0.0049\n",
      "Epoch [236/400], Loss: 0.0048\n",
      "Epoch [237/400], Loss: 0.0049\n",
      "Epoch [238/400], Loss: 0.0051\n",
      "Epoch [239/400], Loss: 0.0051\n",
      "Epoch [240/400], Loss: 0.0053\n",
      "Epoch [241/400], Loss: 0.0055\n",
      "Epoch [242/400], Loss: 0.0059\n",
      "Epoch [243/400], Loss: 0.0059\n",
      "Epoch [244/400], Loss: 0.0060\n",
      "Epoch [245/400], Loss: 0.0059\n",
      "Epoch [246/400], Loss: 0.0062\n",
      "Epoch [247/400], Loss: 0.0062\n",
      "Epoch [248/400], Loss: 0.0062\n",
      "Epoch [249/400], Loss: 0.0063\n",
      "Epoch [250/400], Loss: 0.0061\n",
      "Epoch [251/400], Loss: 0.0063\n",
      "Epoch [252/400], Loss: 0.0065\n",
      "Epoch [253/400], Loss: 0.0064\n",
      "Epoch [254/400], Loss: 0.0067\n",
      "Epoch [255/400], Loss: 0.0066\n",
      "Epoch [256/400], Loss: 0.0067\n",
      "Epoch [257/400], Loss: 0.0067\n",
      "Epoch [258/400], Loss: 0.0067\n",
      "Epoch [259/400], Loss: 0.0069\n",
      "Epoch [260/400], Loss: 0.0067\n",
      "Epoch [261/400], Loss: 0.0065\n",
      "Epoch [262/400], Loss: 0.0061\n",
      "Epoch [263/400], Loss: 0.0060\n",
      "Epoch [264/400], Loss: 0.0056\n",
      "Epoch [265/400], Loss: 0.0051\n",
      "Epoch [266/400], Loss: 0.0049\n",
      "Epoch [267/400], Loss: 0.0045\n",
      "Epoch [268/400], Loss: 0.0040\n",
      "Epoch [269/400], Loss: 0.0037\n",
      "Epoch [270/400], Loss: 0.0036\n",
      "Epoch [271/400], Loss: 0.0034\n",
      "Epoch [272/400], Loss: 0.0031\n",
      "Epoch [273/400], Loss: 0.0031\n",
      "Epoch [274/400], Loss: 0.0031\n",
      "Epoch [275/400], Loss: 0.0029\n",
      "Epoch [276/400], Loss: 0.0026\n",
      "Epoch [277/400], Loss: 0.0025\n",
      "Epoch [278/400], Loss: 0.0022\n",
      "Epoch [279/400], Loss: 0.0022\n",
      "Epoch [280/400], Loss: 0.0022\n",
      "Epoch [281/400], Loss: 0.0021\n",
      "Epoch [282/400], Loss: 0.0020\n",
      "Epoch [283/400], Loss: 0.0016\n",
      "Epoch [284/400], Loss: 0.0018\n",
      "Epoch [285/400], Loss: 0.0016\n",
      "Epoch [286/400], Loss: 0.0015\n",
      "Epoch [287/400], Loss: 0.0015\n",
      "Epoch [288/400], Loss: 0.0014\n",
      "Epoch [289/400], Loss: 0.0014\n",
      "Epoch [290/400], Loss: 0.0012\n",
      "Epoch [291/400], Loss: 0.0012\n",
      "Epoch [292/400], Loss: 0.0012\n",
      "Epoch [293/400], Loss: 0.0012\n",
      "Epoch [294/400], Loss: 0.0011\n",
      "Epoch [295/400], Loss: 0.0011\n",
      "Epoch [296/400], Loss: 0.0011\n",
      "Epoch [297/400], Loss: 0.0010\n",
      "Epoch [298/400], Loss: 0.0011\n",
      "Epoch [299/400], Loss: 0.0010\n",
      "Epoch [300/400], Loss: 0.0010\n",
      "Epoch [301/400], Loss: 0.0009\n",
      "Epoch [302/400], Loss: 0.0009\n",
      "Epoch [303/400], Loss: 0.0009\n",
      "Epoch [304/400], Loss: 0.0009\n",
      "Epoch [305/400], Loss: 0.0008\n",
      "Epoch [306/400], Loss: 0.0009\n",
      "Epoch [307/400], Loss: 0.0008\n",
      "Epoch [308/400], Loss: 0.0008\n",
      "Epoch [309/400], Loss: 0.0008\n",
      "Epoch [310/400], Loss: 0.0009\n",
      "Epoch [311/400], Loss: 0.0009\n",
      "Epoch [312/400], Loss: 0.0008\n",
      "Epoch [313/400], Loss: 0.0009\n",
      "Epoch [314/400], Loss: 0.0011\n",
      "Epoch [315/400], Loss: 0.0012\n",
      "Epoch [316/400], Loss: 0.0010\n",
      "Epoch [317/400], Loss: 0.0012\n",
      "Epoch [318/400], Loss: 0.0008\n",
      "Epoch [319/400], Loss: 0.0008\n",
      "Epoch [320/400], Loss: 0.0008\n",
      "Epoch [321/400], Loss: 0.0009\n",
      "Epoch [322/400], Loss: 0.0008\n",
      "Epoch [323/400], Loss: 0.0008\n",
      "Epoch [324/400], Loss: 0.0009\n",
      "Epoch [325/400], Loss: 0.0010\n",
      "Epoch [326/400], Loss: 0.0012\n",
      "Epoch [327/400], Loss: 0.0012\n",
      "Epoch [328/400], Loss: 0.0011\n",
      "Epoch [329/400], Loss: 0.0010\n",
      "Epoch [330/400], Loss: 0.0008\n",
      "Epoch [331/400], Loss: 0.0009\n",
      "Epoch [332/400], Loss: 0.0009\n",
      "Epoch [333/400], Loss: 0.0009\n",
      "Epoch [334/400], Loss: 0.0007\n",
      "Epoch [335/400], Loss: 0.0008\n",
      "Epoch [336/400], Loss: 0.0005\n",
      "Epoch [337/400], Loss: 0.0005\n",
      "Epoch [338/400], Loss: 0.0005\n",
      "Epoch [339/400], Loss: 0.0003\n",
      "Epoch [340/400], Loss: 0.0004\n",
      "Epoch [341/400], Loss: 0.0005\n",
      "Epoch [342/400], Loss: 0.0004\n",
      "Epoch [343/400], Loss: 0.0004\n",
      "Epoch [344/400], Loss: 0.0004\n",
      "Epoch [345/400], Loss: 0.0004\n",
      "Epoch [346/400], Loss: 0.0004\n",
      "Epoch [347/400], Loss: 0.0003\n",
      "Epoch [348/400], Loss: 0.0004\n",
      "Epoch [349/400], Loss: 0.0004\n",
      "Epoch [350/400], Loss: 0.0005\n",
      "Epoch [351/400], Loss: 0.0004\n",
      "Epoch [352/400], Loss: 0.0004\n",
      "Epoch [353/400], Loss: 0.0003\n",
      "Epoch [354/400], Loss: 0.0002\n",
      "Epoch [355/400], Loss: 0.0003\n",
      "Epoch [356/400], Loss: 0.0003\n",
      "Epoch [357/400], Loss: 0.0003\n",
      "Epoch [358/400], Loss: 0.0002\n",
      "Epoch [359/400], Loss: 0.0002\n",
      "Epoch [360/400], Loss: 0.0002\n",
      "Epoch [361/400], Loss: 0.0002\n",
      "Epoch [362/400], Loss: 0.0002\n",
      "Epoch [363/400], Loss: 0.0002\n",
      "Epoch [364/400], Loss: 0.0002\n",
      "Epoch [365/400], Loss: 0.0002\n",
      "Epoch [366/400], Loss: 0.0001\n",
      "Epoch [367/400], Loss: 0.0002\n",
      "Epoch [368/400], Loss: 0.0001\n",
      "Epoch [369/400], Loss: 0.0001\n",
      "Epoch [370/400], Loss: 0.0001\n",
      "Epoch [371/400], Loss: 0.0001\n",
      "Epoch [372/400], Loss: 0.0001\n",
      "Epoch [373/400], Loss: 0.0001\n",
      "Epoch [374/400], Loss: 0.0001\n",
      "Epoch [375/400], Loss: 0.0001\n",
      "Epoch [376/400], Loss: 0.0001\n",
      "Epoch [377/400], Loss: 0.0001\n",
      "Epoch [378/400], Loss: 0.0001\n",
      "Epoch [379/400], Loss: 0.0001\n",
      "Epoch [380/400], Loss: 0.0001\n",
      "Epoch [381/400], Loss: 0.0001\n",
      "Epoch [382/400], Loss: 0.0001\n",
      "Epoch [383/400], Loss: 0.0001\n",
      "Epoch [384/400], Loss: 0.0001\n",
      "Epoch [385/400], Loss: 0.0001\n",
      "Epoch [386/400], Loss: 0.0001\n",
      "Epoch [387/400], Loss: 0.0001\n",
      "Epoch [388/400], Loss: 0.0001\n",
      "Epoch [389/400], Loss: 0.0001\n",
      "Epoch [390/400], Loss: 0.0001\n",
      "Epoch [391/400], Loss: 0.0001\n",
      "Epoch [392/400], Loss: 0.0002\n",
      "Epoch [393/400], Loss: 0.0001\n",
      "Epoch [394/400], Loss: 0.0001\n",
      "Epoch [395/400], Loss: 0.0002\n",
      "Epoch [396/400], Loss: 0.0002\n",
      "Epoch [397/400], Loss: 0.0001\n",
      "Epoch [398/400], Loss: 0.0002\n",
      "Epoch [399/400], Loss: 0.0002\n",
      "Epoch [400/400], Loss: 0.0002\n",
      "Accuracy: 55.1587%\n",
      "Epoch [1/400], Loss: 0.0008\n",
      "Epoch [2/400], Loss: 0.0009\n",
      "Epoch [3/400], Loss: 0.0019\n",
      "Epoch [4/400], Loss: 0.0035\n",
      "Epoch [5/400], Loss: 0.0073\n",
      "Epoch [6/400], Loss: 0.0083\n",
      "Epoch [7/400], Loss: 0.0067\n",
      "Epoch [8/400], Loss: 0.0081\n",
      "Epoch [9/400], Loss: 0.0103\n",
      "Epoch [10/400], Loss: 0.0215\n",
      "Epoch [11/400], Loss: 0.0187\n",
      "Epoch [12/400], Loss: 0.0019\n",
      "Epoch [13/400], Loss: 0.0095\n",
      "Epoch [14/400], Loss: 0.0079\n",
      "Epoch [15/400], Loss: 0.0106\n",
      "Epoch [16/400], Loss: 0.0094\n",
      "Epoch [17/400], Loss: 0.0081\n",
      "Epoch [18/400], Loss: 0.0050\n",
      "Epoch [19/400], Loss: 0.0073\n",
      "Epoch [20/400], Loss: 0.0045\n",
      "Epoch [21/400], Loss: 0.0080\n",
      "Epoch [22/400], Loss: 0.0002\n",
      "Epoch [23/400], Loss: 0.0004\n",
      "Epoch [24/400], Loss: 0.0063\n",
      "Epoch [25/400], Loss: 0.0001\n",
      "Epoch [26/400], Loss: 0.0023\n",
      "Epoch [27/400], Loss: 0.0041\n",
      "Epoch [28/400], Loss: 0.0081\n",
      "Epoch [29/400], Loss: 0.0108\n",
      "Epoch [30/400], Loss: 0.0132\n",
      "Epoch [31/400], Loss: 0.0156\n",
      "Epoch [32/400], Loss: 0.0179\n",
      "Epoch [33/400], Loss: 0.0196\n",
      "Epoch [34/400], Loss: 0.0017\n",
      "Epoch [35/400], Loss: 0.0143\n",
      "Epoch [36/400], Loss: 0.0003\n",
      "Epoch [37/400], Loss: 0.0033\n",
      "Epoch [38/400], Loss: 0.0118\n",
      "Epoch [39/400], Loss: 0.0080\n",
      "Epoch [40/400], Loss: 0.0056\n",
      "Epoch [41/400], Loss: 0.0065\n",
      "Epoch [42/400], Loss: 0.0051\n",
      "Epoch [43/400], Loss: 0.0046\n",
      "Epoch [44/400], Loss: 0.0025\n",
      "Epoch [45/400], Loss: 0.0081\n",
      "Epoch [46/400], Loss: 0.0017\n",
      "Epoch [47/400], Loss: 0.0053\n",
      "Epoch [48/400], Loss: 0.0059\n",
      "Epoch [49/400], Loss: 0.0022\n",
      "Epoch [50/400], Loss: 0.0018\n",
      "Epoch [51/400], Loss: 0.0013\n",
      "Epoch [52/400], Loss: 0.0149\n",
      "Epoch [53/400], Loss: 0.0055\n",
      "Epoch [54/400], Loss: 0.0130\n",
      "Epoch [55/400], Loss: 0.0035\n",
      "Epoch [56/400], Loss: 0.0087\n",
      "Epoch [57/400], Loss: 0.0018\n",
      "Epoch [58/400], Loss: 0.0071\n",
      "Epoch [59/400], Loss: 0.0081\n",
      "Epoch [60/400], Loss: 0.0048\n",
      "Epoch [61/400], Loss: 0.0058\n",
      "Epoch [62/400], Loss: 0.0079\n",
      "Epoch [63/400], Loss: 0.0062\n",
      "Epoch [64/400], Loss: 0.0027\n",
      "Epoch [65/400], Loss: 0.0011\n",
      "Epoch [66/400], Loss: 0.0070\n",
      "Epoch [67/400], Loss: 0.0012\n",
      "Epoch [68/400], Loss: 0.0014\n",
      "Epoch [69/400], Loss: 0.0077\n",
      "Epoch [70/400], Loss: 0.0029\n",
      "Epoch [71/400], Loss: 0.0024\n",
      "Epoch [72/400], Loss: 0.0019\n",
      "Epoch [73/400], Loss: 0.0019\n",
      "Epoch [74/400], Loss: 0.0015\n",
      "Epoch [75/400], Loss: 0.0015\n",
      "Epoch [76/400], Loss: 0.0012\n",
      "Epoch [77/400], Loss: 0.0012\n",
      "Epoch [78/400], Loss: 0.0012\n",
      "Epoch [79/400], Loss: 0.0012\n",
      "Epoch [80/400], Loss: 0.0013\n",
      "Epoch [81/400], Loss: 0.0005\n",
      "Epoch [82/400], Loss: 0.0007\n",
      "Epoch [83/400], Loss: 0.0010\n",
      "Epoch [84/400], Loss: 0.0010\n",
      "Epoch [85/400], Loss: 0.0007\n",
      "Epoch [86/400], Loss: 0.0008\n",
      "Epoch [87/400], Loss: 0.0009\n",
      "Epoch [88/400], Loss: 0.0010\n",
      "Epoch [89/400], Loss: 0.0012\n",
      "Epoch [90/400], Loss: 0.0011\n",
      "Epoch [91/400], Loss: 0.0010\n",
      "Epoch [92/400], Loss: 0.0009\n",
      "Epoch [93/400], Loss: 0.0010\n",
      "Epoch [94/400], Loss: 0.0010\n",
      "Epoch [95/400], Loss: 0.0009\n",
      "Epoch [96/400], Loss: 0.0009\n",
      "Epoch [97/400], Loss: 0.0009\n",
      "Epoch [98/400], Loss: 0.0009\n",
      "Epoch [99/400], Loss: 0.0010\n",
      "Epoch [100/400], Loss: 0.0011\n",
      "Epoch [101/400], Loss: 0.0013\n",
      "Epoch [102/400], Loss: 0.0014\n",
      "Epoch [103/400], Loss: 0.0015\n",
      "Epoch [104/400], Loss: 0.0017\n",
      "Epoch [105/400], Loss: 0.0019\n",
      "Epoch [106/400], Loss: 0.0022\n",
      "Epoch [107/400], Loss: 0.0024\n",
      "Epoch [108/400], Loss: 0.0027\n",
      "Epoch [109/400], Loss: 0.0029\n",
      "Epoch [110/400], Loss: 0.0031\n",
      "Epoch [111/400], Loss: 0.0032\n",
      "Epoch [112/400], Loss: 0.0034\n",
      "Epoch [113/400], Loss: 0.0036\n",
      "Epoch [114/400], Loss: 0.0037\n",
      "Epoch [115/400], Loss: 0.0036\n",
      "Epoch [116/400], Loss: 0.0036\n",
      "Epoch [117/400], Loss: 0.0035\n",
      "Epoch [118/400], Loss: 0.0034\n",
      "Epoch [119/400], Loss: 0.0037\n",
      "Epoch [120/400], Loss: 0.0031\n",
      "Epoch [121/400], Loss: 0.0031\n",
      "Epoch [122/400], Loss: 0.0029\n",
      "Epoch [123/400], Loss: 0.0027\n",
      "Epoch [124/400], Loss: 0.0028\n",
      "Epoch [125/400], Loss: 0.0027\n",
      "Epoch [126/400], Loss: 0.0028\n",
      "Epoch [127/400], Loss: 0.0027\n",
      "Epoch [128/400], Loss: 0.0026\n",
      "Epoch [129/400], Loss: 0.0024\n",
      "Epoch [130/400], Loss: 0.0022\n",
      "Epoch [131/400], Loss: 0.0023\n",
      "Epoch [132/400], Loss: 0.0021\n",
      "Epoch [133/400], Loss: 0.0019\n",
      "Epoch [134/400], Loss: 0.0019\n",
      "Epoch [135/400], Loss: 0.0019\n",
      "Epoch [136/400], Loss: 0.0018\n",
      "Epoch [137/400], Loss: 0.0017\n",
      "Epoch [138/400], Loss: 0.0016\n",
      "Epoch [139/400], Loss: 0.0016\n",
      "Epoch [140/400], Loss: 0.0015\n",
      "Epoch [141/400], Loss: 0.0015\n",
      "Epoch [142/400], Loss: 0.0016\n",
      "Epoch [143/400], Loss: 0.0015\n",
      "Epoch [144/400], Loss: 0.0014\n",
      "Epoch [145/400], Loss: 0.0014\n",
      "Epoch [146/400], Loss: 0.0015\n",
      "Epoch [147/400], Loss: 0.0015\n",
      "Epoch [148/400], Loss: 0.0016\n",
      "Epoch [149/400], Loss: 0.0015\n",
      "Epoch [150/400], Loss: 0.0016\n",
      "Epoch [151/400], Loss: 0.0016\n",
      "Epoch [152/400], Loss: 0.0017\n",
      "Epoch [153/400], Loss: 0.0017\n",
      "Epoch [154/400], Loss: 0.0017\n",
      "Epoch [155/400], Loss: 0.0017\n",
      "Epoch [156/400], Loss: 0.0017\n",
      "Epoch [157/400], Loss: 0.0016\n",
      "Epoch [158/400], Loss: 0.0016\n",
      "Epoch [159/400], Loss: 0.0016\n",
      "Epoch [160/400], Loss: 0.0016\n",
      "Epoch [161/400], Loss: 0.0016\n",
      "Epoch [162/400], Loss: 0.0015\n",
      "Epoch [163/400], Loss: 0.0014\n",
      "Epoch [164/400], Loss: 0.0015\n",
      "Epoch [165/400], Loss: 0.0014\n",
      "Epoch [166/400], Loss: 0.0014\n",
      "Epoch [167/400], Loss: 0.0013\n",
      "Epoch [168/400], Loss: 0.0014\n",
      "Epoch [169/400], Loss: 0.0015\n",
      "Epoch [170/400], Loss: 0.0014\n",
      "Epoch [171/400], Loss: 0.0014\n",
      "Epoch [172/400], Loss: 0.0013\n",
      "Epoch [173/400], Loss: 0.0013\n",
      "Epoch [174/400], Loss: 0.0013\n",
      "Epoch [175/400], Loss: 0.0011\n",
      "Epoch [176/400], Loss: 0.0011\n",
      "Epoch [177/400], Loss: 0.0012\n",
      "Epoch [178/400], Loss: 0.0009\n",
      "Epoch [179/400], Loss: 0.0009\n",
      "Epoch [180/400], Loss: 0.0009\n",
      "Epoch [181/400], Loss: 0.0008\n",
      "Epoch [182/400], Loss: 0.0007\n",
      "Epoch [183/400], Loss: 0.0006\n",
      "Epoch [184/400], Loss: 0.0006\n",
      "Epoch [185/400], Loss: 0.0006\n",
      "Epoch [186/400], Loss: 0.0005\n",
      "Epoch [187/400], Loss: 0.0006\n",
      "Epoch [188/400], Loss: 0.0006\n",
      "Epoch [189/400], Loss: 0.0005\n",
      "Epoch [190/400], Loss: 0.0006\n",
      "Epoch [191/400], Loss: 0.0005\n",
      "Epoch [192/400], Loss: 0.0005\n",
      "Epoch [193/400], Loss: 0.0005\n",
      "Epoch [194/400], Loss: 0.0005\n",
      "Epoch [195/400], Loss: 0.0005\n",
      "Epoch [196/400], Loss: 0.0006\n",
      "Epoch [197/400], Loss: 0.0006\n",
      "Epoch [198/400], Loss: 0.0005\n",
      "Epoch [199/400], Loss: 0.0005\n",
      "Epoch [200/400], Loss: 0.0006\n",
      "Epoch [201/400], Loss: 0.0005\n",
      "Epoch [202/400], Loss: 0.0005\n",
      "Epoch [203/400], Loss: 0.0005\n",
      "Epoch [204/400], Loss: 0.0004\n",
      "Epoch [205/400], Loss: 0.0004\n",
      "Epoch [206/400], Loss: 0.0003\n",
      "Epoch [207/400], Loss: 0.0005\n",
      "Epoch [208/400], Loss: 0.0005\n",
      "Epoch [209/400], Loss: 0.0005\n",
      "Epoch [210/400], Loss: 0.0005\n",
      "Epoch [211/400], Loss: 0.0004\n",
      "Epoch [212/400], Loss: 0.0003\n",
      "Epoch [213/400], Loss: 0.0003\n",
      "Epoch [214/400], Loss: 0.0003\n",
      "Epoch [215/400], Loss: 0.0004\n",
      "Epoch [216/400], Loss: 0.0003\n",
      "Epoch [217/400], Loss: 0.0003\n",
      "Epoch [218/400], Loss: 0.0003\n",
      "Epoch [219/400], Loss: 0.0003\n",
      "Epoch [220/400], Loss: 0.0003\n",
      "Epoch [221/400], Loss: 0.0003\n",
      "Epoch [222/400], Loss: 0.0003\n",
      "Epoch [223/400], Loss: 0.0002\n",
      "Epoch [224/400], Loss: 0.0003\n",
      "Epoch [225/400], Loss: 0.0002\n",
      "Epoch [226/400], Loss: 0.0002\n",
      "Epoch [227/400], Loss: 0.0002\n",
      "Epoch [228/400], Loss: 0.0002\n",
      "Epoch [229/400], Loss: 0.0002\n",
      "Epoch [230/400], Loss: 0.0002\n",
      "Epoch [231/400], Loss: 0.0002\n",
      "Epoch [232/400], Loss: 0.0003\n",
      "Epoch [233/400], Loss: 0.0003\n",
      "Epoch [234/400], Loss: 0.0002\n",
      "Epoch [235/400], Loss: 0.0003\n",
      "Epoch [236/400], Loss: 0.0003\n",
      "Epoch [237/400], Loss: 0.0003\n",
      "Epoch [238/400], Loss: 0.0003\n",
      "Epoch [239/400], Loss: 0.0003\n",
      "Epoch [240/400], Loss: 0.0003\n",
      "Epoch [241/400], Loss: 0.0003\n",
      "Epoch [242/400], Loss: 0.0003\n",
      "Epoch [243/400], Loss: 0.0003\n",
      "Epoch [244/400], Loss: 0.0003\n",
      "Epoch [245/400], Loss: 0.0003\n",
      "Epoch [246/400], Loss: 0.0003\n",
      "Epoch [247/400], Loss: 0.0003\n",
      "Epoch [248/400], Loss: 0.0004\n",
      "Epoch [249/400], Loss: 0.0003\n",
      "Epoch [250/400], Loss: 0.0003\n",
      "Epoch [251/400], Loss: 0.0003\n",
      "Epoch [252/400], Loss: 0.0003\n",
      "Epoch [253/400], Loss: 0.0003\n",
      "Epoch [254/400], Loss: 0.0002\n",
      "Epoch [255/400], Loss: 0.0003\n",
      "Epoch [256/400], Loss: 0.0002\n",
      "Epoch [257/400], Loss: 0.0002\n",
      "Epoch [258/400], Loss: 0.0002\n",
      "Epoch [259/400], Loss: 0.0002\n",
      "Epoch [260/400], Loss: 0.0002\n",
      "Epoch [261/400], Loss: 0.0003\n",
      "Epoch [262/400], Loss: 0.0003\n",
      "Epoch [263/400], Loss: 0.0003\n",
      "Epoch [264/400], Loss: 0.0003\n",
      "Epoch [265/400], Loss: 0.0003\n",
      "Epoch [266/400], Loss: 0.0003\n",
      "Epoch [267/400], Loss: 0.0003\n",
      "Epoch [268/400], Loss: 0.0003\n",
      "Epoch [269/400], Loss: 0.0003\n",
      "Epoch [270/400], Loss: 0.0003\n",
      "Epoch [271/400], Loss: 0.0002\n",
      "Epoch [272/400], Loss: 0.0002\n",
      "Epoch [273/400], Loss: 0.0002\n",
      "Epoch [274/400], Loss: 0.0002\n",
      "Epoch [275/400], Loss: 0.0002\n",
      "Epoch [276/400], Loss: 0.0002\n",
      "Epoch [277/400], Loss: 0.0003\n",
      "Epoch [278/400], Loss: 0.0003\n",
      "Epoch [279/400], Loss: 0.0006\n",
      "Epoch [280/400], Loss: 0.0004\n",
      "Epoch [281/400], Loss: 0.0003\n",
      "Epoch [282/400], Loss: 0.0002\n",
      "Epoch [283/400], Loss: 0.0002\n",
      "Epoch [284/400], Loss: 0.0002\n",
      "Epoch [285/400], Loss: 0.0003\n",
      "Epoch [286/400], Loss: 0.0002\n",
      "Epoch [287/400], Loss: 0.0002\n",
      "Epoch [288/400], Loss: 0.0002\n",
      "Epoch [289/400], Loss: 0.0002\n",
      "Epoch [290/400], Loss: 0.0002\n",
      "Epoch [291/400], Loss: 0.0002\n",
      "Epoch [292/400], Loss: 0.0002\n",
      "Epoch [293/400], Loss: 0.0001\n",
      "Epoch [294/400], Loss: 0.0002\n",
      "Epoch [295/400], Loss: 0.0002\n",
      "Epoch [296/400], Loss: 0.0002\n",
      "Epoch [297/400], Loss: 0.0002\n",
      "Epoch [298/400], Loss: 0.0002\n",
      "Epoch [299/400], Loss: 0.0001\n",
      "Epoch [300/400], Loss: 0.0001\n",
      "Epoch [301/400], Loss: 0.0002\n",
      "Epoch [302/400], Loss: 0.0002\n",
      "Epoch [303/400], Loss: 0.0001\n",
      "Epoch [304/400], Loss: 0.0001\n",
      "Epoch [305/400], Loss: 0.0001\n",
      "Epoch [306/400], Loss: 0.0001\n",
      "Epoch [307/400], Loss: 0.0001\n",
      "Epoch [308/400], Loss: 0.0001\n",
      "Epoch [309/400], Loss: 0.0001\n",
      "Epoch [310/400], Loss: 0.0001\n",
      "Epoch [311/400], Loss: 0.0001\n",
      "Epoch [312/400], Loss: 0.0001\n",
      "Epoch [313/400], Loss: 0.0001\n",
      "Epoch [314/400], Loss: 0.0001\n",
      "Epoch [315/400], Loss: 0.0001\n",
      "Epoch [316/400], Loss: 0.0001\n",
      "Epoch [317/400], Loss: 0.0001\n",
      "Epoch [318/400], Loss: 0.0001\n",
      "Epoch [319/400], Loss: 0.0000\n",
      "Epoch [320/400], Loss: 0.0000\n",
      "Epoch [321/400], Loss: 0.0001\n",
      "Epoch [322/400], Loss: 0.0000\n",
      "Epoch [323/400], Loss: 0.0000\n",
      "Epoch [324/400], Loss: 0.0000\n",
      "Epoch [325/400], Loss: 0.0000\n",
      "Epoch [326/400], Loss: 0.0001\n",
      "Epoch [327/400], Loss: 0.0000\n",
      "Epoch [328/400], Loss: 0.0000\n",
      "Epoch [329/400], Loss: 0.0000\n",
      "Epoch [330/400], Loss: 0.0000\n",
      "Epoch [331/400], Loss: 0.0000\n",
      "Epoch [332/400], Loss: 0.0000\n",
      "Epoch [333/400], Loss: 0.0000\n",
      "Epoch [334/400], Loss: 0.0000\n",
      "Epoch [335/400], Loss: 0.0000\n",
      "Epoch [336/400], Loss: 0.0000\n",
      "Epoch [337/400], Loss: 0.0000\n",
      "Epoch [338/400], Loss: 0.0000\n",
      "Epoch [339/400], Loss: 0.0000\n",
      "Epoch [340/400], Loss: 0.0000\n",
      "Epoch [341/400], Loss: 0.0000\n",
      "Epoch [342/400], Loss: 0.0000\n",
      "Epoch [343/400], Loss: 0.0000\n",
      "Epoch [344/400], Loss: 0.0000\n",
      "Epoch [345/400], Loss: 0.0000\n",
      "Epoch [346/400], Loss: 0.0000\n",
      "Epoch [347/400], Loss: 0.0000\n",
      "Epoch [348/400], Loss: 0.0000\n",
      "Epoch [349/400], Loss: 0.0000\n",
      "Epoch [350/400], Loss: 0.0000\n",
      "Epoch [351/400], Loss: 0.0000\n",
      "Epoch [352/400], Loss: 0.0000\n",
      "Epoch [353/400], Loss: 0.0000\n",
      "Epoch [354/400], Loss: 0.0000\n",
      "Epoch [355/400], Loss: 0.0000\n",
      "Epoch [356/400], Loss: 0.0000\n",
      "Epoch [357/400], Loss: 0.0000\n",
      "Epoch [358/400], Loss: 0.0000\n",
      "Epoch [359/400], Loss: 0.0000\n",
      "Epoch [360/400], Loss: 0.0000\n",
      "Epoch [361/400], Loss: 0.0000\n",
      "Epoch [362/400], Loss: 0.0000\n",
      "Epoch [363/400], Loss: 0.0000\n",
      "Epoch [364/400], Loss: 0.0000\n",
      "Epoch [365/400], Loss: 0.0000\n",
      "Epoch [366/400], Loss: 0.0000\n",
      "Epoch [367/400], Loss: 0.0000\n",
      "Epoch [368/400], Loss: 0.0000\n",
      "Epoch [369/400], Loss: 0.0000\n",
      "Epoch [370/400], Loss: 0.0000\n",
      "Epoch [371/400], Loss: 0.0001\n",
      "Epoch [372/400], Loss: 0.0000\n",
      "Epoch [373/400], Loss: 0.0000\n",
      "Epoch [374/400], Loss: 0.0000\n",
      "Epoch [375/400], Loss: 0.0001\n",
      "Epoch [376/400], Loss: 0.0000\n",
      "Epoch [377/400], Loss: 0.0000\n",
      "Epoch [378/400], Loss: 0.0000\n",
      "Epoch [379/400], Loss: 0.0000\n",
      "Epoch [380/400], Loss: 0.0000\n",
      "Epoch [381/400], Loss: 0.0000\n",
      "Epoch [382/400], Loss: 0.0000\n",
      "Epoch [383/400], Loss: 0.0000\n",
      "Epoch [384/400], Loss: 0.0000\n",
      "Epoch [385/400], Loss: 0.0001\n",
      "Epoch [386/400], Loss: 0.0001\n",
      "Epoch [387/400], Loss: 0.0000\n",
      "Epoch [388/400], Loss: 0.0001\n",
      "Epoch [389/400], Loss: 0.0000\n",
      "Epoch [390/400], Loss: 0.0000\n",
      "Epoch [391/400], Loss: 0.0000\n",
      "Epoch [392/400], Loss: 0.0000\n",
      "Epoch [393/400], Loss: 0.0001\n",
      "Epoch [394/400], Loss: 0.0000\n",
      "Epoch [395/400], Loss: 0.0001\n",
      "Epoch [396/400], Loss: 0.0001\n",
      "Epoch [397/400], Loss: 0.0001\n",
      "Epoch [398/400], Loss: 0.0001\n",
      "Epoch [399/400], Loss: 0.0001\n",
      "Epoch [400/400], Loss: 0.0001\n",
      "Accuracy: 51.5873%\n",
      "Epoch [1/800], Loss: 0.5937\n",
      "Epoch [2/800], Loss: 0.5625\n",
      "Epoch [3/800], Loss: 0.5383\n",
      "Epoch [4/800], Loss: 0.5211\n",
      "Epoch [5/800], Loss: 0.5088\n",
      "Epoch [6/800], Loss: 0.4998\n",
      "Epoch [7/800], Loss: 0.4928\n",
      "Epoch [8/800], Loss: 0.4871\n",
      "Epoch [9/800], Loss: 0.4818\n",
      "Epoch [10/800], Loss: 0.4767\n",
      "Epoch [11/800], Loss: 0.4714\n",
      "Epoch [12/800], Loss: 0.4660\n",
      "Epoch [13/800], Loss: 0.4601\n",
      "Epoch [14/800], Loss: 0.4537\n",
      "Epoch [15/800], Loss: 0.4467\n",
      "Epoch [16/800], Loss: 0.4392\n",
      "Epoch [17/800], Loss: 0.4312\n",
      "Epoch [18/800], Loss: 0.4226\n",
      "Epoch [19/800], Loss: 0.4138\n",
      "Epoch [20/800], Loss: 0.4046\n",
      "Epoch [21/800], Loss: 0.3953\n",
      "Epoch [22/800], Loss: 0.3858\n",
      "Epoch [23/800], Loss: 0.3763\n",
      "Epoch [24/800], Loss: 0.3668\n",
      "Epoch [25/800], Loss: 0.3573\n",
      "Epoch [26/800], Loss: 0.3479\n",
      "Epoch [27/800], Loss: 0.3385\n",
      "Epoch [28/800], Loss: 0.3294\n",
      "Epoch [29/800], Loss: 0.3204\n",
      "Epoch [30/800], Loss: 0.3116\n",
      "Epoch [31/800], Loss: 0.3030\n",
      "Epoch [32/800], Loss: 0.2947\n",
      "Epoch [33/800], Loss: 0.2865\n",
      "Epoch [34/800], Loss: 0.2786\n",
      "Epoch [35/800], Loss: 0.2710\n",
      "Epoch [36/800], Loss: 0.2636\n",
      "Epoch [37/800], Loss: 0.2565\n",
      "Epoch [38/800], Loss: 0.2495\n",
      "Epoch [39/800], Loss: 0.2428\n",
      "Epoch [40/800], Loss: 0.2363\n",
      "Epoch [41/800], Loss: 0.2299\n",
      "Epoch [42/800], Loss: 0.2237\n",
      "Epoch [43/800], Loss: 0.2176\n",
      "Epoch [44/800], Loss: 0.2117\n",
      "Epoch [45/800], Loss: 0.2060\n",
      "Epoch [46/800], Loss: 0.2005\n",
      "Epoch [47/800], Loss: 0.1951\n",
      "Epoch [48/800], Loss: 0.1898\n",
      "Epoch [49/800], Loss: 0.1847\n",
      "Epoch [50/800], Loss: 0.1797\n",
      "Epoch [51/800], Loss: 0.1749\n",
      "Epoch [52/800], Loss: 0.1703\n",
      "Epoch [53/800], Loss: 0.1658\n",
      "Epoch [54/800], Loss: 0.1615\n",
      "Epoch [55/800], Loss: 0.1573\n",
      "Epoch [56/800], Loss: 0.1533\n",
      "Epoch [57/800], Loss: 0.1495\n",
      "Epoch [58/800], Loss: 0.1458\n",
      "Epoch [59/800], Loss: 0.1422\n",
      "Epoch [60/800], Loss: 0.1387\n",
      "Epoch [61/800], Loss: 0.1354\n",
      "Epoch [62/800], Loss: 0.1322\n",
      "Epoch [63/800], Loss: 0.1291\n",
      "Epoch [64/800], Loss: 0.1261\n",
      "Epoch [65/800], Loss: 0.1233\n",
      "Epoch [66/800], Loss: 0.1205\n",
      "Epoch [67/800], Loss: 0.1179\n",
      "Epoch [68/800], Loss: 0.1154\n",
      "Epoch [69/800], Loss: 0.1131\n",
      "Epoch [70/800], Loss: 0.1108\n",
      "Epoch [71/800], Loss: 0.1085\n",
      "Epoch [72/800], Loss: 0.1064\n",
      "Epoch [73/800], Loss: 0.1042\n",
      "Epoch [74/800], Loss: 0.1021\n",
      "Epoch [75/800], Loss: 0.1001\n",
      "Epoch [76/800], Loss: 0.0981\n",
      "Epoch [77/800], Loss: 0.0962\n",
      "Epoch [78/800], Loss: 0.0943\n",
      "Epoch [79/800], Loss: 0.0925\n",
      "Epoch [80/800], Loss: 0.0908\n",
      "Epoch [81/800], Loss: 0.0891\n",
      "Epoch [82/800], Loss: 0.0875\n",
      "Epoch [83/800], Loss: 0.0859\n",
      "Epoch [84/800], Loss: 0.0844\n",
      "Epoch [85/800], Loss: 0.0829\n",
      "Epoch [86/800], Loss: 0.0815\n",
      "Epoch [87/800], Loss: 0.0802\n",
      "Epoch [88/800], Loss: 0.0789\n",
      "Epoch [89/800], Loss: 0.0776\n",
      "Epoch [90/800], Loss: 0.0764\n",
      "Epoch [91/800], Loss: 0.0752\n",
      "Epoch [92/800], Loss: 0.0741\n",
      "Epoch [93/800], Loss: 0.0730\n",
      "Epoch [94/800], Loss: 0.0719\n",
      "Epoch [95/800], Loss: 0.0709\n",
      "Epoch [96/800], Loss: 0.0698\n",
      "Epoch [97/800], Loss: 0.0689\n",
      "Epoch [98/800], Loss: 0.0679\n",
      "Epoch [99/800], Loss: 0.0670\n",
      "Epoch [100/800], Loss: 0.0662\n",
      "Epoch [101/800], Loss: 0.0653\n",
      "Epoch [102/800], Loss: 0.0645\n",
      "Epoch [103/800], Loss: 0.0637\n",
      "Epoch [104/800], Loss: 0.0630\n",
      "Epoch [105/800], Loss: 0.0622\n",
      "Epoch [106/800], Loss: 0.0616\n",
      "Epoch [107/800], Loss: 0.0609\n",
      "Epoch [108/800], Loss: 0.0603\n",
      "Epoch [109/800], Loss: 0.0596\n",
      "Epoch [110/800], Loss: 0.0590\n",
      "Epoch [111/800], Loss: 0.0584\n",
      "Epoch [112/800], Loss: 0.0578\n",
      "Epoch [113/800], Loss: 0.0572\n",
      "Epoch [114/800], Loss: 0.0566\n",
      "Epoch [115/800], Loss: 0.0560\n",
      "Epoch [116/800], Loss: 0.0554\n",
      "Epoch [117/800], Loss: 0.0548\n",
      "Epoch [118/800], Loss: 0.0542\n",
      "Epoch [119/800], Loss: 0.0537\n",
      "Epoch [120/800], Loss: 0.0531\n",
      "Epoch [121/800], Loss: 0.0525\n",
      "Epoch [122/800], Loss: 0.0520\n",
      "Epoch [123/800], Loss: 0.0515\n",
      "Epoch [124/800], Loss: 0.0509\n",
      "Epoch [125/800], Loss: 0.0504\n",
      "Epoch [126/800], Loss: 0.0500\n",
      "Epoch [127/800], Loss: 0.0495\n",
      "Epoch [128/800], Loss: 0.0490\n",
      "Epoch [129/800], Loss: 0.0486\n",
      "Epoch [130/800], Loss: 0.0482\n",
      "Epoch [131/800], Loss: 0.0478\n",
      "Epoch [132/800], Loss: 0.0474\n",
      "Epoch [133/800], Loss: 0.0470\n",
      "Epoch [134/800], Loss: 0.0466\n",
      "Epoch [135/800], Loss: 0.0462\n",
      "Epoch [136/800], Loss: 0.0459\n",
      "Epoch [137/800], Loss: 0.0455\n",
      "Epoch [138/800], Loss: 0.0451\n",
      "Epoch [139/800], Loss: 0.0448\n",
      "Epoch [140/800], Loss: 0.0444\n",
      "Epoch [141/800], Loss: 0.0440\n",
      "Epoch [142/800], Loss: 0.0437\n",
      "Epoch [143/800], Loss: 0.0433\n",
      "Epoch [144/800], Loss: 0.0430\n",
      "Epoch [145/800], Loss: 0.0427\n",
      "Epoch [146/800], Loss: 0.0424\n",
      "Epoch [147/800], Loss: 0.0421\n",
      "Epoch [148/800], Loss: 0.0418\n",
      "Epoch [149/800], Loss: 0.0415\n",
      "Epoch [150/800], Loss: 0.0411\n",
      "Epoch [151/800], Loss: 0.0408\n",
      "Epoch [152/800], Loss: 0.0405\n",
      "Epoch [153/800], Loss: 0.0402\n",
      "Epoch [154/800], Loss: 0.0399\n",
      "Epoch [155/800], Loss: 0.0396\n",
      "Epoch [156/800], Loss: 0.0392\n",
      "Epoch [157/800], Loss: 0.0389\n",
      "Epoch [158/800], Loss: 0.0386\n",
      "Epoch [159/800], Loss: 0.0383\n",
      "Epoch [160/800], Loss: 0.0380\n",
      "Epoch [161/800], Loss: 0.0377\n",
      "Epoch [162/800], Loss: 0.0374\n",
      "Epoch [163/800], Loss: 0.0370\n",
      "Epoch [164/800], Loss: 0.0366\n",
      "Epoch [165/800], Loss: 0.0362\n",
      "Epoch [166/800], Loss: 0.0357\n",
      "Epoch [167/800], Loss: 0.0352\n",
      "Epoch [168/800], Loss: 0.0346\n",
      "Epoch [169/800], Loss: 0.0341\n",
      "Epoch [170/800], Loss: 0.0335\n",
      "Epoch [171/800], Loss: 0.0330\n",
      "Epoch [172/800], Loss: 0.0326\n",
      "Epoch [173/800], Loss: 0.0322\n",
      "Epoch [174/800], Loss: 0.0318\n",
      "Epoch [175/800], Loss: 0.0314\n",
      "Epoch [176/800], Loss: 0.0310\n",
      "Epoch [177/800], Loss: 0.0307\n",
      "Epoch [178/800], Loss: 0.0304\n",
      "Epoch [179/800], Loss: 0.0301\n",
      "Epoch [180/800], Loss: 0.0298\n",
      "Epoch [181/800], Loss: 0.0296\n",
      "Epoch [182/800], Loss: 0.0293\n",
      "Epoch [183/800], Loss: 0.0291\n",
      "Epoch [184/800], Loss: 0.0290\n",
      "Epoch [185/800], Loss: 0.0288\n",
      "Epoch [186/800], Loss: 0.0286\n",
      "Epoch [187/800], Loss: 0.0285\n",
      "Epoch [188/800], Loss: 0.0284\n",
      "Epoch [189/800], Loss: 0.0284\n",
      "Epoch [190/800], Loss: 0.0284\n",
      "Epoch [191/800], Loss: 0.0284\n",
      "Epoch [192/800], Loss: 0.0283\n",
      "Epoch [193/800], Loss: 0.0282\n",
      "Epoch [194/800], Loss: 0.0282\n",
      "Epoch [195/800], Loss: 0.0282\n",
      "Epoch [196/800], Loss: 0.0282\n",
      "Epoch [197/800], Loss: 0.0282\n",
      "Epoch [198/800], Loss: 0.0282\n",
      "Epoch [199/800], Loss: 0.0282\n",
      "Epoch [200/800], Loss: 0.0281\n",
      "Epoch [201/800], Loss: 0.0280\n",
      "Epoch [202/800], Loss: 0.0279\n",
      "Epoch [203/800], Loss: 0.0278\n",
      "Epoch [204/800], Loss: 0.0278\n",
      "Epoch [205/800], Loss: 0.0277\n",
      "Epoch [206/800], Loss: 0.0277\n",
      "Epoch [207/800], Loss: 0.0276\n",
      "Epoch [208/800], Loss: 0.0276\n",
      "Epoch [209/800], Loss: 0.0276\n",
      "Epoch [210/800], Loss: 0.0276\n",
      "Epoch [211/800], Loss: 0.0275\n",
      "Epoch [212/800], Loss: 0.0275\n",
      "Epoch [213/800], Loss: 0.0273\n",
      "Epoch [214/800], Loss: 0.0273\n",
      "Epoch [215/800], Loss: 0.0272\n",
      "Epoch [216/800], Loss: 0.0272\n",
      "Epoch [217/800], Loss: 0.0270\n",
      "Epoch [218/800], Loss: 0.0269\n",
      "Epoch [219/800], Loss: 0.0268\n",
      "Epoch [220/800], Loss: 0.0267\n",
      "Epoch [221/800], Loss: 0.0267\n",
      "Epoch [222/800], Loss: 0.0266\n",
      "Epoch [223/800], Loss: 0.0266\n",
      "Epoch [224/800], Loss: 0.0266\n",
      "Epoch [225/800], Loss: 0.0266\n",
      "Epoch [226/800], Loss: 0.0266\n",
      "Epoch [227/800], Loss: 0.0266\n",
      "Epoch [228/800], Loss: 0.0266\n",
      "Epoch [229/800], Loss: 0.0265\n",
      "Epoch [230/800], Loss: 0.0266\n",
      "Epoch [231/800], Loss: 0.0265\n",
      "Epoch [232/800], Loss: 0.0263\n",
      "Epoch [233/800], Loss: 0.0263\n",
      "Epoch [234/800], Loss: 0.0262\n",
      "Epoch [235/800], Loss: 0.0261\n",
      "Epoch [236/800], Loss: 0.0262\n",
      "Epoch [237/800], Loss: 0.0262\n",
      "Epoch [238/800], Loss: 0.0262\n",
      "Epoch [239/800], Loss: 0.0262\n",
      "Epoch [240/800], Loss: 0.0260\n",
      "Epoch [241/800], Loss: 0.0261\n",
      "Epoch [242/800], Loss: 0.0262\n",
      "Epoch [243/800], Loss: 0.0261\n",
      "Epoch [244/800], Loss: 0.0261\n",
      "Epoch [245/800], Loss: 0.0259\n",
      "Epoch [246/800], Loss: 0.0259\n",
      "Epoch [247/800], Loss: 0.0259\n",
      "Epoch [248/800], Loss: 0.0258\n",
      "Epoch [249/800], Loss: 0.0258\n",
      "Epoch [250/800], Loss: 0.0258\n",
      "Epoch [251/800], Loss: 0.0259\n",
      "Epoch [252/800], Loss: 0.0258\n",
      "Epoch [253/800], Loss: 0.0256\n",
      "Epoch [254/800], Loss: 0.0254\n",
      "Epoch [255/800], Loss: 0.0254\n",
      "Epoch [256/800], Loss: 0.0255\n",
      "Epoch [257/800], Loss: 0.0253\n",
      "Epoch [258/800], Loss: 0.0255\n",
      "Epoch [259/800], Loss: 0.0253\n",
      "Epoch [260/800], Loss: 0.0250\n",
      "Epoch [261/800], Loss: 0.0250\n",
      "Epoch [262/800], Loss: 0.0249\n",
      "Epoch [263/800], Loss: 0.0247\n",
      "Epoch [264/800], Loss: 0.0248\n",
      "Epoch [265/800], Loss: 0.0249\n",
      "Epoch [266/800], Loss: 0.0248\n",
      "Epoch [267/800], Loss: 0.0248\n",
      "Epoch [268/800], Loss: 0.0247\n",
      "Epoch [269/800], Loss: 0.0247\n",
      "Epoch [270/800], Loss: 0.0247\n",
      "Epoch [271/800], Loss: 0.0247\n",
      "Epoch [272/800], Loss: 0.0245\n",
      "Epoch [273/800], Loss: 0.0244\n",
      "Epoch [274/800], Loss: 0.0243\n",
      "Epoch [275/800], Loss: 0.0244\n",
      "Epoch [276/800], Loss: 0.0244\n",
      "Epoch [277/800], Loss: 0.0245\n",
      "Epoch [278/800], Loss: 0.0245\n",
      "Epoch [279/800], Loss: 0.0245\n",
      "Epoch [280/800], Loss: 0.0241\n",
      "Epoch [281/800], Loss: 0.0241\n",
      "Epoch [282/800], Loss: 0.0241\n",
      "Epoch [283/800], Loss: 0.0241\n",
      "Epoch [284/800], Loss: 0.0239\n",
      "Epoch [285/800], Loss: 0.0240\n",
      "Epoch [286/800], Loss: 0.0239\n",
      "Epoch [287/800], Loss: 0.0240\n",
      "Epoch [288/800], Loss: 0.0240\n",
      "Epoch [289/800], Loss: 0.0239\n",
      "Epoch [290/800], Loss: 0.0237\n",
      "Epoch [291/800], Loss: 0.0235\n",
      "Epoch [292/800], Loss: 0.0234\n",
      "Epoch [293/800], Loss: 0.0235\n",
      "Epoch [294/800], Loss: 0.0236\n",
      "Epoch [295/800], Loss: 0.0236\n",
      "Epoch [296/800], Loss: 0.0233\n",
      "Epoch [297/800], Loss: 0.0233\n",
      "Epoch [298/800], Loss: 0.0233\n",
      "Epoch [299/800], Loss: 0.0232\n",
      "Epoch [300/800], Loss: 0.0230\n",
      "Epoch [301/800], Loss: 0.0228\n",
      "Epoch [302/800], Loss: 0.0227\n",
      "Epoch [303/800], Loss: 0.0226\n",
      "Epoch [304/800], Loss: 0.0227\n",
      "Epoch [305/800], Loss: 0.0225\n",
      "Epoch [306/800], Loss: 0.0225\n",
      "Epoch [307/800], Loss: 0.0224\n",
      "Epoch [308/800], Loss: 0.0221\n",
      "Epoch [309/800], Loss: 0.0222\n",
      "Epoch [310/800], Loss: 0.0222\n",
      "Epoch [311/800], Loss: 0.0221\n",
      "Epoch [312/800], Loss: 0.0221\n",
      "Epoch [313/800], Loss: 0.0221\n",
      "Epoch [314/800], Loss: 0.0218\n",
      "Epoch [315/800], Loss: 0.0217\n",
      "Epoch [316/800], Loss: 0.0217\n",
      "Epoch [317/800], Loss: 0.0217\n",
      "Epoch [318/800], Loss: 0.0216\n",
      "Epoch [319/800], Loss: 0.0215\n",
      "Epoch [320/800], Loss: 0.0213\n",
      "Epoch [321/800], Loss: 0.0212\n",
      "Epoch [322/800], Loss: 0.0211\n",
      "Epoch [323/800], Loss: 0.0211\n",
      "Epoch [324/800], Loss: 0.0209\n",
      "Epoch [325/800], Loss: 0.0206\n",
      "Epoch [326/800], Loss: 0.0205\n",
      "Epoch [327/800], Loss: 0.0204\n",
      "Epoch [328/800], Loss: 0.0204\n",
      "Epoch [329/800], Loss: 0.0203\n",
      "Epoch [330/800], Loss: 0.0204\n",
      "Epoch [331/800], Loss: 0.0204\n",
      "Epoch [332/800], Loss: 0.0203\n",
      "Epoch [333/800], Loss: 0.0202\n",
      "Epoch [334/800], Loss: 0.0202\n",
      "Epoch [335/800], Loss: 0.0201\n",
      "Epoch [336/800], Loss: 0.0201\n",
      "Epoch [337/800], Loss: 0.0200\n",
      "Epoch [338/800], Loss: 0.0200\n",
      "Epoch [339/800], Loss: 0.0199\n",
      "Epoch [340/800], Loss: 0.0197\n",
      "Epoch [341/800], Loss: 0.0197\n",
      "Epoch [342/800], Loss: 0.0196\n",
      "Epoch [343/800], Loss: 0.0195\n",
      "Epoch [344/800], Loss: 0.0194\n",
      "Epoch [345/800], Loss: 0.0194\n",
      "Epoch [346/800], Loss: 0.0194\n",
      "Epoch [347/800], Loss: 0.0193\n",
      "Epoch [348/800], Loss: 0.0194\n",
      "Epoch [349/800], Loss: 0.0193\n",
      "Epoch [350/800], Loss: 0.0190\n",
      "Epoch [351/800], Loss: 0.0190\n",
      "Epoch [352/800], Loss: 0.0189\n",
      "Epoch [353/800], Loss: 0.0189\n",
      "Epoch [354/800], Loss: 0.0190\n",
      "Epoch [355/800], Loss: 0.0191\n",
      "Epoch [356/800], Loss: 0.0190\n",
      "Epoch [357/800], Loss: 0.0189\n",
      "Epoch [358/800], Loss: 0.0189\n",
      "Epoch [359/800], Loss: 0.0187\n",
      "Epoch [360/800], Loss: 0.0187\n",
      "Epoch [361/800], Loss: 0.0186\n",
      "Epoch [362/800], Loss: 0.0187\n",
      "Epoch [363/800], Loss: 0.0187\n",
      "Epoch [364/800], Loss: 0.0186\n",
      "Epoch [365/800], Loss: 0.0185\n",
      "Epoch [366/800], Loss: 0.0185\n",
      "Epoch [367/800], Loss: 0.0185\n",
      "Epoch [368/800], Loss: 0.0186\n",
      "Epoch [369/800], Loss: 0.0186\n",
      "Epoch [370/800], Loss: 0.0185\n",
      "Epoch [371/800], Loss: 0.0185\n",
      "Epoch [372/800], Loss: 0.0185\n",
      "Epoch [373/800], Loss: 0.0186\n",
      "Epoch [374/800], Loss: 0.0184\n",
      "Epoch [375/800], Loss: 0.0184\n",
      "Epoch [376/800], Loss: 0.0183\n",
      "Epoch [377/800], Loss: 0.0182\n",
      "Epoch [378/800], Loss: 0.0181\n",
      "Epoch [379/800], Loss: 0.0182\n",
      "Epoch [380/800], Loss: 0.0182\n",
      "Epoch [381/800], Loss: 0.0181\n",
      "Epoch [382/800], Loss: 0.0181\n",
      "Epoch [383/800], Loss: 0.0181\n",
      "Epoch [384/800], Loss: 0.0181\n",
      "Epoch [385/800], Loss: 0.0180\n",
      "Epoch [386/800], Loss: 0.0179\n",
      "Epoch [387/800], Loss: 0.0178\n",
      "Epoch [388/800], Loss: 0.0178\n",
      "Epoch [389/800], Loss: 0.0178\n",
      "Epoch [390/800], Loss: 0.0177\n",
      "Epoch [391/800], Loss: 0.0177\n",
      "Epoch [392/800], Loss: 0.0175\n",
      "Epoch [393/800], Loss: 0.0175\n",
      "Epoch [394/800], Loss: 0.0173\n",
      "Epoch [395/800], Loss: 0.0173\n",
      "Epoch [396/800], Loss: 0.0170\n",
      "Epoch [397/800], Loss: 0.0170\n",
      "Epoch [398/800], Loss: 0.0170\n",
      "Epoch [399/800], Loss: 0.0167\n",
      "Epoch [400/800], Loss: 0.0163\n",
      "Epoch [401/800], Loss: 0.0160\n",
      "Epoch [402/800], Loss: 0.0158\n",
      "Epoch [403/800], Loss: 0.0156\n",
      "Epoch [404/800], Loss: 0.0155\n",
      "Epoch [405/800], Loss: 0.0154\n",
      "Epoch [406/800], Loss: 0.0153\n",
      "Epoch [407/800], Loss: 0.0153\n",
      "Epoch [408/800], Loss: 0.0152\n",
      "Epoch [409/800], Loss: 0.0151\n",
      "Epoch [410/800], Loss: 0.0150\n",
      "Epoch [411/800], Loss: 0.0147\n",
      "Epoch [412/800], Loss: 0.0147\n",
      "Epoch [413/800], Loss: 0.0145\n",
      "Epoch [414/800], Loss: 0.0144\n",
      "Epoch [415/800], Loss: 0.0144\n",
      "Epoch [416/800], Loss: 0.0144\n",
      "Epoch [417/800], Loss: 0.0144\n",
      "Epoch [418/800], Loss: 0.0145\n",
      "Epoch [419/800], Loss: 0.0145\n",
      "Epoch [420/800], Loss: 0.0144\n",
      "Epoch [421/800], Loss: 0.0143\n",
      "Epoch [422/800], Loss: 0.0141\n",
      "Epoch [423/800], Loss: 0.0142\n",
      "Epoch [424/800], Loss: 0.0142\n",
      "Epoch [425/800], Loss: 0.0143\n",
      "Epoch [426/800], Loss: 0.0142\n",
      "Epoch [427/800], Loss: 0.0142\n",
      "Epoch [428/800], Loss: 0.0142\n",
      "Epoch [429/800], Loss: 0.0142\n",
      "Epoch [430/800], Loss: 0.0141\n",
      "Epoch [431/800], Loss: 0.0140\n",
      "Epoch [432/800], Loss: 0.0139\n",
      "Epoch [433/800], Loss: 0.0139\n",
      "Epoch [434/800], Loss: 0.0140\n",
      "Epoch [435/800], Loss: 0.0139\n",
      "Epoch [436/800], Loss: 0.0137\n",
      "Epoch [437/800], Loss: 0.0137\n",
      "Epoch [438/800], Loss: 0.0136\n",
      "Epoch [439/800], Loss: 0.0136\n",
      "Epoch [440/800], Loss: 0.0135\n",
      "Epoch [441/800], Loss: 0.0134\n",
      "Epoch [442/800], Loss: 0.0134\n",
      "Epoch [443/800], Loss: 0.0133\n",
      "Epoch [444/800], Loss: 0.0133\n",
      "Epoch [445/800], Loss: 0.0132\n",
      "Epoch [446/800], Loss: 0.0131\n",
      "Epoch [447/800], Loss: 0.0130\n",
      "Epoch [448/800], Loss: 0.0131\n",
      "Epoch [449/800], Loss: 0.0130\n",
      "Epoch [450/800], Loss: 0.0131\n",
      "Epoch [451/800], Loss: 0.0132\n",
      "Epoch [452/800], Loss: 0.0130\n",
      "Epoch [453/800], Loss: 0.0129\n",
      "Epoch [454/800], Loss: 0.0131\n",
      "Epoch [455/800], Loss: 0.0130\n",
      "Epoch [456/800], Loss: 0.0130\n",
      "Epoch [457/800], Loss: 0.0131\n",
      "Epoch [458/800], Loss: 0.0130\n",
      "Epoch [459/800], Loss: 0.0129\n",
      "Epoch [460/800], Loss: 0.0129\n",
      "Epoch [461/800], Loss: 0.0129\n",
      "Epoch [462/800], Loss: 0.0130\n",
      "Epoch [463/800], Loss: 0.0131\n",
      "Epoch [464/800], Loss: 0.0132\n",
      "Epoch [465/800], Loss: 0.0132\n",
      "Epoch [466/800], Loss: 0.0132\n",
      "Epoch [467/800], Loss: 0.0132\n",
      "Epoch [468/800], Loss: 0.0132\n",
      "Epoch [469/800], Loss: 0.0131\n",
      "Epoch [470/800], Loss: 0.0131\n",
      "Epoch [471/800], Loss: 0.0132\n",
      "Epoch [472/800], Loss: 0.0130\n",
      "Epoch [473/800], Loss: 0.0131\n",
      "Epoch [474/800], Loss: 0.0129\n",
      "Epoch [475/800], Loss: 0.0128\n",
      "Epoch [476/800], Loss: 0.0129\n",
      "Epoch [477/800], Loss: 0.0128\n",
      "Epoch [478/800], Loss: 0.0127\n",
      "Epoch [479/800], Loss: 0.0126\n",
      "Epoch [480/800], Loss: 0.0126\n",
      "Epoch [481/800], Loss: 0.0126\n",
      "Epoch [482/800], Loss: 0.0126\n",
      "Epoch [483/800], Loss: 0.0126\n",
      "Epoch [484/800], Loss: 0.0127\n",
      "Epoch [485/800], Loss: 0.0126\n",
      "Epoch [486/800], Loss: 0.0131\n",
      "Epoch [487/800], Loss: 0.0138\n",
      "Epoch [488/800], Loss: 0.0141\n",
      "Epoch [489/800], Loss: 0.0144\n",
      "Epoch [490/800], Loss: 0.0149\n",
      "Epoch [491/800], Loss: 0.0160\n",
      "Epoch [492/800], Loss: 0.0167\n",
      "Epoch [493/800], Loss: 0.0177\n",
      "Epoch [494/800], Loss: 0.0185\n",
      "Epoch [495/800], Loss: 0.0193\n",
      "Epoch [496/800], Loss: 0.0196\n",
      "Epoch [497/800], Loss: 0.0198\n",
      "Epoch [498/800], Loss: 0.0200\n",
      "Epoch [499/800], Loss: 0.0203\n",
      "Epoch [500/800], Loss: 0.0206\n",
      "Epoch [501/800], Loss: 0.0202\n",
      "Epoch [502/800], Loss: 0.0197\n",
      "Epoch [503/800], Loss: 0.0181\n",
      "Epoch [504/800], Loss: 0.0166\n",
      "Epoch [505/800], Loss: 0.0156\n",
      "Epoch [506/800], Loss: 0.0145\n",
      "Epoch [507/800], Loss: 0.0137\n",
      "Epoch [508/800], Loss: 0.0129\n",
      "Epoch [509/800], Loss: 0.0124\n",
      "Epoch [510/800], Loss: 0.0117\n",
      "Epoch [511/800], Loss: 0.0112\n",
      "Epoch [512/800], Loss: 0.0107\n",
      "Epoch [513/800], Loss: 0.0103\n",
      "Epoch [514/800], Loss: 0.0099\n",
      "Epoch [515/800], Loss: 0.0095\n",
      "Epoch [516/800], Loss: 0.0094\n",
      "Epoch [517/800], Loss: 0.0093\n",
      "Epoch [518/800], Loss: 0.0092\n",
      "Epoch [519/800], Loss: 0.0090\n",
      "Epoch [520/800], Loss: 0.0089\n",
      "Epoch [521/800], Loss: 0.0088\n",
      "Epoch [522/800], Loss: 0.0085\n",
      "Epoch [523/800], Loss: 0.0082\n",
      "Epoch [524/800], Loss: 0.0078\n",
      "Epoch [525/800], Loss: 0.0076\n",
      "Epoch [526/800], Loss: 0.0074\n",
      "Epoch [527/800], Loss: 0.0072\n",
      "Epoch [528/800], Loss: 0.0072\n",
      "Epoch [529/800], Loss: 0.0073\n",
      "Epoch [530/800], Loss: 0.0072\n",
      "Epoch [531/800], Loss: 0.0071\n",
      "Epoch [532/800], Loss: 0.0072\n",
      "Epoch [533/800], Loss: 0.0072\n",
      "Epoch [534/800], Loss: 0.0072\n",
      "Epoch [535/800], Loss: 0.0072\n",
      "Epoch [536/800], Loss: 0.0071\n",
      "Epoch [537/800], Loss: 0.0072\n",
      "Epoch [538/800], Loss: 0.0074\n",
      "Epoch [539/800], Loss: 0.0075\n",
      "Epoch [540/800], Loss: 0.0076\n",
      "Epoch [541/800], Loss: 0.0077\n",
      "Epoch [542/800], Loss: 0.0079\n",
      "Epoch [543/800], Loss: 0.0080\n",
      "Epoch [544/800], Loss: 0.0081\n",
      "Epoch [545/800], Loss: 0.0082\n",
      "Epoch [546/800], Loss: 0.0083\n",
      "Epoch [547/800], Loss: 0.0085\n",
      "Epoch [548/800], Loss: 0.0086\n",
      "Epoch [549/800], Loss: 0.0088\n",
      "Epoch [550/800], Loss: 0.0088\n",
      "Epoch [551/800], Loss: 0.0088\n",
      "Epoch [552/800], Loss: 0.0088\n",
      "Epoch [553/800], Loss: 0.0088\n",
      "Epoch [554/800], Loss: 0.0089\n",
      "Epoch [555/800], Loss: 0.0090\n",
      "Epoch [556/800], Loss: 0.0091\n",
      "Epoch [557/800], Loss: 0.0091\n",
      "Epoch [558/800], Loss: 0.0092\n",
      "Epoch [559/800], Loss: 0.0092\n",
      "Epoch [560/800], Loss: 0.0093\n",
      "Epoch [561/800], Loss: 0.0093\n",
      "Epoch [562/800], Loss: 0.0093\n",
      "Epoch [563/800], Loss: 0.0093\n",
      "Epoch [564/800], Loss: 0.0092\n",
      "Epoch [565/800], Loss: 0.0092\n",
      "Epoch [566/800], Loss: 0.0092\n",
      "Epoch [567/800], Loss: 0.0091\n",
      "Epoch [568/800], Loss: 0.0091\n",
      "Epoch [569/800], Loss: 0.0089\n",
      "Epoch [570/800], Loss: 0.0088\n",
      "Epoch [571/800], Loss: 0.0087\n",
      "Epoch [572/800], Loss: 0.0086\n",
      "Epoch [573/800], Loss: 0.0084\n",
      "Epoch [574/800], Loss: 0.0083\n",
      "Epoch [575/800], Loss: 0.0082\n",
      "Epoch [576/800], Loss: 0.0081\n",
      "Epoch [577/800], Loss: 0.0080\n",
      "Epoch [578/800], Loss: 0.0080\n",
      "Epoch [579/800], Loss: 0.0079\n",
      "Epoch [580/800], Loss: 0.0079\n",
      "Epoch [581/800], Loss: 0.0078\n",
      "Epoch [582/800], Loss: 0.0077\n",
      "Epoch [583/800], Loss: 0.0078\n",
      "Epoch [584/800], Loss: 0.0078\n",
      "Epoch [585/800], Loss: 0.0078\n",
      "Epoch [586/800], Loss: 0.0077\n",
      "Epoch [587/800], Loss: 0.0077\n",
      "Epoch [588/800], Loss: 0.0076\n",
      "Epoch [589/800], Loss: 0.0075\n",
      "Epoch [590/800], Loss: 0.0075\n",
      "Epoch [591/800], Loss: 0.0074\n",
      "Epoch [592/800], Loss: 0.0074\n",
      "Epoch [593/800], Loss: 0.0073\n",
      "Epoch [594/800], Loss: 0.0073\n",
      "Epoch [595/800], Loss: 0.0073\n",
      "Epoch [596/800], Loss: 0.0073\n",
      "Epoch [597/800], Loss: 0.0073\n",
      "Epoch [598/800], Loss: 0.0073\n",
      "Epoch [599/800], Loss: 0.0073\n",
      "Epoch [600/800], Loss: 0.0073\n",
      "Epoch [601/800], Loss: 0.0073\n",
      "Epoch [602/800], Loss: 0.0073\n",
      "Epoch [603/800], Loss: 0.0073\n",
      "Epoch [604/800], Loss: 0.0073\n",
      "Epoch [605/800], Loss: 0.0073\n",
      "Epoch [606/800], Loss: 0.0073\n",
      "Epoch [607/800], Loss: 0.0073\n",
      "Epoch [608/800], Loss: 0.0073\n",
      "Epoch [609/800], Loss: 0.0073\n",
      "Epoch [610/800], Loss: 0.0073\n",
      "Epoch [611/800], Loss: 0.0073\n",
      "Epoch [612/800], Loss: 0.0073\n",
      "Epoch [613/800], Loss: 0.0073\n",
      "Epoch [614/800], Loss: 0.0073\n",
      "Epoch [615/800], Loss: 0.0073\n",
      "Epoch [616/800], Loss: 0.0073\n",
      "Epoch [617/800], Loss: 0.0073\n",
      "Epoch [618/800], Loss: 0.0073\n",
      "Epoch [619/800], Loss: 0.0073\n",
      "Epoch [620/800], Loss: 0.0073\n",
      "Epoch [621/800], Loss: 0.0073\n",
      "Epoch [622/800], Loss: 0.0073\n",
      "Epoch [623/800], Loss: 0.0073\n",
      "Epoch [624/800], Loss: 0.0073\n",
      "Epoch [625/800], Loss: 0.0073\n",
      "Epoch [626/800], Loss: 0.0073\n",
      "Epoch [627/800], Loss: 0.0073\n",
      "Epoch [628/800], Loss: 0.0073\n",
      "Epoch [629/800], Loss: 0.0073\n",
      "Epoch [630/800], Loss: 0.0073\n",
      "Epoch [631/800], Loss: 0.0073\n",
      "Epoch [632/800], Loss: 0.0073\n",
      "Epoch [633/800], Loss: 0.0073\n",
      "Epoch [634/800], Loss: 0.0073\n",
      "Epoch [635/800], Loss: 0.0073\n",
      "Epoch [636/800], Loss: 0.0072\n",
      "Epoch [637/800], Loss: 0.0072\n",
      "Epoch [638/800], Loss: 0.0072\n",
      "Epoch [639/800], Loss: 0.0072\n",
      "Epoch [640/800], Loss: 0.0071\n",
      "Epoch [641/800], Loss: 0.0071\n",
      "Epoch [642/800], Loss: 0.0071\n",
      "Epoch [643/800], Loss: 0.0071\n",
      "Epoch [644/800], Loss: 0.0071\n",
      "Epoch [645/800], Loss: 0.0071\n",
      "Epoch [646/800], Loss: 0.0070\n",
      "Epoch [647/800], Loss: 0.0070\n",
      "Epoch [648/800], Loss: 0.0070\n",
      "Epoch [649/800], Loss: 0.0070\n",
      "Epoch [650/800], Loss: 0.0069\n",
      "Epoch [651/800], Loss: 0.0069\n",
      "Epoch [652/800], Loss: 0.0069\n",
      "Epoch [653/800], Loss: 0.0069\n",
      "Epoch [654/800], Loss: 0.0069\n",
      "Epoch [655/800], Loss: 0.0069\n",
      "Epoch [656/800], Loss: 0.0068\n",
      "Epoch [657/800], Loss: 0.0068\n",
      "Epoch [658/800], Loss: 0.0068\n",
      "Epoch [659/800], Loss: 0.0068\n",
      "Epoch [660/800], Loss: 0.0068\n",
      "Epoch [661/800], Loss: 0.0068\n",
      "Epoch [662/800], Loss: 0.0068\n",
      "Epoch [663/800], Loss: 0.0068\n",
      "Epoch [664/800], Loss: 0.0068\n",
      "Epoch [665/800], Loss: 0.0068\n",
      "Epoch [666/800], Loss: 0.0068\n",
      "Epoch [667/800], Loss: 0.0068\n",
      "Epoch [668/800], Loss: 0.0068\n",
      "Epoch [669/800], Loss: 0.0068\n",
      "Epoch [670/800], Loss: 0.0068\n",
      "Epoch [671/800], Loss: 0.0068\n",
      "Epoch [672/800], Loss: 0.0068\n",
      "Epoch [673/800], Loss: 0.0068\n",
      "Epoch [674/800], Loss: 0.0068\n",
      "Epoch [675/800], Loss: 0.0068\n",
      "Epoch [676/800], Loss: 0.0068\n",
      "Epoch [677/800], Loss: 0.0068\n",
      "Epoch [678/800], Loss: 0.0068\n",
      "Epoch [679/800], Loss: 0.0068\n",
      "Epoch [680/800], Loss: 0.0068\n",
      "Epoch [681/800], Loss: 0.0068\n",
      "Epoch [682/800], Loss: 0.0068\n",
      "Epoch [683/800], Loss: 0.0068\n",
      "Epoch [684/800], Loss: 0.0068\n",
      "Epoch [685/800], Loss: 0.0068\n",
      "Epoch [686/800], Loss: 0.0068\n",
      "Epoch [687/800], Loss: 0.0068\n",
      "Epoch [688/800], Loss: 0.0068\n",
      "Epoch [689/800], Loss: 0.0068\n",
      "Epoch [690/800], Loss: 0.0068\n",
      "Epoch [691/800], Loss: 0.0068\n",
      "Epoch [692/800], Loss: 0.0068\n",
      "Epoch [693/800], Loss: 0.0068\n",
      "Epoch [694/800], Loss: 0.0068\n",
      "Epoch [695/800], Loss: 0.0068\n",
      "Epoch [696/800], Loss: 0.0068\n",
      "Epoch [697/800], Loss: 0.0068\n",
      "Epoch [698/800], Loss: 0.0068\n",
      "Epoch [699/800], Loss: 0.0068\n",
      "Epoch [700/800], Loss: 0.0068\n",
      "Epoch [701/800], Loss: 0.0068\n",
      "Epoch [702/800], Loss: 0.0068\n",
      "Epoch [703/800], Loss: 0.0068\n",
      "Epoch [704/800], Loss: 0.0068\n",
      "Epoch [705/800], Loss: 0.0068\n",
      "Epoch [706/800], Loss: 0.0068\n",
      "Epoch [707/800], Loss: 0.0068\n",
      "Epoch [708/800], Loss: 0.0068\n",
      "Epoch [709/800], Loss: 0.0068\n",
      "Epoch [710/800], Loss: 0.0068\n",
      "Epoch [711/800], Loss: 0.0069\n",
      "Epoch [712/800], Loss: 0.0069\n",
      "Epoch [713/800], Loss: 0.0069\n",
      "Epoch [714/800], Loss: 0.0069\n",
      "Epoch [715/800], Loss: 0.0069\n",
      "Epoch [716/800], Loss: 0.0069\n",
      "Epoch [717/800], Loss: 0.0069\n",
      "Epoch [718/800], Loss: 0.0069\n",
      "Epoch [719/800], Loss: 0.0069\n",
      "Epoch [720/800], Loss: 0.0069\n",
      "Epoch [721/800], Loss: 0.0069\n",
      "Epoch [722/800], Loss: 0.0069\n",
      "Epoch [723/800], Loss: 0.0069\n",
      "Epoch [724/800], Loss: 0.0069\n",
      "Epoch [725/800], Loss: 0.0069\n",
      "Epoch [726/800], Loss: 0.0069\n",
      "Epoch [727/800], Loss: 0.0069\n",
      "Epoch [728/800], Loss: 0.0069\n",
      "Epoch [729/800], Loss: 0.0069\n",
      "Epoch [730/800], Loss: 0.0069\n",
      "Epoch [731/800], Loss: 0.0069\n",
      "Epoch [732/800], Loss: 0.0069\n",
      "Epoch [733/800], Loss: 0.0069\n",
      "Epoch [734/800], Loss: 0.0069\n",
      "Epoch [735/800], Loss: 0.0069\n",
      "Epoch [736/800], Loss: 0.0069\n",
      "Epoch [737/800], Loss: 0.0069\n",
      "Epoch [738/800], Loss: 0.0069\n",
      "Epoch [739/800], Loss: 0.0069\n",
      "Epoch [740/800], Loss: 0.0069\n",
      "Epoch [741/800], Loss: 0.0069\n",
      "Epoch [742/800], Loss: 0.0069\n",
      "Epoch [743/800], Loss: 0.0069\n",
      "Epoch [744/800], Loss: 0.0069\n",
      "Epoch [745/800], Loss: 0.0069\n",
      "Epoch [746/800], Loss: 0.0069\n",
      "Epoch [747/800], Loss: 0.0069\n",
      "Epoch [748/800], Loss: 0.0069\n",
      "Epoch [749/800], Loss: 0.0069\n",
      "Epoch [750/800], Loss: 0.0069\n",
      "Epoch [751/800], Loss: 0.0070\n",
      "Epoch [752/800], Loss: 0.0070\n",
      "Epoch [753/800], Loss: 0.0070\n",
      "Epoch [754/800], Loss: 0.0070\n",
      "Epoch [755/800], Loss: 0.0070\n",
      "Epoch [756/800], Loss: 0.0070\n",
      "Epoch [757/800], Loss: 0.0070\n",
      "Epoch [758/800], Loss: 0.0070\n",
      "Epoch [759/800], Loss: 0.0071\n",
      "Epoch [760/800], Loss: 0.0071\n",
      "Epoch [761/800], Loss: 0.0071\n",
      "Epoch [762/800], Loss: 0.0072\n",
      "Epoch [763/800], Loss: 0.0072\n",
      "Epoch [764/800], Loss: 0.0072\n",
      "Epoch [765/800], Loss: 0.0072\n",
      "Epoch [766/800], Loss: 0.0073\n",
      "Epoch [767/800], Loss: 0.0073\n",
      "Epoch [768/800], Loss: 0.0073\n",
      "Epoch [769/800], Loss: 0.0073\n",
      "Epoch [770/800], Loss: 0.0073\n",
      "Epoch [771/800], Loss: 0.0073\n",
      "Epoch [772/800], Loss: 0.0073\n",
      "Epoch [773/800], Loss: 0.0074\n",
      "Epoch [774/800], Loss: 0.0074\n",
      "Epoch [775/800], Loss: 0.0074\n",
      "Epoch [776/800], Loss: 0.0074\n",
      "Epoch [777/800], Loss: 0.0074\n",
      "Epoch [778/800], Loss: 0.0074\n",
      "Epoch [779/800], Loss: 0.0074\n",
      "Epoch [780/800], Loss: 0.0074\n",
      "Epoch [781/800], Loss: 0.0074\n",
      "Epoch [782/800], Loss: 0.0074\n",
      "Epoch [783/800], Loss: 0.0074\n",
      "Epoch [784/800], Loss: 0.0074\n",
      "Epoch [785/800], Loss: 0.0074\n",
      "Epoch [786/800], Loss: 0.0074\n",
      "Epoch [787/800], Loss: 0.0074\n",
      "Epoch [788/800], Loss: 0.0074\n",
      "Epoch [789/800], Loss: 0.0074\n",
      "Epoch [790/800], Loss: 0.0074\n",
      "Epoch [791/800], Loss: 0.0074\n",
      "Epoch [792/800], Loss: 0.0074\n",
      "Epoch [793/800], Loss: 0.0074\n",
      "Epoch [794/800], Loss: 0.0074\n",
      "Epoch [795/800], Loss: 0.0074\n",
      "Epoch [796/800], Loss: 0.0074\n",
      "Epoch [797/800], Loss: 0.0075\n",
      "Epoch [798/800], Loss: 0.0075\n",
      "Epoch [799/800], Loss: 0.0075\n",
      "Epoch [800/800], Loss: 0.0075\n",
      "Accuracy: 49.6032%\n",
      "Epoch [1/800], Loss: 0.2599\n",
      "Epoch [2/800], Loss: 0.2590\n",
      "Epoch [3/800], Loss: 0.2670\n",
      "Epoch [4/800], Loss: 0.2744\n",
      "Epoch [5/800], Loss: 0.2806\n",
      "Epoch [6/800], Loss: 0.2857\n",
      "Epoch [7/800], Loss: 0.2900\n",
      "Epoch [8/800], Loss: 0.2936\n",
      "Epoch [9/800], Loss: 0.2966\n",
      "Epoch [10/800], Loss: 0.2991\n",
      "Epoch [11/800], Loss: 0.3009\n",
      "Epoch [12/800], Loss: 0.3021\n",
      "Epoch [13/800], Loss: 0.3026\n",
      "Epoch [14/800], Loss: 0.3023\n",
      "Epoch [15/800], Loss: 0.3011\n",
      "Epoch [16/800], Loss: 0.2989\n",
      "Epoch [17/800], Loss: 0.2956\n",
      "Epoch [18/800], Loss: 0.2913\n",
      "Epoch [19/800], Loss: 0.2859\n",
      "Epoch [20/800], Loss: 0.2794\n",
      "Epoch [21/800], Loss: 0.2721\n",
      "Epoch [22/800], Loss: 0.2639\n",
      "Epoch [23/800], Loss: 0.2552\n",
      "Epoch [24/800], Loss: 0.2463\n",
      "Epoch [25/800], Loss: 0.2371\n",
      "Epoch [26/800], Loss: 0.2281\n",
      "Epoch [27/800], Loss: 0.2192\n",
      "Epoch [28/800], Loss: 0.2106\n",
      "Epoch [29/800], Loss: 0.2025\n",
      "Epoch [30/800], Loss: 0.1948\n",
      "Epoch [31/800], Loss: 0.1875\n",
      "Epoch [32/800], Loss: 0.1806\n",
      "Epoch [33/800], Loss: 0.1741\n",
      "Epoch [34/800], Loss: 0.1680\n",
      "Epoch [35/800], Loss: 0.1622\n",
      "Epoch [36/800], Loss: 0.1566\n",
      "Epoch [37/800], Loss: 0.1514\n",
      "Epoch [38/800], Loss: 0.1464\n",
      "Epoch [39/800], Loss: 0.1416\n",
      "Epoch [40/800], Loss: 0.1370\n",
      "Epoch [41/800], Loss: 0.1326\n",
      "Epoch [42/800], Loss: 0.1283\n",
      "Epoch [43/800], Loss: 0.1242\n",
      "Epoch [44/800], Loss: 0.1203\n",
      "Epoch [45/800], Loss: 0.1166\n",
      "Epoch [46/800], Loss: 0.1130\n",
      "Epoch [47/800], Loss: 0.1095\n",
      "Epoch [48/800], Loss: 0.1061\n",
      "Epoch [49/800], Loss: 0.1028\n",
      "Epoch [50/800], Loss: 0.0996\n",
      "Epoch [51/800], Loss: 0.0963\n",
      "Epoch [52/800], Loss: 0.0931\n",
      "Epoch [53/800], Loss: 0.0900\n",
      "Epoch [54/800], Loss: 0.0870\n",
      "Epoch [55/800], Loss: 0.0840\n",
      "Epoch [56/800], Loss: 0.0810\n",
      "Epoch [57/800], Loss: 0.0781\n",
      "Epoch [58/800], Loss: 0.0752\n",
      "Epoch [59/800], Loss: 0.0724\n",
      "Epoch [60/800], Loss: 0.0696\n",
      "Epoch [61/800], Loss: 0.0670\n",
      "Epoch [62/800], Loss: 0.0645\n",
      "Epoch [63/800], Loss: 0.0621\n",
      "Epoch [64/800], Loss: 0.0598\n",
      "Epoch [65/800], Loss: 0.0576\n",
      "Epoch [66/800], Loss: 0.0555\n",
      "Epoch [67/800], Loss: 0.0535\n",
      "Epoch [68/800], Loss: 0.0516\n",
      "Epoch [69/800], Loss: 0.0498\n",
      "Epoch [70/800], Loss: 0.0481\n",
      "Epoch [71/800], Loss: 0.0465\n",
      "Epoch [72/800], Loss: 0.0450\n",
      "Epoch [73/800], Loss: 0.0435\n",
      "Epoch [74/800], Loss: 0.0421\n",
      "Epoch [75/800], Loss: 0.0408\n",
      "Epoch [76/800], Loss: 0.0395\n",
      "Epoch [77/800], Loss: 0.0383\n",
      "Epoch [78/800], Loss: 0.0371\n",
      "Epoch [79/800], Loss: 0.0360\n",
      "Epoch [80/800], Loss: 0.0350\n",
      "Epoch [81/800], Loss: 0.0340\n",
      "Epoch [82/800], Loss: 0.0330\n",
      "Epoch [83/800], Loss: 0.0321\n",
      "Epoch [84/800], Loss: 0.0312\n",
      "Epoch [85/800], Loss: 0.0304\n",
      "Epoch [86/800], Loss: 0.0295\n",
      "Epoch [87/800], Loss: 0.0288\n",
      "Epoch [88/800], Loss: 0.0280\n",
      "Epoch [89/800], Loss: 0.0273\n",
      "Epoch [90/800], Loss: 0.0266\n",
      "Epoch [91/800], Loss: 0.0259\n",
      "Epoch [92/800], Loss: 0.0253\n",
      "Epoch [93/800], Loss: 0.0247\n",
      "Epoch [94/800], Loss: 0.0240\n",
      "Epoch [95/800], Loss: 0.0234\n",
      "Epoch [96/800], Loss: 0.0228\n",
      "Epoch [97/800], Loss: 0.0223\n",
      "Epoch [98/800], Loss: 0.0217\n",
      "Epoch [99/800], Loss: 0.0211\n",
      "Epoch [100/800], Loss: 0.0206\n",
      "Epoch [101/800], Loss: 0.0201\n",
      "Epoch [102/800], Loss: 0.0195\n",
      "Epoch [103/800], Loss: 0.0190\n",
      "Epoch [104/800], Loss: 0.0185\n",
      "Epoch [105/800], Loss: 0.0180\n",
      "Epoch [106/800], Loss: 0.0175\n",
      "Epoch [107/800], Loss: 0.0171\n",
      "Epoch [108/800], Loss: 0.0166\n",
      "Epoch [109/800], Loss: 0.0162\n",
      "Epoch [110/800], Loss: 0.0157\n",
      "Epoch [111/800], Loss: 0.0153\n",
      "Epoch [112/800], Loss: 0.0149\n",
      "Epoch [113/800], Loss: 0.0145\n",
      "Epoch [114/800], Loss: 0.0142\n",
      "Epoch [115/800], Loss: 0.0138\n",
      "Epoch [116/800], Loss: 0.0134\n",
      "Epoch [117/800], Loss: 0.0131\n",
      "Epoch [118/800], Loss: 0.0127\n",
      "Epoch [119/800], Loss: 0.0124\n",
      "Epoch [120/800], Loss: 0.0121\n",
      "Epoch [121/800], Loss: 0.0117\n",
      "Epoch [122/800], Loss: 0.0114\n",
      "Epoch [123/800], Loss: 0.0111\n",
      "Epoch [124/800], Loss: 0.0108\n",
      "Epoch [125/800], Loss: 0.0105\n",
      "Epoch [126/800], Loss: 0.0102\n",
      "Epoch [127/800], Loss: 0.0100\n",
      "Epoch [128/800], Loss: 0.0097\n",
      "Epoch [129/800], Loss: 0.0095\n",
      "Epoch [130/800], Loss: 0.0092\n",
      "Epoch [131/800], Loss: 0.0090\n",
      "Epoch [132/800], Loss: 0.0087\n",
      "Epoch [133/800], Loss: 0.0085\n",
      "Epoch [134/800], Loss: 0.0083\n",
      "Epoch [135/800], Loss: 0.0081\n",
      "Epoch [136/800], Loss: 0.0079\n",
      "Epoch [137/800], Loss: 0.0077\n",
      "Epoch [138/800], Loss: 0.0075\n",
      "Epoch [139/800], Loss: 0.0073\n",
      "Epoch [140/800], Loss: 0.0072\n",
      "Epoch [141/800], Loss: 0.0070\n",
      "Epoch [142/800], Loss: 0.0068\n",
      "Epoch [143/800], Loss: 0.0066\n",
      "Epoch [144/800], Loss: 0.0065\n",
      "Epoch [145/800], Loss: 0.0063\n",
      "Epoch [146/800], Loss: 0.0061\n",
      "Epoch [147/800], Loss: 0.0060\n",
      "Epoch [148/800], Loss: 0.0058\n",
      "Epoch [149/800], Loss: 0.0056\n",
      "Epoch [150/800], Loss: 0.0055\n",
      "Epoch [151/800], Loss: 0.0053\n",
      "Epoch [152/800], Loss: 0.0052\n",
      "Epoch [153/800], Loss: 0.0050\n",
      "Epoch [154/800], Loss: 0.0049\n",
      "Epoch [155/800], Loss: 0.0047\n",
      "Epoch [156/800], Loss: 0.0046\n",
      "Epoch [157/800], Loss: 0.0044\n",
      "Epoch [158/800], Loss: 0.0042\n",
      "Epoch [159/800], Loss: 0.0041\n",
      "Epoch [160/800], Loss: 0.0040\n",
      "Epoch [161/800], Loss: 0.0038\n",
      "Epoch [162/800], Loss: 0.0037\n",
      "Epoch [163/800], Loss: 0.0036\n",
      "Epoch [164/800], Loss: 0.0034\n",
      "Epoch [165/800], Loss: 0.0033\n",
      "Epoch [166/800], Loss: 0.0032\n",
      "Epoch [167/800], Loss: 0.0031\n",
      "Epoch [168/800], Loss: 0.0030\n",
      "Epoch [169/800], Loss: 0.0029\n",
      "Epoch [170/800], Loss: 0.0028\n",
      "Epoch [171/800], Loss: 0.0028\n",
      "Epoch [172/800], Loss: 0.0027\n",
      "Epoch [173/800], Loss: 0.0026\n",
      "Epoch [174/800], Loss: 0.0025\n",
      "Epoch [175/800], Loss: 0.0025\n",
      "Epoch [176/800], Loss: 0.0024\n",
      "Epoch [177/800], Loss: 0.0023\n",
      "Epoch [178/800], Loss: 0.0023\n",
      "Epoch [179/800], Loss: 0.0022\n",
      "Epoch [180/800], Loss: 0.0021\n",
      "Epoch [181/800], Loss: 0.0020\n",
      "Epoch [182/800], Loss: 0.0020\n",
      "Epoch [183/800], Loss: 0.0019\n",
      "Epoch [184/800], Loss: 0.0018\n",
      "Epoch [185/800], Loss: 0.0018\n",
      "Epoch [186/800], Loss: 0.0017\n",
      "Epoch [187/800], Loss: 0.0017\n",
      "Epoch [188/800], Loss: 0.0016\n",
      "Epoch [189/800], Loss: 0.0016\n",
      "Epoch [190/800], Loss: 0.0016\n",
      "Epoch [191/800], Loss: 0.0015\n",
      "Epoch [192/800], Loss: 0.0015\n",
      "Epoch [193/800], Loss: 0.0015\n",
      "Epoch [194/800], Loss: 0.0015\n",
      "Epoch [195/800], Loss: 0.0015\n",
      "Epoch [196/800], Loss: 0.0015\n",
      "Epoch [197/800], Loss: 0.0014\n",
      "Epoch [198/800], Loss: 0.0014\n",
      "Epoch [199/800], Loss: 0.0014\n",
      "Epoch [200/800], Loss: 0.0014\n",
      "Epoch [201/800], Loss: 0.0014\n",
      "Epoch [202/800], Loss: 0.0014\n",
      "Epoch [203/800], Loss: 0.0014\n",
      "Epoch [204/800], Loss: 0.0014\n",
      "Epoch [205/800], Loss: 0.0014\n",
      "Epoch [206/800], Loss: 0.0014\n",
      "Epoch [207/800], Loss: 0.0014\n",
      "Epoch [208/800], Loss: 0.0015\n",
      "Epoch [209/800], Loss: 0.0015\n",
      "Epoch [210/800], Loss: 0.0015\n",
      "Epoch [211/800], Loss: 0.0015\n",
      "Epoch [212/800], Loss: 0.0015\n",
      "Epoch [213/800], Loss: 0.0015\n",
      "Epoch [214/800], Loss: 0.0015\n",
      "Epoch [215/800], Loss: 0.0016\n",
      "Epoch [216/800], Loss: 0.0016\n",
      "Epoch [217/800], Loss: 0.0016\n",
      "Epoch [218/800], Loss: 0.0016\n",
      "Epoch [219/800], Loss: 0.0016\n",
      "Epoch [220/800], Loss: 0.0017\n",
      "Epoch [221/800], Loss: 0.0017\n",
      "Epoch [222/800], Loss: 0.0017\n",
      "Epoch [223/800], Loss: 0.0016\n",
      "Epoch [224/800], Loss: 0.0017\n",
      "Epoch [225/800], Loss: 0.0017\n",
      "Epoch [226/800], Loss: 0.0017\n",
      "Epoch [227/800], Loss: 0.0017\n",
      "Epoch [228/800], Loss: 0.0017\n",
      "Epoch [229/800], Loss: 0.0017\n",
      "Epoch [230/800], Loss: 0.0017\n",
      "Epoch [231/800], Loss: 0.0017\n",
      "Epoch [232/800], Loss: 0.0017\n",
      "Epoch [233/800], Loss: 0.0017\n",
      "Epoch [234/800], Loss: 0.0017\n",
      "Epoch [235/800], Loss: 0.0017\n",
      "Epoch [236/800], Loss: 0.0016\n",
      "Epoch [237/800], Loss: 0.0016\n",
      "Epoch [238/800], Loss: 0.0016\n",
      "Epoch [239/800], Loss: 0.0016\n",
      "Epoch [240/800], Loss: 0.0016\n",
      "Epoch [241/800], Loss: 0.0016\n",
      "Epoch [242/800], Loss: 0.0015\n",
      "Epoch [243/800], Loss: 0.0015\n",
      "Epoch [244/800], Loss: 0.0015\n",
      "Epoch [245/800], Loss: 0.0015\n",
      "Epoch [246/800], Loss: 0.0015\n",
      "Epoch [247/800], Loss: 0.0015\n",
      "Epoch [248/800], Loss: 0.0015\n",
      "Epoch [249/800], Loss: 0.0015\n",
      "Epoch [250/800], Loss: 0.0015\n",
      "Epoch [251/800], Loss: 0.0015\n",
      "Epoch [252/800], Loss: 0.0015\n",
      "Epoch [253/800], Loss: 0.0014\n",
      "Epoch [254/800], Loss: 0.0014\n",
      "Epoch [255/800], Loss: 0.0015\n",
      "Epoch [256/800], Loss: 0.0015\n",
      "Epoch [257/800], Loss: 0.0015\n",
      "Epoch [258/800], Loss: 0.0015\n",
      "Epoch [259/800], Loss: 0.0016\n",
      "Epoch [260/800], Loss: 0.0016\n",
      "Epoch [261/800], Loss: 0.0016\n",
      "Epoch [262/800], Loss: 0.0016\n",
      "Epoch [263/800], Loss: 0.0016\n",
      "Epoch [264/800], Loss: 0.0017\n",
      "Epoch [265/800], Loss: 0.0017\n",
      "Epoch [266/800], Loss: 0.0017\n",
      "Epoch [267/800], Loss: 0.0018\n",
      "Epoch [268/800], Loss: 0.0018\n",
      "Epoch [269/800], Loss: 0.0019\n",
      "Epoch [270/800], Loss: 0.0020\n",
      "Epoch [271/800], Loss: 0.0020\n",
      "Epoch [272/800], Loss: 0.0021\n",
      "Epoch [273/800], Loss: 0.0022\n",
      "Epoch [274/800], Loss: 0.0022\n",
      "Epoch [275/800], Loss: 0.0023\n",
      "Epoch [276/800], Loss: 0.0023\n",
      "Epoch [277/800], Loss: 0.0024\n",
      "Epoch [278/800], Loss: 0.0025\n",
      "Epoch [279/800], Loss: 0.0025\n",
      "Epoch [280/800], Loss: 0.0026\n",
      "Epoch [281/800], Loss: 0.0026\n",
      "Epoch [282/800], Loss: 0.0026\n",
      "Epoch [283/800], Loss: 0.0027\n",
      "Epoch [284/800], Loss: 0.0027\n",
      "Epoch [285/800], Loss: 0.0028\n",
      "Epoch [286/800], Loss: 0.0028\n",
      "Epoch [287/800], Loss: 0.0029\n",
      "Epoch [288/800], Loss: 0.0029\n",
      "Epoch [289/800], Loss: 0.0029\n",
      "Epoch [290/800], Loss: 0.0030\n",
      "Epoch [291/800], Loss: 0.0031\n",
      "Epoch [292/800], Loss: 0.0030\n",
      "Epoch [293/800], Loss: 0.0030\n",
      "Epoch [294/800], Loss: 0.0031\n",
      "Epoch [295/800], Loss: 0.0032\n",
      "Epoch [296/800], Loss: 0.0032\n",
      "Epoch [297/800], Loss: 0.0032\n",
      "Epoch [298/800], Loss: 0.0032\n",
      "Epoch [299/800], Loss: 0.0032\n",
      "Epoch [300/800], Loss: 0.0032\n",
      "Epoch [301/800], Loss: 0.0033\n",
      "Epoch [302/800], Loss: 0.0034\n",
      "Epoch [303/800], Loss: 0.0034\n",
      "Epoch [304/800], Loss: 0.0034\n",
      "Epoch [305/800], Loss: 0.0034\n",
      "Epoch [306/800], Loss: 0.0033\n",
      "Epoch [307/800], Loss: 0.0034\n",
      "Epoch [308/800], Loss: 0.0033\n",
      "Epoch [309/800], Loss: 0.0033\n",
      "Epoch [310/800], Loss: 0.0033\n",
      "Epoch [311/800], Loss: 0.0034\n",
      "Epoch [312/800], Loss: 0.0033\n",
      "Epoch [313/800], Loss: 0.0036\n",
      "Epoch [314/800], Loss: 0.0038\n",
      "Epoch [315/800], Loss: 0.0039\n",
      "Epoch [316/800], Loss: 0.0035\n",
      "Epoch [317/800], Loss: 0.0036\n",
      "Epoch [318/800], Loss: 0.0038\n",
      "Epoch [319/800], Loss: 0.0034\n",
      "Epoch [320/800], Loss: 0.0033\n",
      "Epoch [321/800], Loss: 0.0033\n",
      "Epoch [322/800], Loss: 0.0036\n",
      "Epoch [323/800], Loss: 0.0035\n",
      "Epoch [324/800], Loss: 0.0033\n",
      "Epoch [325/800], Loss: 0.0032\n",
      "Epoch [326/800], Loss: 0.0035\n",
      "Epoch [327/800], Loss: 0.0035\n",
      "Epoch [328/800], Loss: 0.0034\n",
      "Epoch [329/800], Loss: 0.0033\n",
      "Epoch [330/800], Loss: 0.0032\n",
      "Epoch [331/800], Loss: 0.0032\n",
      "Epoch [332/800], Loss: 0.0031\n",
      "Epoch [333/800], Loss: 0.0032\n",
      "Epoch [334/800], Loss: 0.0032\n",
      "Epoch [335/800], Loss: 0.0034\n",
      "Epoch [336/800], Loss: 0.0032\n",
      "Epoch [337/800], Loss: 0.0032\n",
      "Epoch [338/800], Loss: 0.0032\n",
      "Epoch [339/800], Loss: 0.0031\n",
      "Epoch [340/800], Loss: 0.0032\n",
      "Epoch [341/800], Loss: 0.0032\n",
      "Epoch [342/800], Loss: 0.0032\n",
      "Epoch [343/800], Loss: 0.0032\n",
      "Epoch [344/800], Loss: 0.0032\n",
      "Epoch [345/800], Loss: 0.0032\n",
      "Epoch [346/800], Loss: 0.0032\n",
      "Epoch [347/800], Loss: 0.0032\n",
      "Epoch [348/800], Loss: 0.0032\n",
      "Epoch [349/800], Loss: 0.0033\n",
      "Epoch [350/800], Loss: 0.0032\n",
      "Epoch [351/800], Loss: 0.0032\n",
      "Epoch [352/800], Loss: 0.0032\n",
      "Epoch [353/800], Loss: 0.0032\n",
      "Epoch [354/800], Loss: 0.0032\n",
      "Epoch [355/800], Loss: 0.0032\n",
      "Epoch [356/800], Loss: 0.0032\n",
      "Epoch [357/800], Loss: 0.0033\n",
      "Epoch [358/800], Loss: 0.0033\n",
      "Epoch [359/800], Loss: 0.0033\n",
      "Epoch [360/800], Loss: 0.0033\n",
      "Epoch [361/800], Loss: 0.0032\n",
      "Epoch [362/800], Loss: 0.0033\n",
      "Epoch [363/800], Loss: 0.0034\n",
      "Epoch [364/800], Loss: 0.0035\n",
      "Epoch [365/800], Loss: 0.0035\n",
      "Epoch [366/800], Loss: 0.0036\n",
      "Epoch [367/800], Loss: 0.0036\n",
      "Epoch [368/800], Loss: 0.0035\n",
      "Epoch [369/800], Loss: 0.0036\n",
      "Epoch [370/800], Loss: 0.0037\n",
      "Epoch [371/800], Loss: 0.0037\n",
      "Epoch [372/800], Loss: 0.0037\n",
      "Epoch [373/800], Loss: 0.0038\n",
      "Epoch [374/800], Loss: 0.0038\n",
      "Epoch [375/800], Loss: 0.0039\n",
      "Epoch [376/800], Loss: 0.0039\n",
      "Epoch [377/800], Loss: 0.0038\n",
      "Epoch [378/800], Loss: 0.0039\n",
      "Epoch [379/800], Loss: 0.0039\n",
      "Epoch [380/800], Loss: 0.0038\n",
      "Epoch [381/800], Loss: 0.0038\n",
      "Epoch [382/800], Loss: 0.0038\n",
      "Epoch [383/800], Loss: 0.0038\n",
      "Epoch [384/800], Loss: 0.0038\n",
      "Epoch [385/800], Loss: 0.0039\n",
      "Epoch [386/800], Loss: 0.0039\n",
      "Epoch [387/800], Loss: 0.0039\n",
      "Epoch [388/800], Loss: 0.0039\n",
      "Epoch [389/800], Loss: 0.0039\n",
      "Epoch [390/800], Loss: 0.0039\n",
      "Epoch [391/800], Loss: 0.0039\n",
      "Epoch [392/800], Loss: 0.0039\n",
      "Epoch [393/800], Loss: 0.0039\n",
      "Epoch [394/800], Loss: 0.0040\n",
      "Epoch [395/800], Loss: 0.0040\n",
      "Epoch [396/800], Loss: 0.0040\n",
      "Epoch [397/800], Loss: 0.0040\n",
      "Epoch [398/800], Loss: 0.0040\n",
      "Epoch [399/800], Loss: 0.0041\n",
      "Epoch [400/800], Loss: 0.0041\n",
      "Epoch [401/800], Loss: 0.0041\n",
      "Epoch [402/800], Loss: 0.0041\n",
      "Epoch [403/800], Loss: 0.0041\n",
      "Epoch [404/800], Loss: 0.0041\n",
      "Epoch [405/800], Loss: 0.0042\n",
      "Epoch [406/800], Loss: 0.0043\n",
      "Epoch [407/800], Loss: 0.0043\n",
      "Epoch [408/800], Loss: 0.0043\n",
      "Epoch [409/800], Loss: 0.0042\n",
      "Epoch [410/800], Loss: 0.0043\n",
      "Epoch [411/800], Loss: 0.0042\n",
      "Epoch [412/800], Loss: 0.0042\n",
      "Epoch [413/800], Loss: 0.0043\n",
      "Epoch [414/800], Loss: 0.0042\n",
      "Epoch [415/800], Loss: 0.0042\n",
      "Epoch [416/800], Loss: 0.0042\n",
      "Epoch [417/800], Loss: 0.0042\n",
      "Epoch [418/800], Loss: 0.0042\n",
      "Epoch [419/800], Loss: 0.0042\n",
      "Epoch [420/800], Loss: 0.0042\n",
      "Epoch [421/800], Loss: 0.0042\n",
      "Epoch [422/800], Loss: 0.0042\n",
      "Epoch [423/800], Loss: 0.0042\n",
      "Epoch [424/800], Loss: 0.0042\n",
      "Epoch [425/800], Loss: 0.0042\n",
      "Epoch [426/800], Loss: 0.0041\n",
      "Epoch [427/800], Loss: 0.0042\n",
      "Epoch [428/800], Loss: 0.0042\n",
      "Epoch [429/800], Loss: 0.0041\n",
      "Epoch [430/800], Loss: 0.0041\n",
      "Epoch [431/800], Loss: 0.0041\n",
      "Epoch [432/800], Loss: 0.0041\n",
      "Epoch [433/800], Loss: 0.0041\n",
      "Epoch [434/800], Loss: 0.0042\n",
      "Epoch [435/800], Loss: 0.0042\n",
      "Epoch [436/800], Loss: 0.0042\n",
      "Epoch [437/800], Loss: 0.0042\n",
      "Epoch [438/800], Loss: 0.0042\n",
      "Epoch [439/800], Loss: 0.0042\n",
      "Epoch [440/800], Loss: 0.0042\n",
      "Epoch [441/800], Loss: 0.0042\n",
      "Epoch [442/800], Loss: 0.0042\n",
      "Epoch [443/800], Loss: 0.0041\n",
      "Epoch [444/800], Loss: 0.0041\n",
      "Epoch [445/800], Loss: 0.0041\n",
      "Epoch [446/800], Loss: 0.0042\n",
      "Epoch [447/800], Loss: 0.0041\n",
      "Epoch [448/800], Loss: 0.0041\n",
      "Epoch [449/800], Loss: 0.0040\n",
      "Epoch [450/800], Loss: 0.0040\n",
      "Epoch [451/800], Loss: 0.0041\n",
      "Epoch [452/800], Loss: 0.0042\n",
      "Epoch [453/800], Loss: 0.0043\n",
      "Epoch [454/800], Loss: 0.0043\n",
      "Epoch [455/800], Loss: 0.0042\n",
      "Epoch [456/800], Loss: 0.0042\n",
      "Epoch [457/800], Loss: 0.0042\n",
      "Epoch [458/800], Loss: 0.0042\n",
      "Epoch [459/800], Loss: 0.0042\n",
      "Epoch [460/800], Loss: 0.0042\n",
      "Epoch [461/800], Loss: 0.0041\n",
      "Epoch [462/800], Loss: 0.0042\n",
      "Epoch [463/800], Loss: 0.0041\n",
      "Epoch [464/800], Loss: 0.0042\n",
      "Epoch [465/800], Loss: 0.0042\n",
      "Epoch [466/800], Loss: 0.0042\n",
      "Epoch [467/800], Loss: 0.0042\n",
      "Epoch [468/800], Loss: 0.0042\n",
      "Epoch [469/800], Loss: 0.0042\n",
      "Epoch [470/800], Loss: 0.0042\n",
      "Epoch [471/800], Loss: 0.0042\n",
      "Epoch [472/800], Loss: 0.0043\n",
      "Epoch [473/800], Loss: 0.0042\n",
      "Epoch [474/800], Loss: 0.0042\n",
      "Epoch [475/800], Loss: 0.0042\n",
      "Epoch [476/800], Loss: 0.0042\n",
      "Epoch [477/800], Loss: 0.0042\n",
      "Epoch [478/800], Loss: 0.0043\n",
      "Epoch [479/800], Loss: 0.0044\n",
      "Epoch [480/800], Loss: 0.0042\n",
      "Epoch [481/800], Loss: 0.0042\n",
      "Epoch [482/800], Loss: 0.0043\n",
      "Epoch [483/800], Loss: 0.0043\n",
      "Epoch [484/800], Loss: 0.0042\n",
      "Epoch [485/800], Loss: 0.0042\n",
      "Epoch [486/800], Loss: 0.0043\n",
      "Epoch [487/800], Loss: 0.0043\n",
      "Epoch [488/800], Loss: 0.0043\n",
      "Epoch [489/800], Loss: 0.0044\n",
      "Epoch [490/800], Loss: 0.0043\n",
      "Epoch [491/800], Loss: 0.0043\n",
      "Epoch [492/800], Loss: 0.0043\n",
      "Epoch [493/800], Loss: 0.0043\n",
      "Epoch [494/800], Loss: 0.0044\n",
      "Epoch [495/800], Loss: 0.0042\n",
      "Epoch [496/800], Loss: 0.0043\n",
      "Epoch [497/800], Loss: 0.0043\n",
      "Epoch [498/800], Loss: 0.0043\n",
      "Epoch [499/800], Loss: 0.0042\n",
      "Epoch [500/800], Loss: 0.0043\n",
      "Epoch [501/800], Loss: 0.0043\n",
      "Epoch [502/800], Loss: 0.0043\n",
      "Epoch [503/800], Loss: 0.0043\n",
      "Epoch [504/800], Loss: 0.0043\n",
      "Epoch [505/800], Loss: 0.0043\n",
      "Epoch [506/800], Loss: 0.0044\n",
      "Epoch [507/800], Loss: 0.0042\n",
      "Epoch [508/800], Loss: 0.0043\n",
      "Epoch [509/800], Loss: 0.0044\n",
      "Epoch [510/800], Loss: 0.0043\n",
      "Epoch [511/800], Loss: 0.0043\n",
      "Epoch [512/800], Loss: 0.0043\n",
      "Epoch [513/800], Loss: 0.0042\n",
      "Epoch [514/800], Loss: 0.0043\n",
      "Epoch [515/800], Loss: 0.0042\n",
      "Epoch [516/800], Loss: 0.0043\n",
      "Epoch [517/800], Loss: 0.0043\n",
      "Epoch [518/800], Loss: 0.0044\n",
      "Epoch [519/800], Loss: 0.0043\n",
      "Epoch [520/800], Loss: 0.0043\n",
      "Epoch [521/800], Loss: 0.0043\n",
      "Epoch [522/800], Loss: 0.0043\n",
      "Epoch [523/800], Loss: 0.0042\n",
      "Epoch [524/800], Loss: 0.0042\n",
      "Epoch [525/800], Loss: 0.0043\n",
      "Epoch [526/800], Loss: 0.0042\n",
      "Epoch [527/800], Loss: 0.0042\n",
      "Epoch [528/800], Loss: 0.0042\n",
      "Epoch [529/800], Loss: 0.0042\n",
      "Epoch [530/800], Loss: 0.0043\n",
      "Epoch [531/800], Loss: 0.0043\n",
      "Epoch [532/800], Loss: 0.0044\n",
      "Epoch [533/800], Loss: 0.0043\n",
      "Epoch [534/800], Loss: 0.0044\n",
      "Epoch [535/800], Loss: 0.0043\n",
      "Epoch [536/800], Loss: 0.0043\n",
      "Epoch [537/800], Loss: 0.0042\n",
      "Epoch [538/800], Loss: 0.0040\n",
      "Epoch [539/800], Loss: 0.0041\n",
      "Epoch [540/800], Loss: 0.0041\n",
      "Epoch [541/800], Loss: 0.0041\n",
      "Epoch [542/800], Loss: 0.0040\n",
      "Epoch [543/800], Loss: 0.0041\n",
      "Epoch [544/800], Loss: 0.0041\n",
      "Epoch [545/800], Loss: 0.0040\n",
      "Epoch [546/800], Loss: 0.0040\n",
      "Epoch [547/800], Loss: 0.0039\n",
      "Epoch [548/800], Loss: 0.0040\n",
      "Epoch [549/800], Loss: 0.0040\n",
      "Epoch [550/800], Loss: 0.0038\n",
      "Epoch [551/800], Loss: 0.0040\n",
      "Epoch [552/800], Loss: 0.0038\n",
      "Epoch [553/800], Loss: 0.0039\n",
      "Epoch [554/800], Loss: 0.0039\n",
      "Epoch [555/800], Loss: 0.0040\n",
      "Epoch [556/800], Loss: 0.0039\n",
      "Epoch [557/800], Loss: 0.0039\n",
      "Epoch [558/800], Loss: 0.0040\n",
      "Epoch [559/800], Loss: 0.0039\n",
      "Epoch [560/800], Loss: 0.0039\n",
      "Epoch [561/800], Loss: 0.0039\n",
      "Epoch [562/800], Loss: 0.0038\n",
      "Epoch [563/800], Loss: 0.0037\n",
      "Epoch [564/800], Loss: 0.0038\n",
      "Epoch [565/800], Loss: 0.0038\n",
      "Epoch [566/800], Loss: 0.0038\n",
      "Epoch [567/800], Loss: 0.0039\n",
      "Epoch [568/800], Loss: 0.0037\n",
      "Epoch [569/800], Loss: 0.0037\n",
      "Epoch [570/800], Loss: 0.0037\n",
      "Epoch [571/800], Loss: 0.0038\n",
      "Epoch [572/800], Loss: 0.0039\n",
      "Epoch [573/800], Loss: 0.0038\n",
      "Epoch [574/800], Loss: 0.0037\n",
      "Epoch [575/800], Loss: 0.0037\n",
      "Epoch [576/800], Loss: 0.0039\n",
      "Epoch [577/800], Loss: 0.0038\n",
      "Epoch [578/800], Loss: 0.0037\n",
      "Epoch [579/800], Loss: 0.0037\n",
      "Epoch [580/800], Loss: 0.0038\n",
      "Epoch [581/800], Loss: 0.0037\n",
      "Epoch [582/800], Loss: 0.0036\n",
      "Epoch [583/800], Loss: 0.0037\n",
      "Epoch [584/800], Loss: 0.0037\n",
      "Epoch [585/800], Loss: 0.0038\n",
      "Epoch [586/800], Loss: 0.0037\n",
      "Epoch [587/800], Loss: 0.0037\n",
      "Epoch [588/800], Loss: 0.0036\n",
      "Epoch [589/800], Loss: 0.0036\n",
      "Epoch [590/800], Loss: 0.0036\n",
      "Epoch [591/800], Loss: 0.0036\n",
      "Epoch [592/800], Loss: 0.0036\n",
      "Epoch [593/800], Loss: 0.0035\n",
      "Epoch [594/800], Loss: 0.0036\n",
      "Epoch [595/800], Loss: 0.0035\n",
      "Epoch [596/800], Loss: 0.0037\n",
      "Epoch [597/800], Loss: 0.0038\n",
      "Epoch [598/800], Loss: 0.0036\n",
      "Epoch [599/800], Loss: 0.0037\n",
      "Epoch [600/800], Loss: 0.0036\n",
      "Epoch [601/800], Loss: 0.0036\n",
      "Epoch [602/800], Loss: 0.0034\n",
      "Epoch [603/800], Loss: 0.0036\n",
      "Epoch [604/800], Loss: 0.0037\n",
      "Epoch [605/800], Loss: 0.0035\n",
      "Epoch [606/800], Loss: 0.0037\n",
      "Epoch [607/800], Loss: 0.0038\n",
      "Epoch [608/800], Loss: 0.0036\n",
      "Epoch [609/800], Loss: 0.0038\n",
      "Epoch [610/800], Loss: 0.0036\n",
      "Epoch [611/800], Loss: 0.0036\n",
      "Epoch [612/800], Loss: 0.0035\n",
      "Epoch [613/800], Loss: 0.0038\n",
      "Epoch [614/800], Loss: 0.0037\n",
      "Epoch [615/800], Loss: 0.0036\n",
      "Epoch [616/800], Loss: 0.0036\n",
      "Epoch [617/800], Loss: 0.0035\n",
      "Epoch [618/800], Loss: 0.0034\n",
      "Epoch [619/800], Loss: 0.0035\n",
      "Epoch [620/800], Loss: 0.0035\n",
      "Epoch [621/800], Loss: 0.0034\n",
      "Epoch [622/800], Loss: 0.0036\n",
      "Epoch [623/800], Loss: 0.0035\n",
      "Epoch [624/800], Loss: 0.0035\n",
      "Epoch [625/800], Loss: 0.0037\n",
      "Epoch [626/800], Loss: 0.0036\n",
      "Epoch [627/800], Loss: 0.0036\n",
      "Epoch [628/800], Loss: 0.0038\n",
      "Epoch [629/800], Loss: 0.0037\n",
      "Epoch [630/800], Loss: 0.0035\n",
      "Epoch [631/800], Loss: 0.0036\n",
      "Epoch [632/800], Loss: 0.0036\n",
      "Epoch [633/800], Loss: 0.0035\n",
      "Epoch [634/800], Loss: 0.0036\n",
      "Epoch [635/800], Loss: 0.0036\n",
      "Epoch [636/800], Loss: 0.0036\n",
      "Epoch [637/800], Loss: 0.0035\n",
      "Epoch [638/800], Loss: 0.0036\n",
      "Epoch [639/800], Loss: 0.0036\n",
      "Epoch [640/800], Loss: 0.0035\n",
      "Epoch [641/800], Loss: 0.0036\n",
      "Epoch [642/800], Loss: 0.0035\n",
      "Epoch [643/800], Loss: 0.0036\n",
      "Epoch [644/800], Loss: 0.0037\n",
      "Epoch [645/800], Loss: 0.0036\n",
      "Epoch [646/800], Loss: 0.0037\n",
      "Epoch [647/800], Loss: 0.0037\n",
      "Epoch [648/800], Loss: 0.0037\n",
      "Epoch [649/800], Loss: 0.0036\n",
      "Epoch [650/800], Loss: 0.0036\n",
      "Epoch [651/800], Loss: 0.0037\n",
      "Epoch [652/800], Loss: 0.0038\n",
      "Epoch [653/800], Loss: 0.0036\n",
      "Epoch [654/800], Loss: 0.0037\n",
      "Epoch [655/800], Loss: 0.0038\n",
      "Epoch [656/800], Loss: 0.0037\n",
      "Epoch [657/800], Loss: 0.0037\n",
      "Epoch [658/800], Loss: 0.0036\n",
      "Epoch [659/800], Loss: 0.0036\n",
      "Epoch [660/800], Loss: 0.0037\n",
      "Epoch [661/800], Loss: 0.0036\n",
      "Epoch [662/800], Loss: 0.0037\n",
      "Epoch [663/800], Loss: 0.0037\n",
      "Epoch [664/800], Loss: 0.0036\n",
      "Epoch [665/800], Loss: 0.0036\n",
      "Epoch [666/800], Loss: 0.0037\n",
      "Epoch [667/800], Loss: 0.0037\n",
      "Epoch [668/800], Loss: 0.0037\n",
      "Epoch [669/800], Loss: 0.0037\n",
      "Epoch [670/800], Loss: 0.0037\n",
      "Epoch [671/800], Loss: 0.0036\n",
      "Epoch [672/800], Loss: 0.0037\n",
      "Epoch [673/800], Loss: 0.0037\n",
      "Epoch [674/800], Loss: 0.0036\n",
      "Epoch [675/800], Loss: 0.0037\n",
      "Epoch [676/800], Loss: 0.0036\n",
      "Epoch [677/800], Loss: 0.0037\n",
      "Epoch [678/800], Loss: 0.0036\n",
      "Epoch [679/800], Loss: 0.0037\n",
      "Epoch [680/800], Loss: 0.0035\n",
      "Epoch [681/800], Loss: 0.0036\n",
      "Epoch [682/800], Loss: 0.0037\n",
      "Epoch [683/800], Loss: 0.0037\n",
      "Epoch [684/800], Loss: 0.0035\n",
      "Epoch [685/800], Loss: 0.0037\n",
      "Epoch [686/800], Loss: 0.0036\n",
      "Epoch [687/800], Loss: 0.0036\n",
      "Epoch [688/800], Loss: 0.0037\n",
      "Epoch [689/800], Loss: 0.0038\n",
      "Epoch [690/800], Loss: 0.0037\n",
      "Epoch [691/800], Loss: 0.0037\n",
      "Epoch [692/800], Loss: 0.0037\n",
      "Epoch [693/800], Loss: 0.0036\n",
      "Epoch [694/800], Loss: 0.0036\n",
      "Epoch [695/800], Loss: 0.0037\n",
      "Epoch [696/800], Loss: 0.0037\n",
      "Epoch [697/800], Loss: 0.0036\n",
      "Epoch [698/800], Loss: 0.0035\n",
      "Epoch [699/800], Loss: 0.0036\n",
      "Epoch [700/800], Loss: 0.0036\n",
      "Epoch [701/800], Loss: 0.0036\n",
      "Epoch [702/800], Loss: 0.0035\n",
      "Epoch [703/800], Loss: 0.0035\n",
      "Epoch [704/800], Loss: 0.0035\n",
      "Epoch [705/800], Loss: 0.0034\n",
      "Epoch [706/800], Loss: 0.0035\n",
      "Epoch [707/800], Loss: 0.0036\n",
      "Epoch [708/800], Loss: 0.0035\n",
      "Epoch [709/800], Loss: 0.0035\n",
      "Epoch [710/800], Loss: 0.0036\n",
      "Epoch [711/800], Loss: 0.0035\n",
      "Epoch [712/800], Loss: 0.0036\n",
      "Epoch [713/800], Loss: 0.0036\n",
      "Epoch [714/800], Loss: 0.0035\n",
      "Epoch [715/800], Loss: 0.0035\n",
      "Epoch [716/800], Loss: 0.0035\n",
      "Epoch [717/800], Loss: 0.0035\n",
      "Epoch [718/800], Loss: 0.0036\n",
      "Epoch [719/800], Loss: 0.0035\n",
      "Epoch [720/800], Loss: 0.0036\n",
      "Epoch [721/800], Loss: 0.0036\n",
      "Epoch [722/800], Loss: 0.0036\n",
      "Epoch [723/800], Loss: 0.0035\n",
      "Epoch [724/800], Loss: 0.0035\n",
      "Epoch [725/800], Loss: 0.0034\n",
      "Epoch [726/800], Loss: 0.0034\n",
      "Epoch [727/800], Loss: 0.0035\n",
      "Epoch [728/800], Loss: 0.0035\n",
      "Epoch [729/800], Loss: 0.0035\n",
      "Epoch [730/800], Loss: 0.0034\n",
      "Epoch [731/800], Loss: 0.0035\n",
      "Epoch [732/800], Loss: 0.0036\n",
      "Epoch [733/800], Loss: 0.0035\n",
      "Epoch [734/800], Loss: 0.0035\n",
      "Epoch [735/800], Loss: 0.0035\n",
      "Epoch [736/800], Loss: 0.0035\n",
      "Epoch [737/800], Loss: 0.0034\n",
      "Epoch [738/800], Loss: 0.0034\n",
      "Epoch [739/800], Loss: 0.0036\n",
      "Epoch [740/800], Loss: 0.0035\n",
      "Epoch [741/800], Loss: 0.0034\n",
      "Epoch [742/800], Loss: 0.0034\n",
      "Epoch [743/800], Loss: 0.0035\n",
      "Epoch [744/800], Loss: 0.0034\n",
      "Epoch [745/800], Loss: 0.0035\n",
      "Epoch [746/800], Loss: 0.0035\n",
      "Epoch [747/800], Loss: 0.0034\n",
      "Epoch [748/800], Loss: 0.0035\n",
      "Epoch [749/800], Loss: 0.0035\n",
      "Epoch [750/800], Loss: 0.0035\n",
      "Epoch [751/800], Loss: 0.0036\n",
      "Epoch [752/800], Loss: 0.0035\n",
      "Epoch [753/800], Loss: 0.0035\n",
      "Epoch [754/800], Loss: 0.0035\n",
      "Epoch [755/800], Loss: 0.0035\n",
      "Epoch [756/800], Loss: 0.0036\n",
      "Epoch [757/800], Loss: 0.0036\n",
      "Epoch [758/800], Loss: 0.0034\n",
      "Epoch [759/800], Loss: 0.0035\n",
      "Epoch [760/800], Loss: 0.0035\n",
      "Epoch [761/800], Loss: 0.0036\n",
      "Epoch [762/800], Loss: 0.0036\n",
      "Epoch [763/800], Loss: 0.0035\n",
      "Epoch [764/800], Loss: 0.0036\n",
      "Epoch [765/800], Loss: 0.0037\n",
      "Epoch [766/800], Loss: 0.0037\n",
      "Epoch [767/800], Loss: 0.0036\n",
      "Epoch [768/800], Loss: 0.0037\n",
      "Epoch [769/800], Loss: 0.0036\n",
      "Epoch [770/800], Loss: 0.0036\n",
      "Epoch [771/800], Loss: 0.0035\n",
      "Epoch [772/800], Loss: 0.0036\n",
      "Epoch [773/800], Loss: 0.0035\n",
      "Epoch [774/800], Loss: 0.0035\n",
      "Epoch [775/800], Loss: 0.0034\n",
      "Epoch [776/800], Loss: 0.0037\n",
      "Epoch [777/800], Loss: 0.0036\n",
      "Epoch [778/800], Loss: 0.0036\n",
      "Epoch [779/800], Loss: 0.0037\n",
      "Epoch [780/800], Loss: 0.0036\n",
      "Epoch [781/800], Loss: 0.0037\n",
      "Epoch [782/800], Loss: 0.0036\n",
      "Epoch [783/800], Loss: 0.0036\n",
      "Epoch [784/800], Loss: 0.0037\n",
      "Epoch [785/800], Loss: 0.0036\n",
      "Epoch [786/800], Loss: 0.0036\n",
      "Epoch [787/800], Loss: 0.0035\n",
      "Epoch [788/800], Loss: 0.0037\n",
      "Epoch [789/800], Loss: 0.0036\n",
      "Epoch [790/800], Loss: 0.0037\n",
      "Epoch [791/800], Loss: 0.0037\n",
      "Epoch [792/800], Loss: 0.0037\n",
      "Epoch [793/800], Loss: 0.0038\n",
      "Epoch [794/800], Loss: 0.0037\n",
      "Epoch [795/800], Loss: 0.0038\n",
      "Epoch [796/800], Loss: 0.0038\n",
      "Epoch [797/800], Loss: 0.0037\n",
      "Epoch [798/800], Loss: 0.0039\n",
      "Epoch [799/800], Loss: 0.0037\n",
      "Epoch [800/800], Loss: 0.0037\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/800], Loss: 0.0315\n",
      "Epoch [2/800], Loss: 0.0481\n",
      "Epoch [3/800], Loss: 0.0588\n",
      "Epoch [4/800], Loss: 0.0672\n",
      "Epoch [5/800], Loss: 0.0744\n",
      "Epoch [6/800], Loss: 0.0812\n",
      "Epoch [7/800], Loss: 0.0881\n",
      "Epoch [8/800], Loss: 0.0953\n",
      "Epoch [9/800], Loss: 0.1028\n",
      "Epoch [10/800], Loss: 0.1107\n",
      "Epoch [11/800], Loss: 0.1187\n",
      "Epoch [12/800], Loss: 0.1267\n",
      "Epoch [13/800], Loss: 0.1346\n",
      "Epoch [14/800], Loss: 0.1422\n",
      "Epoch [15/800], Loss: 0.1492\n",
      "Epoch [16/800], Loss: 0.1554\n",
      "Epoch [17/800], Loss: 0.1607\n",
      "Epoch [18/800], Loss: 0.1648\n",
      "Epoch [19/800], Loss: 0.1677\n",
      "Epoch [20/800], Loss: 0.1690\n",
      "Epoch [21/800], Loss: 0.1686\n",
      "Epoch [22/800], Loss: 0.1665\n",
      "Epoch [23/800], Loss: 0.1625\n",
      "Epoch [24/800], Loss: 0.1566\n",
      "Epoch [25/800], Loss: 0.1492\n",
      "Epoch [26/800], Loss: 0.1406\n",
      "Epoch [27/800], Loss: 0.1312\n",
      "Epoch [28/800], Loss: 0.1214\n",
      "Epoch [29/800], Loss: 0.1118\n",
      "Epoch [30/800], Loss: 0.1024\n",
      "Epoch [31/800], Loss: 0.0931\n",
      "Epoch [32/800], Loss: 0.0840\n",
      "Epoch [33/800], Loss: 0.0754\n",
      "Epoch [34/800], Loss: 0.0674\n",
      "Epoch [35/800], Loss: 0.0602\n",
      "Epoch [36/800], Loss: 0.0540\n",
      "Epoch [37/800], Loss: 0.0485\n",
      "Epoch [38/800], Loss: 0.0437\n",
      "Epoch [39/800], Loss: 0.0395\n",
      "Epoch [40/800], Loss: 0.0359\n",
      "Epoch [41/800], Loss: 0.0327\n",
      "Epoch [42/800], Loss: 0.0300\n",
      "Epoch [43/800], Loss: 0.0275\n",
      "Epoch [44/800], Loss: 0.0253\n",
      "Epoch [45/800], Loss: 0.0234\n",
      "Epoch [46/800], Loss: 0.0217\n",
      "Epoch [47/800], Loss: 0.0202\n",
      "Epoch [48/800], Loss: 0.0188\n",
      "Epoch [49/800], Loss: 0.0176\n",
      "Epoch [50/800], Loss: 0.0165\n",
      "Epoch [51/800], Loss: 0.0155\n",
      "Epoch [52/800], Loss: 0.0146\n",
      "Epoch [53/800], Loss: 0.0138\n",
      "Epoch [54/800], Loss: 0.0130\n",
      "Epoch [55/800], Loss: 0.0124\n",
      "Epoch [56/800], Loss: 0.0117\n",
      "Epoch [57/800], Loss: 0.0111\n",
      "Epoch [58/800], Loss: 0.0105\n",
      "Epoch [59/800], Loss: 0.0100\n",
      "Epoch [60/800], Loss: 0.0095\n",
      "Epoch [61/800], Loss: 0.0091\n",
      "Epoch [62/800], Loss: 0.0086\n",
      "Epoch [63/800], Loss: 0.0082\n",
      "Epoch [64/800], Loss: 0.0079\n",
      "Epoch [65/800], Loss: 0.0075\n",
      "Epoch [66/800], Loss: 0.0072\n",
      "Epoch [67/800], Loss: 0.0069\n",
      "Epoch [68/800], Loss: 0.0066\n",
      "Epoch [69/800], Loss: 0.0063\n",
      "Epoch [70/800], Loss: 0.0060\n",
      "Epoch [71/800], Loss: 0.0058\n",
      "Epoch [72/800], Loss: 0.0056\n",
      "Epoch [73/800], Loss: 0.0054\n",
      "Epoch [74/800], Loss: 0.0052\n",
      "Epoch [75/800], Loss: 0.0050\n",
      "Epoch [76/800], Loss: 0.0048\n",
      "Epoch [77/800], Loss: 0.0047\n",
      "Epoch [78/800], Loss: 0.0045\n",
      "Epoch [79/800], Loss: 0.0044\n",
      "Epoch [80/800], Loss: 0.0043\n",
      "Epoch [81/800], Loss: 0.0042\n",
      "Epoch [82/800], Loss: 0.0041\n",
      "Epoch [83/800], Loss: 0.0040\n",
      "Epoch [84/800], Loss: 0.0039\n",
      "Epoch [85/800], Loss: 0.0038\n",
      "Epoch [86/800], Loss: 0.0037\n",
      "Epoch [87/800], Loss: 0.0037\n",
      "Epoch [88/800], Loss: 0.0036\n",
      "Epoch [89/800], Loss: 0.0035\n",
      "Epoch [90/800], Loss: 0.0035\n",
      "Epoch [91/800], Loss: 0.0034\n",
      "Epoch [92/800], Loss: 0.0033\n",
      "Epoch [93/800], Loss: 0.0033\n",
      "Epoch [94/800], Loss: 0.0032\n",
      "Epoch [95/800], Loss: 0.0032\n",
      "Epoch [96/800], Loss: 0.0031\n",
      "Epoch [97/800], Loss: 0.0031\n",
      "Epoch [98/800], Loss: 0.0030\n",
      "Epoch [99/800], Loss: 0.0030\n",
      "Epoch [100/800], Loss: 0.0030\n",
      "Epoch [101/800], Loss: 0.0029\n",
      "Epoch [102/800], Loss: 0.0029\n",
      "Epoch [103/800], Loss: 0.0029\n",
      "Epoch [104/800], Loss: 0.0028\n",
      "Epoch [105/800], Loss: 0.0028\n",
      "Epoch [106/800], Loss: 0.0028\n",
      "Epoch [107/800], Loss: 0.0027\n",
      "Epoch [108/800], Loss: 0.0027\n",
      "Epoch [109/800], Loss: 0.0027\n",
      "Epoch [110/800], Loss: 0.0027\n",
      "Epoch [111/800], Loss: 0.0026\n",
      "Epoch [112/800], Loss: 0.0026\n",
      "Epoch [113/800], Loss: 0.0026\n",
      "Epoch [114/800], Loss: 0.0025\n",
      "Epoch [115/800], Loss: 0.0025\n",
      "Epoch [116/800], Loss: 0.0025\n",
      "Epoch [117/800], Loss: 0.0024\n",
      "Epoch [118/800], Loss: 0.0024\n",
      "Epoch [119/800], Loss: 0.0024\n",
      "Epoch [120/800], Loss: 0.0024\n",
      "Epoch [121/800], Loss: 0.0023\n",
      "Epoch [122/800], Loss: 0.0023\n",
      "Epoch [123/800], Loss: 0.0023\n",
      "Epoch [124/800], Loss: 0.0022\n",
      "Epoch [125/800], Loss: 0.0022\n",
      "Epoch [126/800], Loss: 0.0022\n",
      "Epoch [127/800], Loss: 0.0022\n",
      "Epoch [128/800], Loss: 0.0021\n",
      "Epoch [129/800], Loss: 0.0021\n",
      "Epoch [130/800], Loss: 0.0021\n",
      "Epoch [131/800], Loss: 0.0020\n",
      "Epoch [132/800], Loss: 0.0020\n",
      "Epoch [133/800], Loss: 0.0020\n",
      "Epoch [134/800], Loss: 0.0020\n",
      "Epoch [135/800], Loss: 0.0019\n",
      "Epoch [136/800], Loss: 0.0019\n",
      "Epoch [137/800], Loss: 0.0019\n",
      "Epoch [138/800], Loss: 0.0019\n",
      "Epoch [139/800], Loss: 0.0018\n",
      "Epoch [140/800], Loss: 0.0018\n",
      "Epoch [141/800], Loss: 0.0018\n",
      "Epoch [142/800], Loss: 0.0018\n",
      "Epoch [143/800], Loss: 0.0018\n",
      "Epoch [144/800], Loss: 0.0017\n",
      "Epoch [145/800], Loss: 0.0017\n",
      "Epoch [146/800], Loss: 0.0017\n",
      "Epoch [147/800], Loss: 0.0017\n",
      "Epoch [148/800], Loss: 0.0016\n",
      "Epoch [149/800], Loss: 0.0016\n",
      "Epoch [150/800], Loss: 0.0016\n",
      "Epoch [151/800], Loss: 0.0016\n",
      "Epoch [152/800], Loss: 0.0016\n",
      "Epoch [153/800], Loss: 0.0015\n",
      "Epoch [154/800], Loss: 0.0015\n",
      "Epoch [155/800], Loss: 0.0015\n",
      "Epoch [156/800], Loss: 0.0015\n",
      "Epoch [157/800], Loss: 0.0015\n",
      "Epoch [158/800], Loss: 0.0014\n",
      "Epoch [159/800], Loss: 0.0014\n",
      "Epoch [160/800], Loss: 0.0014\n",
      "Epoch [161/800], Loss: 0.0014\n",
      "Epoch [162/800], Loss: 0.0014\n",
      "Epoch [163/800], Loss: 0.0013\n",
      "Epoch [164/800], Loss: 0.0013\n",
      "Epoch [165/800], Loss: 0.0013\n",
      "Epoch [166/800], Loss: 0.0013\n",
      "Epoch [167/800], Loss: 0.0013\n",
      "Epoch [168/800], Loss: 0.0012\n",
      "Epoch [169/800], Loss: 0.0012\n",
      "Epoch [170/800], Loss: 0.0012\n",
      "Epoch [171/800], Loss: 0.0012\n",
      "Epoch [172/800], Loss: 0.0012\n",
      "Epoch [173/800], Loss: 0.0011\n",
      "Epoch [174/800], Loss: 0.0011\n",
      "Epoch [175/800], Loss: 0.0011\n",
      "Epoch [176/800], Loss: 0.0011\n",
      "Epoch [177/800], Loss: 0.0011\n",
      "Epoch [178/800], Loss: 0.0010\n",
      "Epoch [179/800], Loss: 0.0010\n",
      "Epoch [180/800], Loss: 0.0010\n",
      "Epoch [181/800], Loss: 0.0010\n",
      "Epoch [182/800], Loss: 0.0010\n",
      "Epoch [183/800], Loss: 0.0009\n",
      "Epoch [184/800], Loss: 0.0009\n",
      "Epoch [185/800], Loss: 0.0009\n",
      "Epoch [186/800], Loss: 0.0009\n",
      "Epoch [187/800], Loss: 0.0009\n",
      "Epoch [188/800], Loss: 0.0009\n",
      "Epoch [189/800], Loss: 0.0009\n",
      "Epoch [190/800], Loss: 0.0008\n",
      "Epoch [191/800], Loss: 0.0008\n",
      "Epoch [192/800], Loss: 0.0008\n",
      "Epoch [193/800], Loss: 0.0008\n",
      "Epoch [194/800], Loss: 0.0008\n",
      "Epoch [195/800], Loss: 0.0008\n",
      "Epoch [196/800], Loss: 0.0008\n",
      "Epoch [197/800], Loss: 0.0007\n",
      "Epoch [198/800], Loss: 0.0007\n",
      "Epoch [199/800], Loss: 0.0007\n",
      "Epoch [200/800], Loss: 0.0007\n",
      "Epoch [201/800], Loss: 0.0007\n",
      "Epoch [202/800], Loss: 0.0007\n",
      "Epoch [203/800], Loss: 0.0007\n",
      "Epoch [204/800], Loss: 0.0007\n",
      "Epoch [205/800], Loss: 0.0006\n",
      "Epoch [206/800], Loss: 0.0006\n",
      "Epoch [207/800], Loss: 0.0006\n",
      "Epoch [208/800], Loss: 0.0006\n",
      "Epoch [209/800], Loss: 0.0006\n",
      "Epoch [210/800], Loss: 0.0006\n",
      "Epoch [211/800], Loss: 0.0006\n",
      "Epoch [212/800], Loss: 0.0006\n",
      "Epoch [213/800], Loss: 0.0006\n",
      "Epoch [214/800], Loss: 0.0006\n",
      "Epoch [215/800], Loss: 0.0006\n",
      "Epoch [216/800], Loss: 0.0005\n",
      "Epoch [217/800], Loss: 0.0005\n",
      "Epoch [218/800], Loss: 0.0005\n",
      "Epoch [219/800], Loss: 0.0005\n",
      "Epoch [220/800], Loss: 0.0005\n",
      "Epoch [221/800], Loss: 0.0005\n",
      "Epoch [222/800], Loss: 0.0005\n",
      "Epoch [223/800], Loss: 0.0005\n",
      "Epoch [224/800], Loss: 0.0005\n",
      "Epoch [225/800], Loss: 0.0005\n",
      "Epoch [226/800], Loss: 0.0005\n",
      "Epoch [227/800], Loss: 0.0005\n",
      "Epoch [228/800], Loss: 0.0005\n",
      "Epoch [229/800], Loss: 0.0004\n",
      "Epoch [230/800], Loss: 0.0004\n",
      "Epoch [231/800], Loss: 0.0004\n",
      "Epoch [232/800], Loss: 0.0004\n",
      "Epoch [233/800], Loss: 0.0004\n",
      "Epoch [234/800], Loss: 0.0004\n",
      "Epoch [235/800], Loss: 0.0004\n",
      "Epoch [236/800], Loss: 0.0004\n",
      "Epoch [237/800], Loss: 0.0004\n",
      "Epoch [238/800], Loss: 0.0004\n",
      "Epoch [239/800], Loss: 0.0004\n",
      "Epoch [240/800], Loss: 0.0004\n",
      "Epoch [241/800], Loss: 0.0004\n",
      "Epoch [242/800], Loss: 0.0004\n",
      "Epoch [243/800], Loss: 0.0004\n",
      "Epoch [244/800], Loss: 0.0004\n",
      "Epoch [245/800], Loss: 0.0004\n",
      "Epoch [246/800], Loss: 0.0004\n",
      "Epoch [247/800], Loss: 0.0004\n",
      "Epoch [248/800], Loss: 0.0004\n",
      "Epoch [249/800], Loss: 0.0004\n",
      "Epoch [250/800], Loss: 0.0004\n",
      "Epoch [251/800], Loss: 0.0004\n",
      "Epoch [252/800], Loss: 0.0004\n",
      "Epoch [253/800], Loss: 0.0004\n",
      "Epoch [254/800], Loss: 0.0004\n",
      "Epoch [255/800], Loss: 0.0004\n",
      "Epoch [256/800], Loss: 0.0004\n",
      "Epoch [257/800], Loss: 0.0004\n",
      "Epoch [258/800], Loss: 0.0004\n",
      "Epoch [259/800], Loss: 0.0004\n",
      "Epoch [260/800], Loss: 0.0004\n",
      "Epoch [261/800], Loss: 0.0004\n",
      "Epoch [262/800], Loss: 0.0004\n",
      "Epoch [263/800], Loss: 0.0004\n",
      "Epoch [264/800], Loss: 0.0004\n",
      "Epoch [265/800], Loss: 0.0004\n",
      "Epoch [266/800], Loss: 0.0004\n",
      "Epoch [267/800], Loss: 0.0004\n",
      "Epoch [268/800], Loss: 0.0004\n",
      "Epoch [269/800], Loss: 0.0004\n",
      "Epoch [270/800], Loss: 0.0004\n",
      "Epoch [271/800], Loss: 0.0004\n",
      "Epoch [272/800], Loss: 0.0004\n",
      "Epoch [273/800], Loss: 0.0004\n",
      "Epoch [274/800], Loss: 0.0004\n",
      "Epoch [275/800], Loss: 0.0004\n",
      "Epoch [276/800], Loss: 0.0004\n",
      "Epoch [277/800], Loss: 0.0004\n",
      "Epoch [278/800], Loss: 0.0004\n",
      "Epoch [279/800], Loss: 0.0004\n",
      "Epoch [280/800], Loss: 0.0004\n",
      "Epoch [281/800], Loss: 0.0004\n",
      "Epoch [282/800], Loss: 0.0004\n",
      "Epoch [283/800], Loss: 0.0004\n",
      "Epoch [284/800], Loss: 0.0004\n",
      "Epoch [285/800], Loss: 0.0004\n",
      "Epoch [286/800], Loss: 0.0004\n",
      "Epoch [287/800], Loss: 0.0004\n",
      "Epoch [288/800], Loss: 0.0004\n",
      "Epoch [289/800], Loss: 0.0004\n",
      "Epoch [290/800], Loss: 0.0004\n",
      "Epoch [291/800], Loss: 0.0004\n",
      "Epoch [292/800], Loss: 0.0004\n",
      "Epoch [293/800], Loss: 0.0004\n",
      "Epoch [294/800], Loss: 0.0004\n",
      "Epoch [295/800], Loss: 0.0004\n",
      "Epoch [296/800], Loss: 0.0004\n",
      "Epoch [297/800], Loss: 0.0004\n",
      "Epoch [298/800], Loss: 0.0004\n",
      "Epoch [299/800], Loss: 0.0004\n",
      "Epoch [300/800], Loss: 0.0004\n",
      "Epoch [301/800], Loss: 0.0004\n",
      "Epoch [302/800], Loss: 0.0004\n",
      "Epoch [303/800], Loss: 0.0004\n",
      "Epoch [304/800], Loss: 0.0004\n",
      "Epoch [305/800], Loss: 0.0004\n",
      "Epoch [306/800], Loss: 0.0005\n",
      "Epoch [307/800], Loss: 0.0005\n",
      "Epoch [308/800], Loss: 0.0005\n",
      "Epoch [309/800], Loss: 0.0005\n",
      "Epoch [310/800], Loss: 0.0005\n",
      "Epoch [311/800], Loss: 0.0005\n",
      "Epoch [312/800], Loss: 0.0005\n",
      "Epoch [313/800], Loss: 0.0005\n",
      "Epoch [314/800], Loss: 0.0005\n",
      "Epoch [315/800], Loss: 0.0005\n",
      "Epoch [316/800], Loss: 0.0005\n",
      "Epoch [317/800], Loss: 0.0005\n",
      "Epoch [318/800], Loss: 0.0005\n",
      "Epoch [319/800], Loss: 0.0005\n",
      "Epoch [320/800], Loss: 0.0005\n",
      "Epoch [321/800], Loss: 0.0005\n",
      "Epoch [322/800], Loss: 0.0005\n",
      "Epoch [323/800], Loss: 0.0005\n",
      "Epoch [324/800], Loss: 0.0005\n",
      "Epoch [325/800], Loss: 0.0005\n",
      "Epoch [326/800], Loss: 0.0005\n",
      "Epoch [327/800], Loss: 0.0005\n",
      "Epoch [328/800], Loss: 0.0005\n",
      "Epoch [329/800], Loss: 0.0005\n",
      "Epoch [330/800], Loss: 0.0005\n",
      "Epoch [331/800], Loss: 0.0005\n",
      "Epoch [332/800], Loss: 0.0005\n",
      "Epoch [333/800], Loss: 0.0005\n",
      "Epoch [334/800], Loss: 0.0005\n",
      "Epoch [335/800], Loss: 0.0005\n",
      "Epoch [336/800], Loss: 0.0005\n",
      "Epoch [337/800], Loss: 0.0005\n",
      "Epoch [338/800], Loss: 0.0005\n",
      "Epoch [339/800], Loss: 0.0005\n",
      "Epoch [340/800], Loss: 0.0005\n",
      "Epoch [341/800], Loss: 0.0005\n",
      "Epoch [342/800], Loss: 0.0005\n",
      "Epoch [343/800], Loss: 0.0006\n",
      "Epoch [344/800], Loss: 0.0006\n",
      "Epoch [345/800], Loss: 0.0006\n",
      "Epoch [346/800], Loss: 0.0006\n",
      "Epoch [347/800], Loss: 0.0006\n",
      "Epoch [348/800], Loss: 0.0006\n",
      "Epoch [349/800], Loss: 0.0006\n",
      "Epoch [350/800], Loss: 0.0006\n",
      "Epoch [351/800], Loss: 0.0006\n",
      "Epoch [352/800], Loss: 0.0006\n",
      "Epoch [353/800], Loss: 0.0006\n",
      "Epoch [354/800], Loss: 0.0006\n",
      "Epoch [355/800], Loss: 0.0006\n",
      "Epoch [356/800], Loss: 0.0006\n",
      "Epoch [357/800], Loss: 0.0006\n",
      "Epoch [358/800], Loss: 0.0006\n",
      "Epoch [359/800], Loss: 0.0006\n",
      "Epoch [360/800], Loss: 0.0006\n",
      "Epoch [361/800], Loss: 0.0006\n",
      "Epoch [362/800], Loss: 0.0006\n",
      "Epoch [363/800], Loss: 0.0006\n",
      "Epoch [364/800], Loss: 0.0007\n",
      "Epoch [365/800], Loss: 0.0007\n",
      "Epoch [366/800], Loss: 0.0007\n",
      "Epoch [367/800], Loss: 0.0007\n",
      "Epoch [368/800], Loss: 0.0007\n",
      "Epoch [369/800], Loss: 0.0007\n",
      "Epoch [370/800], Loss: 0.0007\n",
      "Epoch [371/800], Loss: 0.0007\n",
      "Epoch [372/800], Loss: 0.0007\n",
      "Epoch [373/800], Loss: 0.0007\n",
      "Epoch [374/800], Loss: 0.0007\n",
      "Epoch [375/800], Loss: 0.0007\n",
      "Epoch [376/800], Loss: 0.0007\n",
      "Epoch [377/800], Loss: 0.0007\n",
      "Epoch [378/800], Loss: 0.0007\n",
      "Epoch [379/800], Loss: 0.0007\n",
      "Epoch [380/800], Loss: 0.0007\n",
      "Epoch [381/800], Loss: 0.0007\n",
      "Epoch [382/800], Loss: 0.0007\n",
      "Epoch [383/800], Loss: 0.0007\n",
      "Epoch [384/800], Loss: 0.0007\n",
      "Epoch [385/800], Loss: 0.0007\n",
      "Epoch [386/800], Loss: 0.0007\n",
      "Epoch [387/800], Loss: 0.0007\n",
      "Epoch [388/800], Loss: 0.0007\n",
      "Epoch [389/800], Loss: 0.0007\n",
      "Epoch [390/800], Loss: 0.0007\n",
      "Epoch [391/800], Loss: 0.0007\n",
      "Epoch [392/800], Loss: 0.0007\n",
      "Epoch [393/800], Loss: 0.0007\n",
      "Epoch [394/800], Loss: 0.0007\n",
      "Epoch [395/800], Loss: 0.0007\n",
      "Epoch [396/800], Loss: 0.0007\n",
      "Epoch [397/800], Loss: 0.0007\n",
      "Epoch [398/800], Loss: 0.0007\n",
      "Epoch [399/800], Loss: 0.0007\n",
      "Epoch [400/800], Loss: 0.0007\n",
      "Epoch [401/800], Loss: 0.0007\n",
      "Epoch [402/800], Loss: 0.0007\n",
      "Epoch [403/800], Loss: 0.0007\n",
      "Epoch [404/800], Loss: 0.0007\n",
      "Epoch [405/800], Loss: 0.0007\n",
      "Epoch [406/800], Loss: 0.0007\n",
      "Epoch [407/800], Loss: 0.0008\n",
      "Epoch [408/800], Loss: 0.0008\n",
      "Epoch [409/800], Loss: 0.0008\n",
      "Epoch [410/800], Loss: 0.0008\n",
      "Epoch [411/800], Loss: 0.0008\n",
      "Epoch [412/800], Loss: 0.0008\n",
      "Epoch [413/800], Loss: 0.0008\n",
      "Epoch [414/800], Loss: 0.0008\n",
      "Epoch [415/800], Loss: 0.0008\n",
      "Epoch [416/800], Loss: 0.0008\n",
      "Epoch [417/800], Loss: 0.0008\n",
      "Epoch [418/800], Loss: 0.0008\n",
      "Epoch [419/800], Loss: 0.0008\n",
      "Epoch [420/800], Loss: 0.0008\n",
      "Epoch [421/800], Loss: 0.0008\n",
      "Epoch [422/800], Loss: 0.0008\n",
      "Epoch [423/800], Loss: 0.0008\n",
      "Epoch [424/800], Loss: 0.0008\n",
      "Epoch [425/800], Loss: 0.0008\n",
      "Epoch [426/800], Loss: 0.0008\n",
      "Epoch [427/800], Loss: 0.0008\n",
      "Epoch [428/800], Loss: 0.0008\n",
      "Epoch [429/800], Loss: 0.0008\n",
      "Epoch [430/800], Loss: 0.0008\n",
      "Epoch [431/800], Loss: 0.0008\n",
      "Epoch [432/800], Loss: 0.0008\n",
      "Epoch [433/800], Loss: 0.0008\n",
      "Epoch [434/800], Loss: 0.0008\n",
      "Epoch [435/800], Loss: 0.0008\n",
      "Epoch [436/800], Loss: 0.0008\n",
      "Epoch [437/800], Loss: 0.0008\n",
      "Epoch [438/800], Loss: 0.0008\n",
      "Epoch [439/800], Loss: 0.0008\n",
      "Epoch [440/800], Loss: 0.0008\n",
      "Epoch [441/800], Loss: 0.0008\n",
      "Epoch [442/800], Loss: 0.0008\n",
      "Epoch [443/800], Loss: 0.0007\n",
      "Epoch [444/800], Loss: 0.0007\n",
      "Epoch [445/800], Loss: 0.0007\n",
      "Epoch [446/800], Loss: 0.0007\n",
      "Epoch [447/800], Loss: 0.0007\n",
      "Epoch [448/800], Loss: 0.0007\n",
      "Epoch [449/800], Loss: 0.0007\n",
      "Epoch [450/800], Loss: 0.0007\n",
      "Epoch [451/800], Loss: 0.0007\n",
      "Epoch [452/800], Loss: 0.0007\n",
      "Epoch [453/800], Loss: 0.0007\n",
      "Epoch [454/800], Loss: 0.0007\n",
      "Epoch [455/800], Loss: 0.0007\n",
      "Epoch [456/800], Loss: 0.0007\n",
      "Epoch [457/800], Loss: 0.0007\n",
      "Epoch [458/800], Loss: 0.0007\n",
      "Epoch [459/800], Loss: 0.0007\n",
      "Epoch [460/800], Loss: 0.0007\n",
      "Epoch [461/800], Loss: 0.0007\n",
      "Epoch [462/800], Loss: 0.0007\n",
      "Epoch [463/800], Loss: 0.0007\n",
      "Epoch [464/800], Loss: 0.0007\n",
      "Epoch [465/800], Loss: 0.0007\n",
      "Epoch [466/800], Loss: 0.0007\n",
      "Epoch [467/800], Loss: 0.0007\n",
      "Epoch [468/800], Loss: 0.0007\n",
      "Epoch [469/800], Loss: 0.0007\n",
      "Epoch [470/800], Loss: 0.0007\n",
      "Epoch [471/800], Loss: 0.0007\n",
      "Epoch [472/800], Loss: 0.0007\n",
      "Epoch [473/800], Loss: 0.0007\n",
      "Epoch [474/800], Loss: 0.0007\n",
      "Epoch [475/800], Loss: 0.0007\n",
      "Epoch [476/800], Loss: 0.0007\n",
      "Epoch [477/800], Loss: 0.0007\n",
      "Epoch [478/800], Loss: 0.0007\n",
      "Epoch [479/800], Loss: 0.0007\n",
      "Epoch [480/800], Loss: 0.0007\n",
      "Epoch [481/800], Loss: 0.0007\n",
      "Epoch [482/800], Loss: 0.0007\n",
      "Epoch [483/800], Loss: 0.0007\n",
      "Epoch [484/800], Loss: 0.0007\n",
      "Epoch [485/800], Loss: 0.0007\n",
      "Epoch [486/800], Loss: 0.0007\n",
      "Epoch [487/800], Loss: 0.0007\n",
      "Epoch [488/800], Loss: 0.0007\n",
      "Epoch [489/800], Loss: 0.0007\n",
      "Epoch [490/800], Loss: 0.0007\n",
      "Epoch [491/800], Loss: 0.0007\n",
      "Epoch [492/800], Loss: 0.0007\n",
      "Epoch [493/800], Loss: 0.0007\n",
      "Epoch [494/800], Loss: 0.0007\n",
      "Epoch [495/800], Loss: 0.0007\n",
      "Epoch [496/800], Loss: 0.0007\n",
      "Epoch [497/800], Loss: 0.0007\n",
      "Epoch [498/800], Loss: 0.0007\n",
      "Epoch [499/800], Loss: 0.0007\n",
      "Epoch [500/800], Loss: 0.0007\n",
      "Epoch [501/800], Loss: 0.0007\n",
      "Epoch [502/800], Loss: 0.0007\n",
      "Epoch [503/800], Loss: 0.0007\n",
      "Epoch [504/800], Loss: 0.0007\n",
      "Epoch [505/800], Loss: 0.0007\n",
      "Epoch [506/800], Loss: 0.0007\n",
      "Epoch [507/800], Loss: 0.0007\n",
      "Epoch [508/800], Loss: 0.0007\n",
      "Epoch [509/800], Loss: 0.0007\n",
      "Epoch [510/800], Loss: 0.0007\n",
      "Epoch [511/800], Loss: 0.0007\n",
      "Epoch [512/800], Loss: 0.0007\n",
      "Epoch [513/800], Loss: 0.0007\n",
      "Epoch [514/800], Loss: 0.0007\n",
      "Epoch [515/800], Loss: 0.0007\n",
      "Epoch [516/800], Loss: 0.0007\n",
      "Epoch [517/800], Loss: 0.0007\n",
      "Epoch [518/800], Loss: 0.0007\n",
      "Epoch [519/800], Loss: 0.0007\n",
      "Epoch [520/800], Loss: 0.0007\n",
      "Epoch [521/800], Loss: 0.0007\n",
      "Epoch [522/800], Loss: 0.0007\n",
      "Epoch [523/800], Loss: 0.0007\n",
      "Epoch [524/800], Loss: 0.0007\n",
      "Epoch [525/800], Loss: 0.0007\n",
      "Epoch [526/800], Loss: 0.0007\n",
      "Epoch [527/800], Loss: 0.0007\n",
      "Epoch [528/800], Loss: 0.0007\n",
      "Epoch [529/800], Loss: 0.0007\n",
      "Epoch [530/800], Loss: 0.0007\n",
      "Epoch [531/800], Loss: 0.0007\n",
      "Epoch [532/800], Loss: 0.0006\n",
      "Epoch [533/800], Loss: 0.0006\n",
      "Epoch [534/800], Loss: 0.0006\n",
      "Epoch [535/800], Loss: 0.0006\n",
      "Epoch [536/800], Loss: 0.0006\n",
      "Epoch [537/800], Loss: 0.0006\n",
      "Epoch [538/800], Loss: 0.0006\n",
      "Epoch [539/800], Loss: 0.0007\n",
      "Epoch [540/800], Loss: 0.0006\n",
      "Epoch [541/800], Loss: 0.0007\n",
      "Epoch [542/800], Loss: 0.0006\n",
      "Epoch [543/800], Loss: 0.0007\n",
      "Epoch [544/800], Loss: 0.0007\n",
      "Epoch [545/800], Loss: 0.0007\n",
      "Epoch [546/800], Loss: 0.0006\n",
      "Epoch [547/800], Loss: 0.0006\n",
      "Epoch [548/800], Loss: 0.0006\n",
      "Epoch [549/800], Loss: 0.0006\n",
      "Epoch [550/800], Loss: 0.0006\n",
      "Epoch [551/800], Loss: 0.0006\n",
      "Epoch [552/800], Loss: 0.0006\n",
      "Epoch [553/800], Loss: 0.0006\n",
      "Epoch [554/800], Loss: 0.0006\n",
      "Epoch [555/800], Loss: 0.0006\n",
      "Epoch [556/800], Loss: 0.0006\n",
      "Epoch [557/800], Loss: 0.0005\n",
      "Epoch [558/800], Loss: 0.0005\n",
      "Epoch [559/800], Loss: 0.0005\n",
      "Epoch [560/800], Loss: 0.0005\n",
      "Epoch [561/800], Loss: 0.0005\n",
      "Epoch [562/800], Loss: 0.0005\n",
      "Epoch [563/800], Loss: 0.0005\n",
      "Epoch [564/800], Loss: 0.0005\n",
      "Epoch [565/800], Loss: 0.0005\n",
      "Epoch [566/800], Loss: 0.0005\n",
      "Epoch [567/800], Loss: 0.0005\n",
      "Epoch [568/800], Loss: 0.0005\n",
      "Epoch [569/800], Loss: 0.0005\n",
      "Epoch [570/800], Loss: 0.0005\n",
      "Epoch [571/800], Loss: 0.0005\n",
      "Epoch [572/800], Loss: 0.0005\n",
      "Epoch [573/800], Loss: 0.0005\n",
      "Epoch [574/800], Loss: 0.0005\n",
      "Epoch [575/800], Loss: 0.0005\n",
      "Epoch [576/800], Loss: 0.0005\n",
      "Epoch [577/800], Loss: 0.0005\n",
      "Epoch [578/800], Loss: 0.0005\n",
      "Epoch [579/800], Loss: 0.0005\n",
      "Epoch [580/800], Loss: 0.0005\n",
      "Epoch [581/800], Loss: 0.0005\n",
      "Epoch [582/800], Loss: 0.0005\n",
      "Epoch [583/800], Loss: 0.0005\n",
      "Epoch [584/800], Loss: 0.0005\n",
      "Epoch [585/800], Loss: 0.0005\n",
      "Epoch [586/800], Loss: 0.0005\n",
      "Epoch [587/800], Loss: 0.0005\n",
      "Epoch [588/800], Loss: 0.0005\n",
      "Epoch [589/800], Loss: 0.0004\n",
      "Epoch [590/800], Loss: 0.0004\n",
      "Epoch [591/800], Loss: 0.0004\n",
      "Epoch [592/800], Loss: 0.0004\n",
      "Epoch [593/800], Loss: 0.0004\n",
      "Epoch [594/800], Loss: 0.0004\n",
      "Epoch [595/800], Loss: 0.0004\n",
      "Epoch [596/800], Loss: 0.0004\n",
      "Epoch [597/800], Loss: 0.0004\n",
      "Epoch [598/800], Loss: 0.0004\n",
      "Epoch [599/800], Loss: 0.0004\n",
      "Epoch [600/800], Loss: 0.0004\n",
      "Epoch [601/800], Loss: 0.0004\n",
      "Epoch [602/800], Loss: 0.0004\n",
      "Epoch [603/800], Loss: 0.0004\n",
      "Epoch [604/800], Loss: 0.0004\n",
      "Epoch [605/800], Loss: 0.0004\n",
      "Epoch [606/800], Loss: 0.0004\n",
      "Epoch [607/800], Loss: 0.0004\n",
      "Epoch [608/800], Loss: 0.0004\n",
      "Epoch [609/800], Loss: 0.0004\n",
      "Epoch [610/800], Loss: 0.0004\n",
      "Epoch [611/800], Loss: 0.0004\n",
      "Epoch [612/800], Loss: 0.0004\n",
      "Epoch [613/800], Loss: 0.0004\n",
      "Epoch [614/800], Loss: 0.0004\n",
      "Epoch [615/800], Loss: 0.0004\n",
      "Epoch [616/800], Loss: 0.0004\n",
      "Epoch [617/800], Loss: 0.0004\n",
      "Epoch [618/800], Loss: 0.0004\n",
      "Epoch [619/800], Loss: 0.0004\n",
      "Epoch [620/800], Loss: 0.0003\n",
      "Epoch [621/800], Loss: 0.0004\n",
      "Epoch [622/800], Loss: 0.0003\n",
      "Epoch [623/800], Loss: 0.0003\n",
      "Epoch [624/800], Loss: 0.0003\n",
      "Epoch [625/800], Loss: 0.0003\n",
      "Epoch [626/800], Loss: 0.0003\n",
      "Epoch [627/800], Loss: 0.0003\n",
      "Epoch [628/800], Loss: 0.0003\n",
      "Epoch [629/800], Loss: 0.0003\n",
      "Epoch [630/800], Loss: 0.0003\n",
      "Epoch [631/800], Loss: 0.0003\n",
      "Epoch [632/800], Loss: 0.0003\n",
      "Epoch [633/800], Loss: 0.0003\n",
      "Epoch [634/800], Loss: 0.0003\n",
      "Epoch [635/800], Loss: 0.0003\n",
      "Epoch [636/800], Loss: 0.0003\n",
      "Epoch [637/800], Loss: 0.0003\n",
      "Epoch [638/800], Loss: 0.0003\n",
      "Epoch [639/800], Loss: 0.0003\n",
      "Epoch [640/800], Loss: 0.0003\n",
      "Epoch [641/800], Loss: 0.0003\n",
      "Epoch [642/800], Loss: 0.0003\n",
      "Epoch [643/800], Loss: 0.0003\n",
      "Epoch [644/800], Loss: 0.0003\n",
      "Epoch [645/800], Loss: 0.0003\n",
      "Epoch [646/800], Loss: 0.0003\n",
      "Epoch [647/800], Loss: 0.0003\n",
      "Epoch [648/800], Loss: 0.0003\n",
      "Epoch [649/800], Loss: 0.0003\n",
      "Epoch [650/800], Loss: 0.0003\n",
      "Epoch [651/800], Loss: 0.0003\n",
      "Epoch [652/800], Loss: 0.0003\n",
      "Epoch [653/800], Loss: 0.0003\n",
      "Epoch [654/800], Loss: 0.0003\n",
      "Epoch [655/800], Loss: 0.0003\n",
      "Epoch [656/800], Loss: 0.0003\n",
      "Epoch [657/800], Loss: 0.0003\n",
      "Epoch [658/800], Loss: 0.0003\n",
      "Epoch [659/800], Loss: 0.0003\n",
      "Epoch [660/800], Loss: 0.0003\n",
      "Epoch [661/800], Loss: 0.0003\n",
      "Epoch [662/800], Loss: 0.0003\n",
      "Epoch [663/800], Loss: 0.0003\n",
      "Epoch [664/800], Loss: 0.0003\n",
      "Epoch [665/800], Loss: 0.0003\n",
      "Epoch [666/800], Loss: 0.0003\n",
      "Epoch [667/800], Loss: 0.0003\n",
      "Epoch [668/800], Loss: 0.0003\n",
      "Epoch [669/800], Loss: 0.0003\n",
      "Epoch [670/800], Loss: 0.0003\n",
      "Epoch [671/800], Loss: 0.0003\n",
      "Epoch [672/800], Loss: 0.0003\n",
      "Epoch [673/800], Loss: 0.0003\n",
      "Epoch [674/800], Loss: 0.0003\n",
      "Epoch [675/800], Loss: 0.0003\n",
      "Epoch [676/800], Loss: 0.0003\n",
      "Epoch [677/800], Loss: 0.0003\n",
      "Epoch [678/800], Loss: 0.0003\n",
      "Epoch [679/800], Loss: 0.0003\n",
      "Epoch [680/800], Loss: 0.0003\n",
      "Epoch [681/800], Loss: 0.0003\n",
      "Epoch [682/800], Loss: 0.0003\n",
      "Epoch [683/800], Loss: 0.0003\n",
      "Epoch [684/800], Loss: 0.0003\n",
      "Epoch [685/800], Loss: 0.0003\n",
      "Epoch [686/800], Loss: 0.0003\n",
      "Epoch [687/800], Loss: 0.0003\n",
      "Epoch [688/800], Loss: 0.0003\n",
      "Epoch [689/800], Loss: 0.0003\n",
      "Epoch [690/800], Loss: 0.0003\n",
      "Epoch [691/800], Loss: 0.0003\n",
      "Epoch [692/800], Loss: 0.0003\n",
      "Epoch [693/800], Loss: 0.0003\n",
      "Epoch [694/800], Loss: 0.0003\n",
      "Epoch [695/800], Loss: 0.0003\n",
      "Epoch [696/800], Loss: 0.0003\n",
      "Epoch [697/800], Loss: 0.0003\n",
      "Epoch [698/800], Loss: 0.0003\n",
      "Epoch [699/800], Loss: 0.0003\n",
      "Epoch [700/800], Loss: 0.0003\n",
      "Epoch [701/800], Loss: 0.0003\n",
      "Epoch [702/800], Loss: 0.0003\n",
      "Epoch [703/800], Loss: 0.0003\n",
      "Epoch [704/800], Loss: 0.0003\n",
      "Epoch [705/800], Loss: 0.0003\n",
      "Epoch [706/800], Loss: 0.0003\n",
      "Epoch [707/800], Loss: 0.0003\n",
      "Epoch [708/800], Loss: 0.0003\n",
      "Epoch [709/800], Loss: 0.0003\n",
      "Epoch [710/800], Loss: 0.0003\n",
      "Epoch [711/800], Loss: 0.0003\n",
      "Epoch [712/800], Loss: 0.0002\n",
      "Epoch [713/800], Loss: 0.0002\n",
      "Epoch [714/800], Loss: 0.0002\n",
      "Epoch [715/800], Loss: 0.0002\n",
      "Epoch [716/800], Loss: 0.0002\n",
      "Epoch [717/800], Loss: 0.0002\n",
      "Epoch [718/800], Loss: 0.0002\n",
      "Epoch [719/800], Loss: 0.0002\n",
      "Epoch [720/800], Loss: 0.0002\n",
      "Epoch [721/800], Loss: 0.0002\n",
      "Epoch [722/800], Loss: 0.0002\n",
      "Epoch [723/800], Loss: 0.0002\n",
      "Epoch [724/800], Loss: 0.0002\n",
      "Epoch [725/800], Loss: 0.0002\n",
      "Epoch [726/800], Loss: 0.0002\n",
      "Epoch [727/800], Loss: 0.0002\n",
      "Epoch [728/800], Loss: 0.0002\n",
      "Epoch [729/800], Loss: 0.0002\n",
      "Epoch [730/800], Loss: 0.0002\n",
      "Epoch [731/800], Loss: 0.0002\n",
      "Epoch [732/800], Loss: 0.0002\n",
      "Epoch [733/800], Loss: 0.0002\n",
      "Epoch [734/800], Loss: 0.0002\n",
      "Epoch [735/800], Loss: 0.0002\n",
      "Epoch [736/800], Loss: 0.0002\n",
      "Epoch [737/800], Loss: 0.0002\n",
      "Epoch [738/800], Loss: 0.0002\n",
      "Epoch [739/800], Loss: 0.0002\n",
      "Epoch [740/800], Loss: 0.0002\n",
      "Epoch [741/800], Loss: 0.0002\n",
      "Epoch [742/800], Loss: 0.0002\n",
      "Epoch [743/800], Loss: 0.0002\n",
      "Epoch [744/800], Loss: 0.0002\n",
      "Epoch [745/800], Loss: 0.0002\n",
      "Epoch [746/800], Loss: 0.0002\n",
      "Epoch [747/800], Loss: 0.0002\n",
      "Epoch [748/800], Loss: 0.0002\n",
      "Epoch [749/800], Loss: 0.0002\n",
      "Epoch [750/800], Loss: 0.0002\n",
      "Epoch [751/800], Loss: 0.0002\n",
      "Epoch [752/800], Loss: 0.0002\n",
      "Epoch [753/800], Loss: 0.0002\n",
      "Epoch [754/800], Loss: 0.0002\n",
      "Epoch [755/800], Loss: 0.0002\n",
      "Epoch [756/800], Loss: 0.0002\n",
      "Epoch [757/800], Loss: 0.0002\n",
      "Epoch [758/800], Loss: 0.0002\n",
      "Epoch [759/800], Loss: 0.0002\n",
      "Epoch [760/800], Loss: 0.0002\n",
      "Epoch [761/800], Loss: 0.0002\n",
      "Epoch [762/800], Loss: 0.0002\n",
      "Epoch [763/800], Loss: 0.0002\n",
      "Epoch [764/800], Loss: 0.0002\n",
      "Epoch [765/800], Loss: 0.0002\n",
      "Epoch [766/800], Loss: 0.0002\n",
      "Epoch [767/800], Loss: 0.0002\n",
      "Epoch [768/800], Loss: 0.0002\n",
      "Epoch [769/800], Loss: 0.0002\n",
      "Epoch [770/800], Loss: 0.0002\n",
      "Epoch [771/800], Loss: 0.0002\n",
      "Epoch [772/800], Loss: 0.0002\n",
      "Epoch [773/800], Loss: 0.0002\n",
      "Epoch [774/800], Loss: 0.0002\n",
      "Epoch [775/800], Loss: 0.0002\n",
      "Epoch [776/800], Loss: 0.0002\n",
      "Epoch [777/800], Loss: 0.0002\n",
      "Epoch [778/800], Loss: 0.0002\n",
      "Epoch [779/800], Loss: 0.0002\n",
      "Epoch [780/800], Loss: 0.0002\n",
      "Epoch [781/800], Loss: 0.0002\n",
      "Epoch [782/800], Loss: 0.0002\n",
      "Epoch [783/800], Loss: 0.0002\n",
      "Epoch [784/800], Loss: 0.0002\n",
      "Epoch [785/800], Loss: 0.0002\n",
      "Epoch [786/800], Loss: 0.0002\n",
      "Epoch [787/800], Loss: 0.0002\n",
      "Epoch [788/800], Loss: 0.0002\n",
      "Epoch [789/800], Loss: 0.0002\n",
      "Epoch [790/800], Loss: 0.0002\n",
      "Epoch [791/800], Loss: 0.0002\n",
      "Epoch [792/800], Loss: 0.0002\n",
      "Epoch [793/800], Loss: 0.0002\n",
      "Epoch [794/800], Loss: 0.0002\n",
      "Epoch [795/800], Loss: 0.0002\n",
      "Epoch [796/800], Loss: 0.0002\n",
      "Epoch [797/800], Loss: 0.0002\n",
      "Epoch [798/800], Loss: 0.0002\n",
      "Epoch [799/800], Loss: 0.0002\n",
      "Epoch [800/800], Loss: 0.0002\n",
      "Accuracy: 50.7937%\n",
      "Epoch [1/800], Loss: 0.0095\n",
      "Epoch [2/800], Loss: 0.0122\n",
      "Epoch [3/800], Loss: 0.0141\n",
      "Epoch [4/800], Loss: 0.0163\n",
      "Epoch [5/800], Loss: 0.0199\n",
      "Epoch [6/800], Loss: 0.0259\n",
      "Epoch [7/800], Loss: 0.0350\n",
      "Epoch [8/800], Loss: 0.0460\n",
      "Epoch [9/800], Loss: 0.0573\n",
      "Epoch [10/800], Loss: 0.0682\n",
      "Epoch [11/800], Loss: 0.0778\n",
      "Epoch [12/800], Loss: 0.0861\n",
      "Epoch [13/800], Loss: 0.0931\n",
      "Epoch [14/800], Loss: 0.0989\n",
      "Epoch [15/800], Loss: 0.1033\n",
      "Epoch [16/800], Loss: 0.1065\n",
      "Epoch [17/800], Loss: 0.1083\n",
      "Epoch [18/800], Loss: 0.1088\n",
      "Epoch [19/800], Loss: 0.1079\n",
      "Epoch [20/800], Loss: 0.1058\n",
      "Epoch [21/800], Loss: 0.1026\n",
      "Epoch [22/800], Loss: 0.0982\n",
      "Epoch [23/800], Loss: 0.0925\n",
      "Epoch [24/800], Loss: 0.0855\n",
      "Epoch [25/800], Loss: 0.0777\n",
      "Epoch [26/800], Loss: 0.0693\n",
      "Epoch [27/800], Loss: 0.0603\n",
      "Epoch [28/800], Loss: 0.0511\n",
      "Epoch [29/800], Loss: 0.0420\n",
      "Epoch [30/800], Loss: 0.0352\n",
      "Epoch [31/800], Loss: 0.0284\n",
      "Epoch [32/800], Loss: 0.0238\n",
      "Epoch [33/800], Loss: 0.0198\n",
      "Epoch [34/800], Loss: 0.0169\n",
      "Epoch [35/800], Loss: 0.0148\n",
      "Epoch [36/800], Loss: 0.0131\n",
      "Epoch [37/800], Loss: 0.0118\n",
      "Epoch [38/800], Loss: 0.0108\n",
      "Epoch [39/800], Loss: 0.0100\n",
      "Epoch [40/800], Loss: 0.0093\n",
      "Epoch [41/800], Loss: 0.0088\n",
      "Epoch [42/800], Loss: 0.0083\n",
      "Epoch [43/800], Loss: 0.0079\n",
      "Epoch [44/800], Loss: 0.0076\n",
      "Epoch [45/800], Loss: 0.0073\n",
      "Epoch [46/800], Loss: 0.0071\n",
      "Epoch [47/800], Loss: 0.0070\n",
      "Epoch [48/800], Loss: 0.0068\n",
      "Epoch [49/800], Loss: 0.0068\n",
      "Epoch [50/800], Loss: 0.0067\n",
      "Epoch [51/800], Loss: 0.0067\n",
      "Epoch [52/800], Loss: 0.0067\n",
      "Epoch [53/800], Loss: 0.0067\n",
      "Epoch [54/800], Loss: 0.0067\n",
      "Epoch [55/800], Loss: 0.0068\n",
      "Epoch [56/800], Loss: 0.0068\n",
      "Epoch [57/800], Loss: 0.0068\n",
      "Epoch [58/800], Loss: 0.0068\n",
      "Epoch [59/800], Loss: 0.0069\n",
      "Epoch [60/800], Loss: 0.0069\n",
      "Epoch [61/800], Loss: 0.0069\n",
      "Epoch [62/800], Loss: 0.0069\n",
      "Epoch [63/800], Loss: 0.0069\n",
      "Epoch [64/800], Loss: 0.0069\n",
      "Epoch [65/800], Loss: 0.0069\n",
      "Epoch [66/800], Loss: 0.0069\n",
      "Epoch [67/800], Loss: 0.0068\n",
      "Epoch [68/800], Loss: 0.0068\n",
      "Epoch [69/800], Loss: 0.0068\n",
      "Epoch [70/800], Loss: 0.0067\n",
      "Epoch [71/800], Loss: 0.0067\n",
      "Epoch [72/800], Loss: 0.0067\n",
      "Epoch [73/800], Loss: 0.0066\n",
      "Epoch [74/800], Loss: 0.0065\n",
      "Epoch [75/800], Loss: 0.0065\n",
      "Epoch [76/800], Loss: 0.0064\n",
      "Epoch [77/800], Loss: 0.0063\n",
      "Epoch [78/800], Loss: 0.0062\n",
      "Epoch [79/800], Loss: 0.0062\n",
      "Epoch [80/800], Loss: 0.0061\n",
      "Epoch [81/800], Loss: 0.0060\n",
      "Epoch [82/800], Loss: 0.0059\n",
      "Epoch [83/800], Loss: 0.0058\n",
      "Epoch [84/800], Loss: 0.0057\n",
      "Epoch [85/800], Loss: 0.0056\n",
      "Epoch [86/800], Loss: 0.0055\n",
      "Epoch [87/800], Loss: 0.0054\n",
      "Epoch [88/800], Loss: 0.0053\n",
      "Epoch [89/800], Loss: 0.0053\n",
      "Epoch [90/800], Loss: 0.0052\n",
      "Epoch [91/800], Loss: 0.0051\n",
      "Epoch [92/800], Loss: 0.0049\n",
      "Epoch [93/800], Loss: 0.0048\n",
      "Epoch [94/800], Loss: 0.0047\n",
      "Epoch [95/800], Loss: 0.0046\n",
      "Epoch [96/800], Loss: 0.0045\n",
      "Epoch [97/800], Loss: 0.0044\n",
      "Epoch [98/800], Loss: 0.0043\n",
      "Epoch [99/800], Loss: 0.0042\n",
      "Epoch [100/800], Loss: 0.0041\n",
      "Epoch [101/800], Loss: 0.0040\n",
      "Epoch [102/800], Loss: 0.0039\n",
      "Epoch [103/800], Loss: 0.0038\n",
      "Epoch [104/800], Loss: 0.0037\n",
      "Epoch [105/800], Loss: 0.0036\n",
      "Epoch [106/800], Loss: 0.0035\n",
      "Epoch [107/800], Loss: 0.0034\n",
      "Epoch [108/800], Loss: 0.0034\n",
      "Epoch [109/800], Loss: 0.0033\n",
      "Epoch [110/800], Loss: 0.0032\n",
      "Epoch [111/800], Loss: 0.0031\n",
      "Epoch [112/800], Loss: 0.0031\n",
      "Epoch [113/800], Loss: 0.0030\n",
      "Epoch [114/800], Loss: 0.0029\n",
      "Epoch [115/800], Loss: 0.0028\n",
      "Epoch [116/800], Loss: 0.0028\n",
      "Epoch [117/800], Loss: 0.0027\n",
      "Epoch [118/800], Loss: 0.0027\n",
      "Epoch [119/800], Loss: 0.0026\n",
      "Epoch [120/800], Loss: 0.0025\n",
      "Epoch [121/800], Loss: 0.0025\n",
      "Epoch [122/800], Loss: 0.0024\n",
      "Epoch [123/800], Loss: 0.0024\n",
      "Epoch [124/800], Loss: 0.0023\n",
      "Epoch [125/800], Loss: 0.0023\n",
      "Epoch [126/800], Loss: 0.0023\n",
      "Epoch [127/800], Loss: 0.0022\n",
      "Epoch [128/800], Loss: 0.0022\n",
      "Epoch [129/800], Loss: 0.0022\n",
      "Epoch [130/800], Loss: 0.0022\n",
      "Epoch [131/800], Loss: 0.0022\n",
      "Epoch [132/800], Loss: 0.0021\n",
      "Epoch [133/800], Loss: 0.0021\n",
      "Epoch [134/800], Loss: 0.0021\n",
      "Epoch [135/800], Loss: 0.0021\n",
      "Epoch [136/800], Loss: 0.0021\n",
      "Epoch [137/800], Loss: 0.0021\n",
      "Epoch [138/800], Loss: 0.0021\n",
      "Epoch [139/800], Loss: 0.0021\n",
      "Epoch [140/800], Loss: 0.0021\n",
      "Epoch [141/800], Loss: 0.0021\n",
      "Epoch [142/800], Loss: 0.0021\n",
      "Epoch [143/800], Loss: 0.0021\n",
      "Epoch [144/800], Loss: 0.0021\n",
      "Epoch [145/800], Loss: 0.0021\n",
      "Epoch [146/800], Loss: 0.0021\n",
      "Epoch [147/800], Loss: 0.0021\n",
      "Epoch [148/800], Loss: 0.0020\n",
      "Epoch [149/800], Loss: 0.0020\n",
      "Epoch [150/800], Loss: 0.0020\n",
      "Epoch [151/800], Loss: 0.0020\n",
      "Epoch [152/800], Loss: 0.0020\n",
      "Epoch [153/800], Loss: 0.0020\n",
      "Epoch [154/800], Loss: 0.0020\n",
      "Epoch [155/800], Loss: 0.0020\n",
      "Epoch [156/800], Loss: 0.0020\n",
      "Epoch [157/800], Loss: 0.0020\n",
      "Epoch [158/800], Loss: 0.0020\n",
      "Epoch [159/800], Loss: 0.0020\n",
      "Epoch [160/800], Loss: 0.0020\n",
      "Epoch [161/800], Loss: 0.0020\n",
      "Epoch [162/800], Loss: 0.0020\n",
      "Epoch [163/800], Loss: 0.0020\n",
      "Epoch [164/800], Loss: 0.0020\n",
      "Epoch [165/800], Loss: 0.0020\n",
      "Epoch [166/800], Loss: 0.0020\n",
      "Epoch [167/800], Loss: 0.0020\n",
      "Epoch [168/800], Loss: 0.0021\n",
      "Epoch [169/800], Loss: 0.0021\n",
      "Epoch [170/800], Loss: 0.0021\n",
      "Epoch [171/800], Loss: 0.0021\n",
      "Epoch [172/800], Loss: 0.0021\n",
      "Epoch [173/800], Loss: 0.0021\n",
      "Epoch [174/800], Loss: 0.0021\n",
      "Epoch [175/800], Loss: 0.0021\n",
      "Epoch [176/800], Loss: 0.0021\n",
      "Epoch [177/800], Loss: 0.0021\n",
      "Epoch [178/800], Loss: 0.0021\n",
      "Epoch [179/800], Loss: 0.0021\n",
      "Epoch [180/800], Loss: 0.0021\n",
      "Epoch [181/800], Loss: 0.0021\n",
      "Epoch [182/800], Loss: 0.0021\n",
      "Epoch [183/800], Loss: 0.0021\n",
      "Epoch [184/800], Loss: 0.0021\n",
      "Epoch [185/800], Loss: 0.0021\n",
      "Epoch [186/800], Loss: 0.0021\n",
      "Epoch [187/800], Loss: 0.0021\n",
      "Epoch [188/800], Loss: 0.0021\n",
      "Epoch [189/800], Loss: 0.0021\n",
      "Epoch [190/800], Loss: 0.0020\n",
      "Epoch [191/800], Loss: 0.0020\n",
      "Epoch [192/800], Loss: 0.0020\n",
      "Epoch [193/800], Loss: 0.0020\n",
      "Epoch [194/800], Loss: 0.0020\n",
      "Epoch [195/800], Loss: 0.0020\n",
      "Epoch [196/800], Loss: 0.0020\n",
      "Epoch [197/800], Loss: 0.0020\n",
      "Epoch [198/800], Loss: 0.0020\n",
      "Epoch [199/800], Loss: 0.0020\n",
      "Epoch [200/800], Loss: 0.0020\n",
      "Epoch [201/800], Loss: 0.0020\n",
      "Epoch [202/800], Loss: 0.0020\n",
      "Epoch [203/800], Loss: 0.0020\n",
      "Epoch [204/800], Loss: 0.0020\n",
      "Epoch [205/800], Loss: 0.0020\n",
      "Epoch [206/800], Loss: 0.0020\n",
      "Epoch [207/800], Loss: 0.0020\n",
      "Epoch [208/800], Loss: 0.0020\n",
      "Epoch [209/800], Loss: 0.0019\n",
      "Epoch [210/800], Loss: 0.0019\n",
      "Epoch [211/800], Loss: 0.0019\n",
      "Epoch [212/800], Loss: 0.0019\n",
      "Epoch [213/800], Loss: 0.0018\n",
      "Epoch [214/800], Loss: 0.0018\n",
      "Epoch [215/800], Loss: 0.0018\n",
      "Epoch [216/800], Loss: 0.0018\n",
      "Epoch [217/800], Loss: 0.0018\n",
      "Epoch [218/800], Loss: 0.0017\n",
      "Epoch [219/800], Loss: 0.0017\n",
      "Epoch [220/800], Loss: 0.0017\n",
      "Epoch [221/800], Loss: 0.0017\n",
      "Epoch [222/800], Loss: 0.0017\n",
      "Epoch [223/800], Loss: 0.0017\n",
      "Epoch [224/800], Loss: 0.0017\n",
      "Epoch [225/800], Loss: 0.0017\n",
      "Epoch [226/800], Loss: 0.0017\n",
      "Epoch [227/800], Loss: 0.0016\n",
      "Epoch [228/800], Loss: 0.0016\n",
      "Epoch [229/800], Loss: 0.0016\n",
      "Epoch [230/800], Loss: 0.0016\n",
      "Epoch [231/800], Loss: 0.0016\n",
      "Epoch [232/800], Loss: 0.0016\n",
      "Epoch [233/800], Loss: 0.0016\n",
      "Epoch [234/800], Loss: 0.0016\n",
      "Epoch [235/800], Loss: 0.0016\n",
      "Epoch [236/800], Loss: 0.0016\n",
      "Epoch [237/800], Loss: 0.0016\n",
      "Epoch [238/800], Loss: 0.0016\n",
      "Epoch [239/800], Loss: 0.0016\n",
      "Epoch [240/800], Loss: 0.0016\n",
      "Epoch [241/800], Loss: 0.0016\n",
      "Epoch [242/800], Loss: 0.0016\n",
      "Epoch [243/800], Loss: 0.0016\n",
      "Epoch [244/800], Loss: 0.0016\n",
      "Epoch [245/800], Loss: 0.0016\n",
      "Epoch [246/800], Loss: 0.0016\n",
      "Epoch [247/800], Loss: 0.0016\n",
      "Epoch [248/800], Loss: 0.0016\n",
      "Epoch [249/800], Loss: 0.0016\n",
      "Epoch [250/800], Loss: 0.0016\n",
      "Epoch [251/800], Loss: 0.0016\n",
      "Epoch [252/800], Loss: 0.0016\n",
      "Epoch [253/800], Loss: 0.0016\n",
      "Epoch [254/800], Loss: 0.0016\n",
      "Epoch [255/800], Loss: 0.0015\n",
      "Epoch [256/800], Loss: 0.0015\n",
      "Epoch [257/800], Loss: 0.0015\n",
      "Epoch [258/800], Loss: 0.0015\n",
      "Epoch [259/800], Loss: 0.0015\n",
      "Epoch [260/800], Loss: 0.0015\n",
      "Epoch [261/800], Loss: 0.0015\n",
      "Epoch [262/800], Loss: 0.0015\n",
      "Epoch [263/800], Loss: 0.0015\n",
      "Epoch [264/800], Loss: 0.0015\n",
      "Epoch [265/800], Loss: 0.0014\n",
      "Epoch [266/800], Loss: 0.0014\n",
      "Epoch [267/800], Loss: 0.0014\n",
      "Epoch [268/800], Loss: 0.0014\n",
      "Epoch [269/800], Loss: 0.0014\n",
      "Epoch [270/800], Loss: 0.0014\n",
      "Epoch [271/800], Loss: 0.0014\n",
      "Epoch [272/800], Loss: 0.0013\n",
      "Epoch [273/800], Loss: 0.0013\n",
      "Epoch [274/800], Loss: 0.0013\n",
      "Epoch [275/800], Loss: 0.0013\n",
      "Epoch [276/800], Loss: 0.0013\n",
      "Epoch [277/800], Loss: 0.0013\n",
      "Epoch [278/800], Loss: 0.0013\n",
      "Epoch [279/800], Loss: 0.0013\n",
      "Epoch [280/800], Loss: 0.0012\n",
      "Epoch [281/800], Loss: 0.0012\n",
      "Epoch [282/800], Loss: 0.0012\n",
      "Epoch [283/800], Loss: 0.0012\n",
      "Epoch [284/800], Loss: 0.0012\n",
      "Epoch [285/800], Loss: 0.0012\n",
      "Epoch [286/800], Loss: 0.0011\n",
      "Epoch [287/800], Loss: 0.0011\n",
      "Epoch [288/800], Loss: 0.0011\n",
      "Epoch [289/800], Loss: 0.0011\n",
      "Epoch [290/800], Loss: 0.0011\n",
      "Epoch [291/800], Loss: 0.0011\n",
      "Epoch [292/800], Loss: 0.0011\n",
      "Epoch [293/800], Loss: 0.0011\n",
      "Epoch [294/800], Loss: 0.0011\n",
      "Epoch [295/800], Loss: 0.0010\n",
      "Epoch [296/800], Loss: 0.0010\n",
      "Epoch [297/800], Loss: 0.0010\n",
      "Epoch [298/800], Loss: 0.0010\n",
      "Epoch [299/800], Loss: 0.0010\n",
      "Epoch [300/800], Loss: 0.0010\n",
      "Epoch [301/800], Loss: 0.0010\n",
      "Epoch [302/800], Loss: 0.0009\n",
      "Epoch [303/800], Loss: 0.0009\n",
      "Epoch [304/800], Loss: 0.0009\n",
      "Epoch [305/800], Loss: 0.0009\n",
      "Epoch [306/800], Loss: 0.0009\n",
      "Epoch [307/800], Loss: 0.0009\n",
      "Epoch [308/800], Loss: 0.0009\n",
      "Epoch [309/800], Loss: 0.0009\n",
      "Epoch [310/800], Loss: 0.0009\n",
      "Epoch [311/800], Loss: 0.0009\n",
      "Epoch [312/800], Loss: 0.0009\n",
      "Epoch [313/800], Loss: 0.0009\n",
      "Epoch [314/800], Loss: 0.0009\n",
      "Epoch [315/800], Loss: 0.0009\n",
      "Epoch [316/800], Loss: 0.0009\n",
      "Epoch [317/800], Loss: 0.0008\n",
      "Epoch [318/800], Loss: 0.0008\n",
      "Epoch [319/800], Loss: 0.0008\n",
      "Epoch [320/800], Loss: 0.0008\n",
      "Epoch [321/800], Loss: 0.0008\n",
      "Epoch [322/800], Loss: 0.0008\n",
      "Epoch [323/800], Loss: 0.0008\n",
      "Epoch [324/800], Loss: 0.0008\n",
      "Epoch [325/800], Loss: 0.0008\n",
      "Epoch [326/800], Loss: 0.0008\n",
      "Epoch [327/800], Loss: 0.0008\n",
      "Epoch [328/800], Loss: 0.0008\n",
      "Epoch [329/800], Loss: 0.0008\n",
      "Epoch [330/800], Loss: 0.0008\n",
      "Epoch [331/800], Loss: 0.0008\n",
      "Epoch [332/800], Loss: 0.0008\n",
      "Epoch [333/800], Loss: 0.0008\n",
      "Epoch [334/800], Loss: 0.0008\n",
      "Epoch [335/800], Loss: 0.0008\n",
      "Epoch [336/800], Loss: 0.0008\n",
      "Epoch [337/800], Loss: 0.0008\n",
      "Epoch [338/800], Loss: 0.0008\n",
      "Epoch [339/800], Loss: 0.0008\n",
      "Epoch [340/800], Loss: 0.0008\n",
      "Epoch [341/800], Loss: 0.0008\n",
      "Epoch [342/800], Loss: 0.0008\n",
      "Epoch [343/800], Loss: 0.0008\n",
      "Epoch [344/800], Loss: 0.0008\n",
      "Epoch [345/800], Loss: 0.0008\n",
      "Epoch [346/800], Loss: 0.0008\n",
      "Epoch [347/800], Loss: 0.0008\n",
      "Epoch [348/800], Loss: 0.0008\n",
      "Epoch [349/800], Loss: 0.0008\n",
      "Epoch [350/800], Loss: 0.0008\n",
      "Epoch [351/800], Loss: 0.0008\n",
      "Epoch [352/800], Loss: 0.0008\n",
      "Epoch [353/800], Loss: 0.0008\n",
      "Epoch [354/800], Loss: 0.0007\n",
      "Epoch [355/800], Loss: 0.0007\n",
      "Epoch [356/800], Loss: 0.0007\n",
      "Epoch [357/800], Loss: 0.0007\n",
      "Epoch [358/800], Loss: 0.0007\n",
      "Epoch [359/800], Loss: 0.0007\n",
      "Epoch [360/800], Loss: 0.0007\n",
      "Epoch [361/800], Loss: 0.0007\n",
      "Epoch [362/800], Loss: 0.0007\n",
      "Epoch [363/800], Loss: 0.0007\n",
      "Epoch [364/800], Loss: 0.0007\n",
      "Epoch [365/800], Loss: 0.0007\n",
      "Epoch [366/800], Loss: 0.0007\n",
      "Epoch [367/800], Loss: 0.0006\n",
      "Epoch [368/800], Loss: 0.0006\n",
      "Epoch [369/800], Loss: 0.0006\n",
      "Epoch [370/800], Loss: 0.0006\n",
      "Epoch [371/800], Loss: 0.0006\n",
      "Epoch [372/800], Loss: 0.0006\n",
      "Epoch [373/800], Loss: 0.0006\n",
      "Epoch [374/800], Loss: 0.0006\n",
      "Epoch [375/800], Loss: 0.0006\n",
      "Epoch [376/800], Loss: 0.0006\n",
      "Epoch [377/800], Loss: 0.0006\n",
      "Epoch [378/800], Loss: 0.0006\n",
      "Epoch [379/800], Loss: 0.0006\n",
      "Epoch [380/800], Loss: 0.0006\n",
      "Epoch [381/800], Loss: 0.0006\n",
      "Epoch [382/800], Loss: 0.0006\n",
      "Epoch [383/800], Loss: 0.0006\n",
      "Epoch [384/800], Loss: 0.0006\n",
      "Epoch [385/800], Loss: 0.0005\n",
      "Epoch [386/800], Loss: 0.0005\n",
      "Epoch [387/800], Loss: 0.0005\n",
      "Epoch [388/800], Loss: 0.0006\n",
      "Epoch [389/800], Loss: 0.0006\n",
      "Epoch [390/800], Loss: 0.0006\n",
      "Epoch [391/800], Loss: 0.0006\n",
      "Epoch [392/800], Loss: 0.0007\n",
      "Epoch [393/800], Loss: 0.0007\n",
      "Epoch [394/800], Loss: 0.0007\n",
      "Epoch [395/800], Loss: 0.0007\n",
      "Epoch [396/800], Loss: 0.0007\n",
      "Epoch [397/800], Loss: 0.0008\n",
      "Epoch [398/800], Loss: 0.0008\n",
      "Epoch [399/800], Loss: 0.0008\n",
      "Epoch [400/800], Loss: 0.0008\n",
      "Epoch [401/800], Loss: 0.0008\n",
      "Epoch [402/800], Loss: 0.0008\n",
      "Epoch [403/800], Loss: 0.0008\n",
      "Epoch [404/800], Loss: 0.0008\n",
      "Epoch [405/800], Loss: 0.0008\n",
      "Epoch [406/800], Loss: 0.0008\n",
      "Epoch [407/800], Loss: 0.0008\n",
      "Epoch [408/800], Loss: 0.0008\n",
      "Epoch [409/800], Loss: 0.0008\n",
      "Epoch [410/800], Loss: 0.0008\n",
      "Epoch [411/800], Loss: 0.0008\n",
      "Epoch [412/800], Loss: 0.0008\n",
      "Epoch [413/800], Loss: 0.0008\n",
      "Epoch [414/800], Loss: 0.0009\n",
      "Epoch [415/800], Loss: 0.0009\n",
      "Epoch [416/800], Loss: 0.0009\n",
      "Epoch [417/800], Loss: 0.0009\n",
      "Epoch [418/800], Loss: 0.0009\n",
      "Epoch [419/800], Loss: 0.0009\n",
      "Epoch [420/800], Loss: 0.0009\n",
      "Epoch [421/800], Loss: 0.0009\n",
      "Epoch [422/800], Loss: 0.0009\n",
      "Epoch [423/800], Loss: 0.0009\n",
      "Epoch [424/800], Loss: 0.0008\n",
      "Epoch [425/800], Loss: 0.0009\n",
      "Epoch [426/800], Loss: 0.0009\n",
      "Epoch [427/800], Loss: 0.0009\n",
      "Epoch [428/800], Loss: 0.0010\n",
      "Epoch [429/800], Loss: 0.0010\n",
      "Epoch [430/800], Loss: 0.0010\n",
      "Epoch [431/800], Loss: 0.0010\n",
      "Epoch [432/800], Loss: 0.0010\n",
      "Epoch [433/800], Loss: 0.0011\n",
      "Epoch [434/800], Loss: 0.0011\n",
      "Epoch [435/800], Loss: 0.0011\n",
      "Epoch [436/800], Loss: 0.0012\n",
      "Epoch [437/800], Loss: 0.0012\n",
      "Epoch [438/800], Loss: 0.0011\n",
      "Epoch [439/800], Loss: 0.0011\n",
      "Epoch [440/800], Loss: 0.0012\n",
      "Epoch [441/800], Loss: 0.0012\n",
      "Epoch [442/800], Loss: 0.0011\n",
      "Epoch [443/800], Loss: 0.0011\n",
      "Epoch [444/800], Loss: 0.0012\n",
      "Epoch [445/800], Loss: 0.0011\n",
      "Epoch [446/800], Loss: 0.0011\n",
      "Epoch [447/800], Loss: 0.0012\n",
      "Epoch [448/800], Loss: 0.0012\n",
      "Epoch [449/800], Loss: 0.0012\n",
      "Epoch [450/800], Loss: 0.0012\n",
      "Epoch [451/800], Loss: 0.0012\n",
      "Epoch [452/800], Loss: 0.0012\n",
      "Epoch [453/800], Loss: 0.0012\n",
      "Epoch [454/800], Loss: 0.0012\n",
      "Epoch [455/800], Loss: 0.0013\n",
      "Epoch [456/800], Loss: 0.0013\n",
      "Epoch [457/800], Loss: 0.0013\n",
      "Epoch [458/800], Loss: 0.0013\n",
      "Epoch [459/800], Loss: 0.0014\n",
      "Epoch [460/800], Loss: 0.0013\n",
      "Epoch [461/800], Loss: 0.0013\n",
      "Epoch [462/800], Loss: 0.0014\n",
      "Epoch [463/800], Loss: 0.0014\n",
      "Epoch [464/800], Loss: 0.0014\n",
      "Epoch [465/800], Loss: 0.0012\n",
      "Epoch [466/800], Loss: 0.0014\n",
      "Epoch [467/800], Loss: 0.0014\n",
      "Epoch [468/800], Loss: 0.0014\n",
      "Epoch [469/800], Loss: 0.0014\n",
      "Epoch [470/800], Loss: 0.0014\n",
      "Epoch [471/800], Loss: 0.0016\n",
      "Epoch [472/800], Loss: 0.0017\n",
      "Epoch [473/800], Loss: 0.0018\n",
      "Epoch [474/800], Loss: 0.0018\n",
      "Epoch [475/800], Loss: 0.0021\n",
      "Epoch [476/800], Loss: 0.0020\n",
      "Epoch [477/800], Loss: 0.0022\n",
      "Epoch [478/800], Loss: 0.0023\n",
      "Epoch [479/800], Loss: 0.0597\n",
      "Epoch [480/800], Loss: 0.0552\n",
      "Epoch [481/800], Loss: 0.0530\n",
      "Epoch [482/800], Loss: 0.0492\n",
      "Epoch [483/800], Loss: 0.0485\n",
      "Epoch [484/800], Loss: 0.0510\n",
      "Epoch [485/800], Loss: 0.0482\n",
      "Epoch [486/800], Loss: 0.0473\n",
      "Epoch [487/800], Loss: 0.0448\n",
      "Epoch [488/800], Loss: 0.0414\n",
      "Epoch [489/800], Loss: 0.0400\n",
      "Epoch [490/800], Loss: 0.0376\n",
      "Epoch [491/800], Loss: 0.0348\n",
      "Epoch [492/800], Loss: 0.0379\n",
      "Epoch [493/800], Loss: 0.0302\n",
      "Epoch [494/800], Loss: 0.0293\n",
      "Epoch [495/800], Loss: 0.0257\n",
      "Epoch [496/800], Loss: 0.0251\n",
      "Epoch [497/800], Loss: 0.0240\n",
      "Epoch [498/800], Loss: 0.0233\n",
      "Epoch [499/800], Loss: 0.0209\n",
      "Epoch [500/800], Loss: 0.0202\n",
      "Epoch [501/800], Loss: 0.0185\n",
      "Epoch [502/800], Loss: 0.0140\n",
      "Epoch [503/800], Loss: 0.0151\n",
      "Epoch [504/800], Loss: 0.0135\n",
      "Epoch [505/800], Loss: 0.0106\n",
      "Epoch [506/800], Loss: 0.0116\n",
      "Epoch [507/800], Loss: 0.0086\n",
      "Epoch [508/800], Loss: 0.0079\n",
      "Epoch [509/800], Loss: 0.0075\n",
      "Epoch [510/800], Loss: 0.0069\n",
      "Epoch [511/800], Loss: 0.0073\n",
      "Epoch [512/800], Loss: 0.0072\n",
      "Epoch [513/800], Loss: 0.0067\n",
      "Epoch [514/800], Loss: 0.0069\n",
      "Epoch [515/800], Loss: 0.0070\n",
      "Epoch [516/800], Loss: 0.0064\n",
      "Epoch [517/800], Loss: 0.0058\n",
      "Epoch [518/800], Loss: 0.0056\n",
      "Epoch [519/800], Loss: 0.0073\n",
      "Epoch [520/800], Loss: 0.0055\n",
      "Epoch [521/800], Loss: 0.0053\n",
      "Epoch [522/800], Loss: 0.0078\n",
      "Epoch [523/800], Loss: 0.0055\n",
      "Epoch [524/800], Loss: 0.0046\n",
      "Epoch [525/800], Loss: 0.0044\n",
      "Epoch [526/800], Loss: 0.0046\n",
      "Epoch [527/800], Loss: 0.0046\n",
      "Epoch [528/800], Loss: 0.0041\n",
      "Epoch [529/800], Loss: 0.0039\n",
      "Epoch [530/800], Loss: 0.0037\n",
      "Epoch [531/800], Loss: 0.0035\n",
      "Epoch [532/800], Loss: 0.0035\n",
      "Epoch [533/800], Loss: 0.0037\n",
      "Epoch [534/800], Loss: 0.0036\n",
      "Epoch [535/800], Loss: 0.0030\n",
      "Epoch [536/800], Loss: 0.0030\n",
      "Epoch [537/800], Loss: 0.0030\n",
      "Epoch [538/800], Loss: 0.0029\n",
      "Epoch [539/800], Loss: 0.0027\n",
      "Epoch [540/800], Loss: 0.0026\n",
      "Epoch [541/800], Loss: 0.0026\n",
      "Epoch [542/800], Loss: 0.0025\n",
      "Epoch [543/800], Loss: 0.0024\n",
      "Epoch [544/800], Loss: 0.0025\n",
      "Epoch [545/800], Loss: 0.0021\n",
      "Epoch [546/800], Loss: 0.0023\n",
      "Epoch [547/800], Loss: 0.0022\n",
      "Epoch [548/800], Loss: 0.0021\n",
      "Epoch [549/800], Loss: 0.0021\n",
      "Epoch [550/800], Loss: 0.0022\n",
      "Epoch [551/800], Loss: 0.0018\n",
      "Epoch [552/800], Loss: 0.0020\n",
      "Epoch [553/800], Loss: 0.0019\n",
      "Epoch [554/800], Loss: 0.0019\n",
      "Epoch [555/800], Loss: 0.0018\n",
      "Epoch [556/800], Loss: 0.0017\n",
      "Epoch [557/800], Loss: 0.0017\n",
      "Epoch [558/800], Loss: 0.0016\n",
      "Epoch [559/800], Loss: 0.0017\n",
      "Epoch [560/800], Loss: 0.0017\n",
      "Epoch [561/800], Loss: 0.0017\n",
      "Epoch [562/800], Loss: 0.0017\n",
      "Epoch [563/800], Loss: 0.0017\n",
      "Epoch [564/800], Loss: 0.0018\n",
      "Epoch [565/800], Loss: 0.0017\n",
      "Epoch [566/800], Loss: 0.0018\n",
      "Epoch [567/800], Loss: 0.0018\n",
      "Epoch [568/800], Loss: 0.0019\n",
      "Epoch [569/800], Loss: 0.0018\n",
      "Epoch [570/800], Loss: 0.0017\n",
      "Epoch [571/800], Loss: 0.0017\n",
      "Epoch [572/800], Loss: 0.0017\n",
      "Epoch [573/800], Loss: 0.0018\n",
      "Epoch [574/800], Loss: 0.0018\n",
      "Epoch [575/800], Loss: 0.0018\n",
      "Epoch [576/800], Loss: 0.0018\n",
      "Epoch [577/800], Loss: 0.0020\n",
      "Epoch [578/800], Loss: 0.0020\n",
      "Epoch [579/800], Loss: 0.0020\n",
      "Epoch [580/800], Loss: 0.0020\n",
      "Epoch [581/800], Loss: 0.0020\n",
      "Epoch [582/800], Loss: 0.0021\n",
      "Epoch [583/800], Loss: 0.0023\n",
      "Epoch [584/800], Loss: 0.0022\n",
      "Epoch [585/800], Loss: 0.0022\n",
      "Epoch [586/800], Loss: 0.0024\n",
      "Epoch [587/800], Loss: 0.0022\n",
      "Epoch [588/800], Loss: 0.0024\n",
      "Epoch [589/800], Loss: 0.0022\n",
      "Epoch [590/800], Loss: 0.0022\n",
      "Epoch [591/800], Loss: 0.0023\n",
      "Epoch [592/800], Loss: 0.0024\n",
      "Epoch [593/800], Loss: 0.0024\n",
      "Epoch [594/800], Loss: 0.0024\n",
      "Epoch [595/800], Loss: 0.0024\n",
      "Epoch [596/800], Loss: 0.0024\n",
      "Epoch [597/800], Loss: 0.0024\n",
      "Epoch [598/800], Loss: 0.0023\n",
      "Epoch [599/800], Loss: 0.0023\n",
      "Epoch [600/800], Loss: 0.0022\n",
      "Epoch [601/800], Loss: 0.0022\n",
      "Epoch [602/800], Loss: 0.0022\n",
      "Epoch [603/800], Loss: 0.0021\n",
      "Epoch [604/800], Loss: 0.0021\n",
      "Epoch [605/800], Loss: 0.0022\n",
      "Epoch [606/800], Loss: 0.0022\n",
      "Epoch [607/800], Loss: 0.0023\n",
      "Epoch [608/800], Loss: 0.0023\n",
      "Epoch [609/800], Loss: 0.0025\n",
      "Epoch [610/800], Loss: 0.0027\n",
      "Epoch [611/800], Loss: 0.0027\n",
      "Epoch [612/800], Loss: 0.0026\n",
      "Epoch [613/800], Loss: 0.0030\n",
      "Epoch [614/800], Loss: 0.0032\n",
      "Epoch [615/800], Loss: 0.0036\n",
      "Epoch [616/800], Loss: 0.0035\n",
      "Epoch [617/800], Loss: 0.0034\n",
      "Epoch [618/800], Loss: 0.0035\n",
      "Epoch [619/800], Loss: 0.0038\n",
      "Epoch [620/800], Loss: 0.0038\n",
      "Epoch [621/800], Loss: 0.0039\n",
      "Epoch [622/800], Loss: 0.0040\n",
      "Epoch [623/800], Loss: 0.0039\n",
      "Epoch [624/800], Loss: 0.0041\n",
      "Epoch [625/800], Loss: 0.0041\n",
      "Epoch [626/800], Loss: 0.0040\n",
      "Epoch [627/800], Loss: 0.0038\n",
      "Epoch [628/800], Loss: 0.0040\n",
      "Epoch [629/800], Loss: 0.0044\n",
      "Epoch [630/800], Loss: 0.0046\n",
      "Epoch [631/800], Loss: 0.0047\n",
      "Epoch [632/800], Loss: 0.0041\n",
      "Epoch [633/800], Loss: 0.0046\n",
      "Epoch [634/800], Loss: 0.0047\n",
      "Epoch [635/800], Loss: 0.0048\n",
      "Epoch [636/800], Loss: 0.0046\n",
      "Epoch [637/800], Loss: 0.0047\n",
      "Epoch [638/800], Loss: 0.0049\n",
      "Epoch [639/800], Loss: 0.0048\n",
      "Epoch [640/800], Loss: 0.0048\n",
      "Epoch [641/800], Loss: 0.0046\n",
      "Epoch [642/800], Loss: 0.0046\n",
      "Epoch [643/800], Loss: 0.0048\n",
      "Epoch [644/800], Loss: 0.0045\n",
      "Epoch [645/800], Loss: 0.0049\n",
      "Epoch [646/800], Loss: 0.0050\n",
      "Epoch [647/800], Loss: 0.0049\n",
      "Epoch [648/800], Loss: 0.0048\n",
      "Epoch [649/800], Loss: 0.0050\n",
      "Epoch [650/800], Loss: 0.0051\n",
      "Epoch [651/800], Loss: 0.0050\n",
      "Epoch [652/800], Loss: 0.0050\n",
      "Epoch [653/800], Loss: 0.0051\n",
      "Epoch [654/800], Loss: 0.0051\n",
      "Epoch [655/800], Loss: 0.0047\n",
      "Epoch [656/800], Loss: 0.0048\n",
      "Epoch [657/800], Loss: 0.0050\n",
      "Epoch [658/800], Loss: 0.0052\n",
      "Epoch [659/800], Loss: 0.0054\n",
      "Epoch [660/800], Loss: 0.0054\n",
      "Epoch [661/800], Loss: 0.0053\n",
      "Epoch [662/800], Loss: 0.0055\n",
      "Epoch [663/800], Loss: 0.0054\n",
      "Epoch [664/800], Loss: 0.0056\n",
      "Epoch [665/800], Loss: 0.0059\n",
      "Epoch [666/800], Loss: 0.0060\n",
      "Epoch [667/800], Loss: 0.0063\n",
      "Epoch [668/800], Loss: 0.0058\n",
      "Epoch [669/800], Loss: 0.0058\n",
      "Epoch [670/800], Loss: 0.0058\n",
      "Epoch [671/800], Loss: 0.0057\n",
      "Epoch [672/800], Loss: 0.0062\n",
      "Epoch [673/800], Loss: 0.0063\n",
      "Epoch [674/800], Loss: 0.0062\n",
      "Epoch [675/800], Loss: 0.0068\n",
      "Epoch [676/800], Loss: 0.0070\n",
      "Epoch [677/800], Loss: 0.0065\n",
      "Epoch [678/800], Loss: 0.0065\n",
      "Epoch [679/800], Loss: 0.0061\n",
      "Epoch [680/800], Loss: 0.0062\n",
      "Epoch [681/800], Loss: 0.0062\n",
      "Epoch [682/800], Loss: 0.0055\n",
      "Epoch [683/800], Loss: 0.0058\n",
      "Epoch [684/800], Loss: 0.0056\n",
      "Epoch [685/800], Loss: 0.0064\n",
      "Epoch [686/800], Loss: 0.0057\n",
      "Epoch [687/800], Loss: 0.0051\n",
      "Epoch [688/800], Loss: 0.0048\n",
      "Epoch [689/800], Loss: 0.0042\n",
      "Epoch [690/800], Loss: 0.0039\n",
      "Epoch [691/800], Loss: 0.0043\n",
      "Epoch [692/800], Loss: 0.0046\n",
      "Epoch [693/800], Loss: 0.0052\n",
      "Epoch [694/800], Loss: 0.0049\n",
      "Epoch [695/800], Loss: 0.0048\n",
      "Epoch [696/800], Loss: 0.0040\n",
      "Epoch [697/800], Loss: 0.0038\n",
      "Epoch [698/800], Loss: 0.0040\n",
      "Epoch [699/800], Loss: 0.0034\n",
      "Epoch [700/800], Loss: 0.0030\n",
      "Epoch [701/800], Loss: 0.0033\n",
      "Epoch [702/800], Loss: 0.0032\n",
      "Epoch [703/800], Loss: 0.0033\n",
      "Epoch [704/800], Loss: 0.0030\n",
      "Epoch [705/800], Loss: 0.0029\n",
      "Epoch [706/800], Loss: 0.0027\n",
      "Epoch [707/800], Loss: 0.0030\n",
      "Epoch [708/800], Loss: 0.0029\n",
      "Epoch [709/800], Loss: 0.0027\n",
      "Epoch [710/800], Loss: 0.0027\n",
      "Epoch [711/800], Loss: 0.0024\n",
      "Epoch [712/800], Loss: 0.0021\n",
      "Epoch [713/800], Loss: 0.0025\n",
      "Epoch [714/800], Loss: 0.0021\n",
      "Epoch [715/800], Loss: 0.0018\n",
      "Epoch [716/800], Loss: 0.0020\n",
      "Epoch [717/800], Loss: 0.0018\n",
      "Epoch [718/800], Loss: 0.0020\n",
      "Epoch [719/800], Loss: 0.0017\n",
      "Epoch [720/800], Loss: 0.0018\n",
      "Epoch [721/800], Loss: 0.0023\n",
      "Epoch [722/800], Loss: 0.0019\n",
      "Epoch [723/800], Loss: 0.0019\n",
      "Epoch [724/800], Loss: 0.0020\n",
      "Epoch [725/800], Loss: 0.0019\n",
      "Epoch [726/800], Loss: 0.0023\n",
      "Epoch [727/800], Loss: 0.0025\n",
      "Epoch [728/800], Loss: 0.0021\n",
      "Epoch [729/800], Loss: 0.0020\n",
      "Epoch [730/800], Loss: 0.0021\n",
      "Epoch [731/800], Loss: 0.0023\n",
      "Epoch [732/800], Loss: 0.0020\n",
      "Epoch [733/800], Loss: 0.0024\n",
      "Epoch [734/800], Loss: 0.0017\n",
      "Epoch [735/800], Loss: 0.0020\n",
      "Epoch [736/800], Loss: 0.0021\n",
      "Epoch [737/800], Loss: 0.0018\n",
      "Epoch [738/800], Loss: 0.0020\n",
      "Epoch [739/800], Loss: 0.0016\n",
      "Epoch [740/800], Loss: 0.0018\n",
      "Epoch [741/800], Loss: 0.0018\n",
      "Epoch [742/800], Loss: 0.0016\n",
      "Epoch [743/800], Loss: 0.0016\n",
      "Epoch [744/800], Loss: 0.0018\n",
      "Epoch [745/800], Loss: 0.0018\n",
      "Epoch [746/800], Loss: 0.0019\n",
      "Epoch [747/800], Loss: 0.0017\n",
      "Epoch [748/800], Loss: 0.0017\n",
      "Epoch [749/800], Loss: 0.0018\n",
      "Epoch [750/800], Loss: 0.0017\n",
      "Epoch [751/800], Loss: 0.0016\n",
      "Epoch [752/800], Loss: 0.0017\n",
      "Epoch [753/800], Loss: 0.0015\n",
      "Epoch [754/800], Loss: 0.0016\n",
      "Epoch [755/800], Loss: 0.0017\n",
      "Epoch [756/800], Loss: 0.0013\n",
      "Epoch [757/800], Loss: 0.0014\n",
      "Epoch [758/800], Loss: 0.0012\n",
      "Epoch [759/800], Loss: 0.0011\n",
      "Epoch [760/800], Loss: 0.0013\n",
      "Epoch [761/800], Loss: 0.0011\n",
      "Epoch [762/800], Loss: 0.0011\n",
      "Epoch [763/800], Loss: 0.0013\n",
      "Epoch [764/800], Loss: 0.0013\n",
      "Epoch [765/800], Loss: 0.0011\n",
      "Epoch [766/800], Loss: 0.0012\n",
      "Epoch [767/800], Loss: 0.0010\n",
      "Epoch [768/800], Loss: 0.0010\n",
      "Epoch [769/800], Loss: 0.0010\n",
      "Epoch [770/800], Loss: 0.0008\n",
      "Epoch [771/800], Loss: 0.0009\n",
      "Epoch [772/800], Loss: 0.0009\n",
      "Epoch [773/800], Loss: 0.0008\n",
      "Epoch [774/800], Loss: 0.0008\n",
      "Epoch [775/800], Loss: 0.0009\n",
      "Epoch [776/800], Loss: 0.0009\n",
      "Epoch [777/800], Loss: 0.0007\n",
      "Epoch [778/800], Loss: 0.0008\n",
      "Epoch [779/800], Loss: 0.0009\n",
      "Epoch [780/800], Loss: 0.0007\n",
      "Epoch [781/800], Loss: 0.0008\n",
      "Epoch [782/800], Loss: 0.0010\n",
      "Epoch [783/800], Loss: 0.0008\n",
      "Epoch [784/800], Loss: 0.0006\n",
      "Epoch [785/800], Loss: 0.0007\n",
      "Epoch [786/800], Loss: 0.0007\n",
      "Epoch [787/800], Loss: 0.0007\n",
      "Epoch [788/800], Loss: 0.0007\n",
      "Epoch [789/800], Loss: 0.0007\n",
      "Epoch [790/800], Loss: 0.0007\n",
      "Epoch [791/800], Loss: 0.0007\n",
      "Epoch [792/800], Loss: 0.0008\n",
      "Epoch [793/800], Loss: 0.0006\n",
      "Epoch [794/800], Loss: 0.0006\n",
      "Epoch [795/800], Loss: 0.0007\n",
      "Epoch [796/800], Loss: 0.0007\n",
      "Epoch [797/800], Loss: 0.0006\n",
      "Epoch [798/800], Loss: 0.0007\n",
      "Epoch [799/800], Loss: 0.0005\n",
      "Epoch [800/800], Loss: 0.0006\n",
      "Accuracy: 53.5714%\n",
      "Epoch [1/800], Loss: 0.0029\n",
      "Epoch [2/800], Loss: 0.0031\n",
      "Epoch [3/800], Loss: 0.0037\n",
      "Epoch [4/800], Loss: 0.0078\n",
      "Epoch [5/800], Loss: 0.0155\n",
      "Epoch [6/800], Loss: 0.0188\n",
      "Epoch [7/800], Loss: 0.0270\n",
      "Epoch [8/800], Loss: 0.0237\n",
      "Epoch [9/800], Loss: 0.0331\n",
      "Epoch [10/800], Loss: 0.0391\n",
      "Epoch [11/800], Loss: 0.0470\n",
      "Epoch [12/800], Loss: 0.0558\n",
      "Epoch [13/800], Loss: 0.0822\n",
      "Epoch [14/800], Loss: 0.0894\n",
      "Epoch [15/800], Loss: 0.0837\n",
      "Epoch [16/800], Loss: 0.0785\n",
      "Epoch [17/800], Loss: 0.0868\n",
      "Epoch [18/800], Loss: 0.0634\n",
      "Epoch [19/800], Loss: 0.0596\n",
      "Epoch [20/800], Loss: 0.0467\n",
      "Epoch [21/800], Loss: 0.0417\n",
      "Epoch [22/800], Loss: 0.0336\n",
      "Epoch [23/800], Loss: 0.0271\n",
      "Epoch [24/800], Loss: 0.0220\n",
      "Epoch [25/800], Loss: 0.0203\n",
      "Epoch [26/800], Loss: 0.0207\n",
      "Epoch [27/800], Loss: 0.0197\n",
      "Epoch [28/800], Loss: 0.0211\n",
      "Epoch [29/800], Loss: 0.0192\n",
      "Epoch [30/800], Loss: 0.0204\n",
      "Epoch [31/800], Loss: 0.0183\n",
      "Epoch [32/800], Loss: 0.0195\n",
      "Epoch [33/800], Loss: 0.0182\n",
      "Epoch [34/800], Loss: 0.0171\n",
      "Epoch [35/800], Loss: 0.0171\n",
      "Epoch [36/800], Loss: 0.0178\n",
      "Epoch [37/800], Loss: 0.0179\n",
      "Epoch [38/800], Loss: 0.0174\n",
      "Epoch [39/800], Loss: 0.0170\n",
      "Epoch [40/800], Loss: 0.0181\n",
      "Epoch [41/800], Loss: 0.0179\n",
      "Epoch [42/800], Loss: 0.0161\n",
      "Epoch [43/800], Loss: 0.0158\n",
      "Epoch [44/800], Loss: 0.0149\n",
      "Epoch [45/800], Loss: 0.0153\n",
      "Epoch [46/800], Loss: 0.0148\n",
      "Epoch [47/800], Loss: 0.0146\n",
      "Epoch [48/800], Loss: 0.0141\n",
      "Epoch [49/800], Loss: 0.0135\n",
      "Epoch [50/800], Loss: 0.0139\n",
      "Epoch [51/800], Loss: 0.0106\n",
      "Epoch [52/800], Loss: 0.0092\n",
      "Epoch [53/800], Loss: 0.0086\n",
      "Epoch [54/800], Loss: 0.0082\n",
      "Epoch [55/800], Loss: 0.0080\n",
      "Epoch [56/800], Loss: 0.0079\n",
      "Epoch [57/800], Loss: 0.0076\n",
      "Epoch [58/800], Loss: 0.0074\n",
      "Epoch [59/800], Loss: 0.0075\n",
      "Epoch [60/800], Loss: 0.0075\n",
      "Epoch [61/800], Loss: 0.0076\n",
      "Epoch [62/800], Loss: 0.0076\n",
      "Epoch [63/800], Loss: 0.0076\n",
      "Epoch [64/800], Loss: 0.0076\n",
      "Epoch [65/800], Loss: 0.0076\n",
      "Epoch [66/800], Loss: 0.0076\n",
      "Epoch [67/800], Loss: 0.0076\n",
      "Epoch [68/800], Loss: 0.0076\n",
      "Epoch [69/800], Loss: 0.0075\n",
      "Epoch [70/800], Loss: 0.0075\n",
      "Epoch [71/800], Loss: 0.0074\n",
      "Epoch [72/800], Loss: 0.0075\n",
      "Epoch [73/800], Loss: 0.0075\n",
      "Epoch [74/800], Loss: 0.0079\n",
      "Epoch [75/800], Loss: 0.0078\n",
      "Epoch [76/800], Loss: 0.0079\n",
      "Epoch [77/800], Loss: 0.0079\n",
      "Epoch [78/800], Loss: 0.0081\n",
      "Epoch [79/800], Loss: 0.0091\n",
      "Epoch [80/800], Loss: 0.0080\n",
      "Epoch [81/800], Loss: 0.0080\n",
      "Epoch [82/800], Loss: 0.0096\n",
      "Epoch [83/800], Loss: 0.0083\n",
      "Epoch [84/800], Loss: 0.0078\n",
      "Epoch [85/800], Loss: 0.0079\n",
      "Epoch [86/800], Loss: 0.0097\n",
      "Epoch [87/800], Loss: 0.0086\n",
      "Epoch [88/800], Loss: 0.0083\n",
      "Epoch [89/800], Loss: 0.0082\n",
      "Epoch [90/800], Loss: 0.0081\n",
      "Epoch [91/800], Loss: 0.0083\n",
      "Epoch [92/800], Loss: 0.0080\n",
      "Epoch [93/800], Loss: 0.0077\n",
      "Epoch [94/800], Loss: 0.0076\n",
      "Epoch [95/800], Loss: 0.0074\n",
      "Epoch [96/800], Loss: 0.0078\n",
      "Epoch [97/800], Loss: 0.0076\n",
      "Epoch [98/800], Loss: 0.0076\n",
      "Epoch [99/800], Loss: 0.0071\n",
      "Epoch [100/800], Loss: 0.0074\n",
      "Epoch [101/800], Loss: 0.0072\n",
      "Epoch [102/800], Loss: 0.0073\n",
      "Epoch [103/800], Loss: 0.0072\n",
      "Epoch [104/800], Loss: 0.0074\n",
      "Epoch [105/800], Loss: 0.0071\n",
      "Epoch [106/800], Loss: 0.0071\n",
      "Epoch [107/800], Loss: 0.0068\n",
      "Epoch [108/800], Loss: 0.0068\n",
      "Epoch [109/800], Loss: 0.0061\n",
      "Epoch [110/800], Loss: 0.0064\n",
      "Epoch [111/800], Loss: 0.0059\n",
      "Epoch [112/800], Loss: 0.0057\n",
      "Epoch [113/800], Loss: 0.0053\n",
      "Epoch [114/800], Loss: 0.0064\n",
      "Epoch [115/800], Loss: 0.0018\n",
      "Epoch [116/800], Loss: 0.0043\n",
      "Epoch [117/800], Loss: 0.0014\n",
      "Epoch [118/800], Loss: 0.0028\n",
      "Epoch [119/800], Loss: 0.0026\n",
      "Epoch [120/800], Loss: 0.0024\n",
      "Epoch [121/800], Loss: 0.0020\n",
      "Epoch [122/800], Loss: 0.0019\n",
      "Epoch [123/800], Loss: 0.0020\n",
      "Epoch [124/800], Loss: 0.0026\n",
      "Epoch [125/800], Loss: 0.0024\n",
      "Epoch [126/800], Loss: 0.0025\n",
      "Epoch [127/800], Loss: 0.0020\n",
      "Epoch [128/800], Loss: 0.0020\n",
      "Epoch [129/800], Loss: 0.0021\n",
      "Epoch [130/800], Loss: 0.0021\n",
      "Epoch [131/800], Loss: 0.0022\n",
      "Epoch [132/800], Loss: 0.0036\n",
      "Epoch [133/800], Loss: 0.0031\n",
      "Epoch [134/800], Loss: 0.0028\n",
      "Epoch [135/800], Loss: 0.0027\n",
      "Epoch [136/800], Loss: 0.0028\n",
      "Epoch [137/800], Loss: 0.0036\n",
      "Epoch [138/800], Loss: 0.0031\n",
      "Epoch [139/800], Loss: 0.0035\n",
      "Epoch [140/800], Loss: 0.0028\n",
      "Epoch [141/800], Loss: 0.0028\n",
      "Epoch [142/800], Loss: 0.0029\n",
      "Epoch [143/800], Loss: 0.0026\n",
      "Epoch [144/800], Loss: 0.0029\n",
      "Epoch [145/800], Loss: 0.0027\n",
      "Epoch [146/800], Loss: 0.0026\n",
      "Epoch [147/800], Loss: 0.0026\n",
      "Epoch [148/800], Loss: 0.0024\n",
      "Epoch [149/800], Loss: 0.0025\n",
      "Epoch [150/800], Loss: 0.0026\n",
      "Epoch [151/800], Loss: 0.0024\n",
      "Epoch [152/800], Loss: 0.0026\n",
      "Epoch [153/800], Loss: 0.0027\n",
      "Epoch [154/800], Loss: 0.0029\n",
      "Epoch [155/800], Loss: 0.0031\n",
      "Epoch [156/800], Loss: 0.0029\n",
      "Epoch [157/800], Loss: 0.0028\n",
      "Epoch [158/800], Loss: 0.0028\n",
      "Epoch [159/800], Loss: 0.0027\n",
      "Epoch [160/800], Loss: 0.0025\n",
      "Epoch [161/800], Loss: 0.0025\n",
      "Epoch [162/800], Loss: 0.0026\n",
      "Epoch [163/800], Loss: 0.0026\n",
      "Epoch [164/800], Loss: 0.0025\n",
      "Epoch [165/800], Loss: 0.0026\n",
      "Epoch [166/800], Loss: 0.0028\n",
      "Epoch [167/800], Loss: 0.0029\n",
      "Epoch [168/800], Loss: 0.0030\n",
      "Epoch [169/800], Loss: 0.0031\n",
      "Epoch [170/800], Loss: 0.0029\n",
      "Epoch [171/800], Loss: 0.0028\n",
      "Epoch [172/800], Loss: 0.0029\n",
      "Epoch [173/800], Loss: 0.0028\n",
      "Epoch [174/800], Loss: 0.0026\n",
      "Epoch [175/800], Loss: 0.0025\n",
      "Epoch [176/800], Loss: 0.0024\n",
      "Epoch [177/800], Loss: 0.0023\n",
      "Epoch [178/800], Loss: 0.0020\n",
      "Epoch [179/800], Loss: 0.0020\n",
      "Epoch [180/800], Loss: 0.0020\n",
      "Epoch [181/800], Loss: 0.0018\n",
      "Epoch [182/800], Loss: 0.0018\n",
      "Epoch [183/800], Loss: 0.0018\n",
      "Epoch [184/800], Loss: 0.0020\n",
      "Epoch [185/800], Loss: 0.0019\n",
      "Epoch [186/800], Loss: 0.0019\n",
      "Epoch [187/800], Loss: 0.0018\n",
      "Epoch [188/800], Loss: 0.0019\n",
      "Epoch [189/800], Loss: 0.0018\n",
      "Epoch [190/800], Loss: 0.0018\n",
      "Epoch [191/800], Loss: 0.0019\n",
      "Epoch [192/800], Loss: 0.0018\n",
      "Epoch [193/800], Loss: 0.0019\n",
      "Epoch [194/800], Loss: 0.0018\n",
      "Epoch [195/800], Loss: 0.0019\n",
      "Epoch [196/800], Loss: 0.0020\n",
      "Epoch [197/800], Loss: 0.0020\n",
      "Epoch [198/800], Loss: 0.0021\n",
      "Epoch [199/800], Loss: 0.0023\n",
      "Epoch [200/800], Loss: 0.0025\n",
      "Epoch [201/800], Loss: 0.0025\n",
      "Epoch [202/800], Loss: 0.0024\n",
      "Epoch [203/800], Loss: 0.0024\n",
      "Epoch [204/800], Loss: 0.0024\n",
      "Epoch [205/800], Loss: 0.0023\n",
      "Epoch [206/800], Loss: 0.0023\n",
      "Epoch [207/800], Loss: 0.0022\n",
      "Epoch [208/800], Loss: 0.0022\n",
      "Epoch [209/800], Loss: 0.0020\n",
      "Epoch [210/800], Loss: 0.0020\n",
      "Epoch [211/800], Loss: 0.0019\n",
      "Epoch [212/800], Loss: 0.0018\n",
      "Epoch [213/800], Loss: 0.0017\n",
      "Epoch [214/800], Loss: 0.0017\n",
      "Epoch [215/800], Loss: 0.0016\n",
      "Epoch [216/800], Loss: 0.0015\n",
      "Epoch [217/800], Loss: 0.0015\n",
      "Epoch [218/800], Loss: 0.0014\n",
      "Epoch [219/800], Loss: 0.0014\n",
      "Epoch [220/800], Loss: 0.0013\n",
      "Epoch [221/800], Loss: 0.0013\n",
      "Epoch [222/800], Loss: 0.0013\n",
      "Epoch [223/800], Loss: 0.0012\n",
      "Epoch [224/800], Loss: 0.0012\n",
      "Epoch [225/800], Loss: 0.0012\n",
      "Epoch [226/800], Loss: 0.0011\n",
      "Epoch [227/800], Loss: 0.0011\n",
      "Epoch [228/800], Loss: 0.0011\n",
      "Epoch [229/800], Loss: 0.0011\n",
      "Epoch [230/800], Loss: 0.0011\n",
      "Epoch [231/800], Loss: 0.0011\n",
      "Epoch [232/800], Loss: 0.0011\n",
      "Epoch [233/800], Loss: 0.0011\n",
      "Epoch [234/800], Loss: 0.0012\n",
      "Epoch [235/800], Loss: 0.0012\n",
      "Epoch [236/800], Loss: 0.0012\n",
      "Epoch [237/800], Loss: 0.0013\n",
      "Epoch [238/800], Loss: 0.0014\n",
      "Epoch [239/800], Loss: 0.0015\n",
      "Epoch [240/800], Loss: 0.0015\n",
      "Epoch [241/800], Loss: 0.0016\n",
      "Epoch [242/800], Loss: 0.0017\n",
      "Epoch [243/800], Loss: 0.0018\n",
      "Epoch [244/800], Loss: 0.0017\n",
      "Epoch [245/800], Loss: 0.0018\n",
      "Epoch [246/800], Loss: 0.0019\n",
      "Epoch [247/800], Loss: 0.0019\n",
      "Epoch [248/800], Loss: 0.0019\n",
      "Epoch [249/800], Loss: 0.0019\n",
      "Epoch [250/800], Loss: 0.0020\n",
      "Epoch [251/800], Loss: 0.0020\n",
      "Epoch [252/800], Loss: 0.0022\n",
      "Epoch [253/800], Loss: 0.0023\n",
      "Epoch [254/800], Loss: 0.0023\n",
      "Epoch [255/800], Loss: 0.0022\n",
      "Epoch [256/800], Loss: 0.0023\n",
      "Epoch [257/800], Loss: 0.0022\n",
      "Epoch [258/800], Loss: 0.0021\n",
      "Epoch [259/800], Loss: 0.0024\n",
      "Epoch [260/800], Loss: 0.0022\n",
      "Epoch [261/800], Loss: 0.0022\n",
      "Epoch [262/800], Loss: 0.0023\n",
      "Epoch [263/800], Loss: 0.0023\n",
      "Epoch [264/800], Loss: 0.0023\n",
      "Epoch [265/800], Loss: 0.0023\n",
      "Epoch [266/800], Loss: 0.0022\n",
      "Epoch [267/800], Loss: 0.0022\n",
      "Epoch [268/800], Loss: 0.0022\n",
      "Epoch [269/800], Loss: 0.0021\n",
      "Epoch [270/800], Loss: 0.0021\n",
      "Epoch [271/800], Loss: 0.0023\n",
      "Epoch [272/800], Loss: 0.0020\n",
      "Epoch [273/800], Loss: 0.0022\n",
      "Epoch [274/800], Loss: 0.0022\n",
      "Epoch [275/800], Loss: 0.0022\n",
      "Epoch [276/800], Loss: 0.0022\n",
      "Epoch [277/800], Loss: 0.0022\n",
      "Epoch [278/800], Loss: 0.0025\n",
      "Epoch [279/800], Loss: 0.0026\n",
      "Epoch [280/800], Loss: 0.0026\n",
      "Epoch [281/800], Loss: 0.0027\n",
      "Epoch [282/800], Loss: 0.0027\n",
      "Epoch [283/800], Loss: 0.0027\n",
      "Epoch [284/800], Loss: 0.0027\n",
      "Epoch [285/800], Loss: 0.0027\n",
      "Epoch [286/800], Loss: 0.0027\n",
      "Epoch [287/800], Loss: 0.0027\n",
      "Epoch [288/800], Loss: 0.0027\n",
      "Epoch [289/800], Loss: 0.0028\n",
      "Epoch [290/800], Loss: 0.0029\n",
      "Epoch [291/800], Loss: 0.0029\n",
      "Epoch [292/800], Loss: 0.0029\n",
      "Epoch [293/800], Loss: 0.0028\n",
      "Epoch [294/800], Loss: 0.0028\n",
      "Epoch [295/800], Loss: 0.0027\n",
      "Epoch [296/800], Loss: 0.0029\n",
      "Epoch [297/800], Loss: 0.0029\n",
      "Epoch [298/800], Loss: 0.0029\n",
      "Epoch [299/800], Loss: 0.0028\n",
      "Epoch [300/800], Loss: 0.0027\n",
      "Epoch [301/800], Loss: 0.0027\n",
      "Epoch [302/800], Loss: 0.0025\n",
      "Epoch [303/800], Loss: 0.0026\n",
      "Epoch [304/800], Loss: 0.0025\n",
      "Epoch [305/800], Loss: 0.0025\n",
      "Epoch [306/800], Loss: 0.0024\n",
      "Epoch [307/800], Loss: 0.0024\n",
      "Epoch [308/800], Loss: 0.0024\n",
      "Epoch [309/800], Loss: 0.0023\n",
      "Epoch [310/800], Loss: 0.0024\n",
      "Epoch [311/800], Loss: 0.0023\n",
      "Epoch [312/800], Loss: 0.0024\n",
      "Epoch [313/800], Loss: 0.0023\n",
      "Epoch [314/800], Loss: 0.0023\n",
      "Epoch [315/800], Loss: 0.0024\n",
      "Epoch [316/800], Loss: 0.0023\n",
      "Epoch [317/800], Loss: 0.0023\n",
      "Epoch [318/800], Loss: 0.0023\n",
      "Epoch [319/800], Loss: 0.0022\n",
      "Epoch [320/800], Loss: 0.0021\n",
      "Epoch [321/800], Loss: 0.0022\n",
      "Epoch [322/800], Loss: 0.0022\n",
      "Epoch [323/800], Loss: 0.0021\n",
      "Epoch [324/800], Loss: 0.0020\n",
      "Epoch [325/800], Loss: 0.0021\n",
      "Epoch [326/800], Loss: 0.0020\n",
      "Epoch [327/800], Loss: 0.0022\n",
      "Epoch [328/800], Loss: 0.0020\n",
      "Epoch [329/800], Loss: 0.0019\n",
      "Epoch [330/800], Loss: 0.0019\n",
      "Epoch [331/800], Loss: 0.0018\n",
      "Epoch [332/800], Loss: 0.0018\n",
      "Epoch [333/800], Loss: 0.0018\n",
      "Epoch [334/800], Loss: 0.0017\n",
      "Epoch [335/800], Loss: 0.0016\n",
      "Epoch [336/800], Loss: 0.0015\n",
      "Epoch [337/800], Loss: 0.0015\n",
      "Epoch [338/800], Loss: 0.0014\n",
      "Epoch [339/800], Loss: 0.0014\n",
      "Epoch [340/800], Loss: 0.0014\n",
      "Epoch [341/800], Loss: 0.0014\n",
      "Epoch [342/800], Loss: 0.0013\n",
      "Epoch [343/800], Loss: 0.0013\n",
      "Epoch [344/800], Loss: 0.0013\n",
      "Epoch [345/800], Loss: 0.0013\n",
      "Epoch [346/800], Loss: 0.0012\n",
      "Epoch [347/800], Loss: 0.0012\n",
      "Epoch [348/800], Loss: 0.0011\n",
      "Epoch [349/800], Loss: 0.0011\n",
      "Epoch [350/800], Loss: 0.0011\n",
      "Epoch [351/800], Loss: 0.0011\n",
      "Epoch [352/800], Loss: 0.0011\n",
      "Epoch [353/800], Loss: 0.0011\n",
      "Epoch [354/800], Loss: 0.0010\n",
      "Epoch [355/800], Loss: 0.0010\n",
      "Epoch [356/800], Loss: 0.0010\n",
      "Epoch [357/800], Loss: 0.0010\n",
      "Epoch [358/800], Loss: 0.0010\n",
      "Epoch [359/800], Loss: 0.0009\n",
      "Epoch [360/800], Loss: 0.0008\n",
      "Epoch [361/800], Loss: 0.0008\n",
      "Epoch [362/800], Loss: 0.0007\n",
      "Epoch [363/800], Loss: 0.0007\n",
      "Epoch [364/800], Loss: 0.0006\n",
      "Epoch [365/800], Loss: 0.0006\n",
      "Epoch [366/800], Loss: 0.0006\n",
      "Epoch [367/800], Loss: 0.0005\n",
      "Epoch [368/800], Loss: 0.0005\n",
      "Epoch [369/800], Loss: 0.0006\n",
      "Epoch [370/800], Loss: 0.0005\n",
      "Epoch [371/800], Loss: 0.0004\n",
      "Epoch [372/800], Loss: 0.0005\n",
      "Epoch [373/800], Loss: 0.0004\n",
      "Epoch [374/800], Loss: 0.0004\n",
      "Epoch [375/800], Loss: 0.0004\n",
      "Epoch [376/800], Loss: 0.0004\n",
      "Epoch [377/800], Loss: 0.0004\n",
      "Epoch [378/800], Loss: 0.0004\n",
      "Epoch [379/800], Loss: 0.0004\n",
      "Epoch [380/800], Loss: 0.0004\n",
      "Epoch [381/800], Loss: 0.0004\n",
      "Epoch [382/800], Loss: 0.0004\n",
      "Epoch [383/800], Loss: 0.0004\n",
      "Epoch [384/800], Loss: 0.0004\n",
      "Epoch [385/800], Loss: 0.0004\n",
      "Epoch [386/800], Loss: 0.0004\n",
      "Epoch [387/800], Loss: 0.0003\n",
      "Epoch [388/800], Loss: 0.0004\n",
      "Epoch [389/800], Loss: 0.0003\n",
      "Epoch [390/800], Loss: 0.0003\n",
      "Epoch [391/800], Loss: 0.0003\n",
      "Epoch [392/800], Loss: 0.0003\n",
      "Epoch [393/800], Loss: 0.0004\n",
      "Epoch [394/800], Loss: 0.0003\n",
      "Epoch [395/800], Loss: 0.0004\n",
      "Epoch [396/800], Loss: 0.0003\n",
      "Epoch [397/800], Loss: 0.0003\n",
      "Epoch [398/800], Loss: 0.0003\n",
      "Epoch [399/800], Loss: 0.0003\n",
      "Epoch [400/800], Loss: 0.0003\n",
      "Epoch [401/800], Loss: 0.0003\n",
      "Epoch [402/800], Loss: 0.0002\n",
      "Epoch [403/800], Loss: 0.0001\n",
      "Epoch [404/800], Loss: 0.0002\n",
      "Epoch [405/800], Loss: 0.0001\n",
      "Epoch [406/800], Loss: 0.0002\n",
      "Epoch [407/800], Loss: 0.0002\n",
      "Epoch [408/800], Loss: 0.0002\n",
      "Epoch [409/800], Loss: 0.0002\n",
      "Epoch [410/800], Loss: 0.0003\n",
      "Epoch [411/800], Loss: 0.0003\n",
      "Epoch [412/800], Loss: 0.0002\n",
      "Epoch [413/800], Loss: 0.0002\n",
      "Epoch [414/800], Loss: 0.0003\n",
      "Epoch [415/800], Loss: 0.0003\n",
      "Epoch [416/800], Loss: 0.0003\n",
      "Epoch [417/800], Loss: 0.0003\n",
      "Epoch [418/800], Loss: 0.0003\n",
      "Epoch [419/800], Loss: 0.0003\n",
      "Epoch [420/800], Loss: 0.0003\n",
      "Epoch [421/800], Loss: 0.0003\n",
      "Epoch [422/800], Loss: 0.0003\n",
      "Epoch [423/800], Loss: 0.0002\n",
      "Epoch [424/800], Loss: 0.0002\n",
      "Epoch [425/800], Loss: 0.0003\n",
      "Epoch [426/800], Loss: 0.0003\n",
      "Epoch [427/800], Loss: 0.0003\n",
      "Epoch [428/800], Loss: 0.0002\n",
      "Epoch [429/800], Loss: 0.0002\n",
      "Epoch [430/800], Loss: 0.0002\n",
      "Epoch [431/800], Loss: 0.0003\n",
      "Epoch [432/800], Loss: 0.0003\n",
      "Epoch [433/800], Loss: 0.0002\n",
      "Epoch [434/800], Loss: 0.0002\n",
      "Epoch [435/800], Loss: 0.0002\n",
      "Epoch [436/800], Loss: 0.0002\n",
      "Epoch [437/800], Loss: 0.0002\n",
      "Epoch [438/800], Loss: 0.0002\n",
      "Epoch [439/800], Loss: 0.0003\n",
      "Epoch [440/800], Loss: 0.0002\n",
      "Epoch [441/800], Loss: 0.0003\n",
      "Epoch [442/800], Loss: 0.0003\n",
      "Epoch [443/800], Loss: 0.0003\n",
      "Epoch [444/800], Loss: 0.0002\n",
      "Epoch [445/800], Loss: 0.0003\n",
      "Epoch [446/800], Loss: 0.0002\n",
      "Epoch [447/800], Loss: 0.0003\n",
      "Epoch [448/800], Loss: 0.0002\n",
      "Epoch [449/800], Loss: 0.0002\n",
      "Epoch [450/800], Loss: 0.0002\n",
      "Epoch [451/800], Loss: 0.0002\n",
      "Epoch [452/800], Loss: 0.0003\n",
      "Epoch [453/800], Loss: 0.0002\n",
      "Epoch [454/800], Loss: 0.0002\n",
      "Epoch [455/800], Loss: 0.0003\n",
      "Epoch [456/800], Loss: 0.0003\n",
      "Epoch [457/800], Loss: 0.0002\n",
      "Epoch [458/800], Loss: 0.0002\n",
      "Epoch [459/800], Loss: 0.0002\n",
      "Epoch [460/800], Loss: 0.0002\n",
      "Epoch [461/800], Loss: 0.0002\n",
      "Epoch [462/800], Loss: 0.0002\n",
      "Epoch [463/800], Loss: 0.0002\n",
      "Epoch [464/800], Loss: 0.0003\n",
      "Epoch [465/800], Loss: 0.0002\n",
      "Epoch [466/800], Loss: 0.0002\n",
      "Epoch [467/800], Loss: 0.0002\n",
      "Epoch [468/800], Loss: 0.0002\n",
      "Epoch [469/800], Loss: 0.0002\n",
      "Epoch [470/800], Loss: 0.0003\n",
      "Epoch [471/800], Loss: 0.0003\n",
      "Epoch [472/800], Loss: 0.0003\n",
      "Epoch [473/800], Loss: 0.0003\n",
      "Epoch [474/800], Loss: 0.0004\n",
      "Epoch [475/800], Loss: 0.0004\n",
      "Epoch [476/800], Loss: 0.0004\n",
      "Epoch [477/800], Loss: 0.0004\n",
      "Epoch [478/800], Loss: 0.0004\n",
      "Epoch [479/800], Loss: 0.0004\n",
      "Epoch [480/800], Loss: 0.0003\n",
      "Epoch [481/800], Loss: 0.0003\n",
      "Epoch [482/800], Loss: 0.0003\n",
      "Epoch [483/800], Loss: 0.0003\n",
      "Epoch [484/800], Loss: 0.0003\n",
      "Epoch [485/800], Loss: 0.0003\n",
      "Epoch [486/800], Loss: 0.0004\n",
      "Epoch [487/800], Loss: 0.0003\n",
      "Epoch [488/800], Loss: 0.0003\n",
      "Epoch [489/800], Loss: 0.0003\n",
      "Epoch [490/800], Loss: 0.0004\n",
      "Epoch [491/800], Loss: 0.0003\n",
      "Epoch [492/800], Loss: 0.0003\n",
      "Epoch [493/800], Loss: 0.0003\n",
      "Epoch [494/800], Loss: 0.0002\n",
      "Epoch [495/800], Loss: 0.0003\n",
      "Epoch [496/800], Loss: 0.0003\n",
      "Epoch [497/800], Loss: 0.0003\n",
      "Epoch [498/800], Loss: 0.0002\n",
      "Epoch [499/800], Loss: 0.0002\n",
      "Epoch [500/800], Loss: 0.0003\n",
      "Epoch [501/800], Loss: 0.0003\n",
      "Epoch [502/800], Loss: 0.0003\n",
      "Epoch [503/800], Loss: 0.0003\n",
      "Epoch [504/800], Loss: 0.0002\n",
      "Epoch [505/800], Loss: 0.0003\n",
      "Epoch [506/800], Loss: 0.0003\n",
      "Epoch [507/800], Loss: 0.0002\n",
      "Epoch [508/800], Loss: 0.0002\n",
      "Epoch [509/800], Loss: 0.0003\n",
      "Epoch [510/800], Loss: 0.0003\n",
      "Epoch [511/800], Loss: 0.0003\n",
      "Epoch [512/800], Loss: 0.0003\n",
      "Epoch [513/800], Loss: 0.0003\n",
      "Epoch [514/800], Loss: 0.0002\n",
      "Epoch [515/800], Loss: 0.0002\n",
      "Epoch [516/800], Loss: 0.0002\n",
      "Epoch [517/800], Loss: 0.0002\n",
      "Epoch [518/800], Loss: 0.0002\n",
      "Epoch [519/800], Loss: 0.0002\n",
      "Epoch [520/800], Loss: 0.0002\n",
      "Epoch [521/800], Loss: 0.0002\n",
      "Epoch [522/800], Loss: 0.0002\n",
      "Epoch [523/800], Loss: 0.0003\n",
      "Epoch [524/800], Loss: 0.0002\n",
      "Epoch [525/800], Loss: 0.0002\n",
      "Epoch [526/800], Loss: 0.0002\n",
      "Epoch [527/800], Loss: 0.0001\n",
      "Epoch [528/800], Loss: 0.0002\n",
      "Epoch [529/800], Loss: 0.0002\n",
      "Epoch [530/800], Loss: 0.0001\n",
      "Epoch [531/800], Loss: 0.0001\n",
      "Epoch [532/800], Loss: 0.0001\n",
      "Epoch [533/800], Loss: 0.0001\n",
      "Epoch [534/800], Loss: 0.0001\n",
      "Epoch [535/800], Loss: 0.0001\n",
      "Epoch [536/800], Loss: 0.0001\n",
      "Epoch [537/800], Loss: 0.0001\n",
      "Epoch [538/800], Loss: 0.0001\n",
      "Epoch [539/800], Loss: 0.0001\n",
      "Epoch [540/800], Loss: 0.0001\n",
      "Epoch [541/800], Loss: 0.0001\n",
      "Epoch [542/800], Loss: 0.0001\n",
      "Epoch [543/800], Loss: 0.0001\n",
      "Epoch [544/800], Loss: 0.0001\n",
      "Epoch [545/800], Loss: 0.0001\n",
      "Epoch [546/800], Loss: 0.0001\n",
      "Epoch [547/800], Loss: 0.0001\n",
      "Epoch [548/800], Loss: 0.0001\n",
      "Epoch [549/800], Loss: 0.0001\n",
      "Epoch [550/800], Loss: 0.0000\n",
      "Epoch [551/800], Loss: 0.0000\n",
      "Epoch [552/800], Loss: 0.0001\n",
      "Epoch [553/800], Loss: 0.0001\n",
      "Epoch [554/800], Loss: 0.0001\n",
      "Epoch [555/800], Loss: 0.0001\n",
      "Epoch [556/800], Loss: 0.0001\n",
      "Epoch [557/800], Loss: 0.0001\n",
      "Epoch [558/800], Loss: 0.0000\n",
      "Epoch [559/800], Loss: 0.0000\n",
      "Epoch [560/800], Loss: 0.0000\n",
      "Epoch [561/800], Loss: 0.0000\n",
      "Epoch [562/800], Loss: 0.0001\n",
      "Epoch [563/800], Loss: 0.0000\n",
      "Epoch [564/800], Loss: 0.0001\n",
      "Epoch [565/800], Loss: 0.0001\n",
      "Epoch [566/800], Loss: 0.0000\n",
      "Epoch [567/800], Loss: 0.0001\n",
      "Epoch [568/800], Loss: 0.0000\n",
      "Epoch [569/800], Loss: 0.0000\n",
      "Epoch [570/800], Loss: 0.0000\n",
      "Epoch [571/800], Loss: 0.0000\n",
      "Epoch [572/800], Loss: 0.0000\n",
      "Epoch [573/800], Loss: 0.0000\n",
      "Epoch [574/800], Loss: 0.0000\n",
      "Epoch [575/800], Loss: 0.0000\n",
      "Epoch [576/800], Loss: 0.0000\n",
      "Epoch [577/800], Loss: 0.0000\n",
      "Epoch [578/800], Loss: 0.0000\n",
      "Epoch [579/800], Loss: 0.0000\n",
      "Epoch [580/800], Loss: 0.0000\n",
      "Epoch [581/800], Loss: 0.0000\n",
      "Epoch [582/800], Loss: 0.0000\n",
      "Epoch [583/800], Loss: 0.0000\n",
      "Epoch [584/800], Loss: 0.0000\n",
      "Epoch [585/800], Loss: 0.0000\n",
      "Epoch [586/800], Loss: 0.0000\n",
      "Epoch [587/800], Loss: 0.0000\n",
      "Epoch [588/800], Loss: 0.0000\n",
      "Epoch [589/800], Loss: 0.0000\n",
      "Epoch [590/800], Loss: 0.0000\n",
      "Epoch [591/800], Loss: 0.0000\n",
      "Epoch [592/800], Loss: 0.0000\n",
      "Epoch [593/800], Loss: 0.0000\n",
      "Epoch [594/800], Loss: 0.0000\n",
      "Epoch [595/800], Loss: 0.0000\n",
      "Epoch [596/800], Loss: 0.0000\n",
      "Epoch [597/800], Loss: 0.0000\n",
      "Epoch [598/800], Loss: 0.0000\n",
      "Epoch [599/800], Loss: 0.0000\n",
      "Epoch [600/800], Loss: 0.0000\n",
      "Epoch [601/800], Loss: 0.0000\n",
      "Epoch [602/800], Loss: 0.0000\n",
      "Epoch [603/800], Loss: 0.0000\n",
      "Epoch [604/800], Loss: 0.0000\n",
      "Epoch [605/800], Loss: 0.0000\n",
      "Epoch [606/800], Loss: 0.0000\n",
      "Epoch [607/800], Loss: 0.0000\n",
      "Epoch [608/800], Loss: 0.0000\n",
      "Epoch [609/800], Loss: 0.0000\n",
      "Epoch [610/800], Loss: 0.0000\n",
      "Epoch [611/800], Loss: 0.0000\n",
      "Epoch [612/800], Loss: 0.0000\n",
      "Epoch [613/800], Loss: 0.0000\n",
      "Epoch [614/800], Loss: 0.0000\n",
      "Epoch [615/800], Loss: 0.0000\n",
      "Epoch [616/800], Loss: 0.0000\n",
      "Epoch [617/800], Loss: 0.0000\n",
      "Epoch [618/800], Loss: 0.0000\n",
      "Epoch [619/800], Loss: 0.0000\n",
      "Epoch [620/800], Loss: 0.0000\n",
      "Epoch [621/800], Loss: 0.0000\n",
      "Epoch [622/800], Loss: 0.0000\n",
      "Epoch [623/800], Loss: 0.0000\n",
      "Epoch [624/800], Loss: 0.0000\n",
      "Epoch [625/800], Loss: 0.0000\n",
      "Epoch [626/800], Loss: 0.0000\n",
      "Epoch [627/800], Loss: 0.0000\n",
      "Epoch [628/800], Loss: 0.0000\n",
      "Epoch [629/800], Loss: 0.0000\n",
      "Epoch [630/800], Loss: 0.0000\n",
      "Epoch [631/800], Loss: 0.0000\n",
      "Epoch [632/800], Loss: 0.0000\n",
      "Epoch [633/800], Loss: 0.0000\n",
      "Epoch [634/800], Loss: 0.0000\n",
      "Epoch [635/800], Loss: 0.0000\n",
      "Epoch [636/800], Loss: 0.0000\n",
      "Epoch [637/800], Loss: 0.0000\n",
      "Epoch [638/800], Loss: 0.0000\n",
      "Epoch [639/800], Loss: 0.0001\n",
      "Epoch [640/800], Loss: 0.0000\n",
      "Epoch [641/800], Loss: 0.0000\n",
      "Epoch [642/800], Loss: 0.0000\n",
      "Epoch [643/800], Loss: 0.0000\n",
      "Epoch [644/800], Loss: 0.0000\n",
      "Epoch [645/800], Loss: 0.0000\n",
      "Epoch [646/800], Loss: 0.0000\n",
      "Epoch [647/800], Loss: 0.0000\n",
      "Epoch [648/800], Loss: 0.0000\n",
      "Epoch [649/800], Loss: 0.0000\n",
      "Epoch [650/800], Loss: 0.0000\n",
      "Epoch [651/800], Loss: 0.0000\n",
      "Epoch [652/800], Loss: 0.0000\n",
      "Epoch [653/800], Loss: 0.0000\n",
      "Epoch [654/800], Loss: 0.0001\n",
      "Epoch [655/800], Loss: 0.0001\n",
      "Epoch [656/800], Loss: 0.0000\n",
      "Epoch [657/800], Loss: 0.0000\n",
      "Epoch [658/800], Loss: 0.0000\n",
      "Epoch [659/800], Loss: 0.0000\n",
      "Epoch [660/800], Loss: 0.0000\n",
      "Epoch [661/800], Loss: 0.0000\n",
      "Epoch [662/800], Loss: 0.0000\n",
      "Epoch [663/800], Loss: 0.0000\n",
      "Epoch [664/800], Loss: 0.0000\n",
      "Epoch [665/800], Loss: 0.0000\n",
      "Epoch [666/800], Loss: 0.0000\n",
      "Epoch [667/800], Loss: 0.0000\n",
      "Epoch [668/800], Loss: 0.0000\n",
      "Epoch [669/800], Loss: 0.0000\n",
      "Epoch [670/800], Loss: 0.0000\n",
      "Epoch [671/800], Loss: 0.0000\n",
      "Epoch [672/800], Loss: 0.0000\n",
      "Epoch [673/800], Loss: 0.0000\n",
      "Epoch [674/800], Loss: 0.0001\n",
      "Epoch [675/800], Loss: 0.0001\n",
      "Epoch [676/800], Loss: 0.0000\n",
      "Epoch [677/800], Loss: 0.0000\n",
      "Epoch [678/800], Loss: 0.0000\n",
      "Epoch [679/800], Loss: 0.0000\n",
      "Epoch [680/800], Loss: 0.0000\n",
      "Epoch [681/800], Loss: 0.0001\n",
      "Epoch [682/800], Loss: 0.0001\n",
      "Epoch [683/800], Loss: 0.0001\n",
      "Epoch [684/800], Loss: 0.0000\n",
      "Epoch [685/800], Loss: 0.0000\n",
      "Epoch [686/800], Loss: 0.0001\n",
      "Epoch [687/800], Loss: 0.0001\n",
      "Epoch [688/800], Loss: 0.0001\n",
      "Epoch [689/800], Loss: 0.0001\n",
      "Epoch [690/800], Loss: 0.0001\n",
      "Epoch [691/800], Loss: 0.0000\n",
      "Epoch [692/800], Loss: 0.0001\n",
      "Epoch [693/800], Loss: 0.0001\n",
      "Epoch [694/800], Loss: 0.0000\n",
      "Epoch [695/800], Loss: 0.0000\n",
      "Epoch [696/800], Loss: 0.0001\n",
      "Epoch [697/800], Loss: 0.0001\n",
      "Epoch [698/800], Loss: 0.0001\n",
      "Epoch [699/800], Loss: 0.0001\n",
      "Epoch [700/800], Loss: 0.0001\n",
      "Epoch [701/800], Loss: 0.0001\n",
      "Epoch [702/800], Loss: 0.0001\n",
      "Epoch [703/800], Loss: 0.0001\n",
      "Epoch [704/800], Loss: 0.0001\n",
      "Epoch [705/800], Loss: 0.0001\n",
      "Epoch [706/800], Loss: 0.0001\n",
      "Epoch [707/800], Loss: 0.0001\n",
      "Epoch [708/800], Loss: 0.0000\n",
      "Epoch [709/800], Loss: 0.0000\n",
      "Epoch [710/800], Loss: 0.0001\n",
      "Epoch [711/800], Loss: 0.0001\n",
      "Epoch [712/800], Loss: 0.0000\n",
      "Epoch [713/800], Loss: 0.0000\n",
      "Epoch [714/800], Loss: 0.0001\n",
      "Epoch [715/800], Loss: 0.0000\n",
      "Epoch [716/800], Loss: 0.0000\n",
      "Epoch [717/800], Loss: 0.0000\n",
      "Epoch [718/800], Loss: 0.0000\n",
      "Epoch [719/800], Loss: 0.0000\n",
      "Epoch [720/800], Loss: 0.0000\n",
      "Epoch [721/800], Loss: 0.0000\n",
      "Epoch [722/800], Loss: 0.0001\n",
      "Epoch [723/800], Loss: 0.0000\n",
      "Epoch [724/800], Loss: 0.0000\n",
      "Epoch [725/800], Loss: 0.0000\n",
      "Epoch [726/800], Loss: 0.0001\n",
      "Epoch [727/800], Loss: 0.0000\n",
      "Epoch [728/800], Loss: 0.0000\n",
      "Epoch [729/800], Loss: 0.0000\n",
      "Epoch [730/800], Loss: 0.0001\n",
      "Epoch [731/800], Loss: 0.0001\n",
      "Epoch [732/800], Loss: 0.0000\n",
      "Epoch [733/800], Loss: 0.0000\n",
      "Epoch [734/800], Loss: 0.0001\n",
      "Epoch [735/800], Loss: 0.0000\n",
      "Epoch [736/800], Loss: 0.0000\n",
      "Epoch [737/800], Loss: 0.0000\n",
      "Epoch [738/800], Loss: 0.0000\n",
      "Epoch [739/800], Loss: 0.0001\n",
      "Epoch [740/800], Loss: 0.0000\n",
      "Epoch [741/800], Loss: 0.0001\n",
      "Epoch [742/800], Loss: 0.0001\n",
      "Epoch [743/800], Loss: 0.0001\n",
      "Epoch [744/800], Loss: 0.0001\n",
      "Epoch [745/800], Loss: 0.0000\n",
      "Epoch [746/800], Loss: 0.0001\n",
      "Epoch [747/800], Loss: 0.0001\n",
      "Epoch [748/800], Loss: 0.0001\n",
      "Epoch [749/800], Loss: 0.0001\n",
      "Epoch [750/800], Loss: 0.0000\n",
      "Epoch [751/800], Loss: 0.0001\n",
      "Epoch [752/800], Loss: 0.0001\n",
      "Epoch [753/800], Loss: 0.0001\n",
      "Epoch [754/800], Loss: 0.0001\n",
      "Epoch [755/800], Loss: 0.0001\n",
      "Epoch [756/800], Loss: 0.0001\n",
      "Epoch [757/800], Loss: 0.0001\n",
      "Epoch [758/800], Loss: 0.0001\n",
      "Epoch [759/800], Loss: 0.0001\n",
      "Epoch [760/800], Loss: 0.0001\n",
      "Epoch [761/800], Loss: 0.0001\n",
      "Epoch [762/800], Loss: 0.0001\n",
      "Epoch [763/800], Loss: 0.0001\n",
      "Epoch [764/800], Loss: 0.0001\n",
      "Epoch [765/800], Loss: 0.0001\n",
      "Epoch [766/800], Loss: 0.0001\n",
      "Epoch [767/800], Loss: 0.0001\n",
      "Epoch [768/800], Loss: 0.0001\n",
      "Epoch [769/800], Loss: 0.0001\n",
      "Epoch [770/800], Loss: 0.0001\n",
      "Epoch [771/800], Loss: 0.0001\n",
      "Epoch [772/800], Loss: 0.0001\n",
      "Epoch [773/800], Loss: 0.0001\n",
      "Epoch [774/800], Loss: 0.0001\n",
      "Epoch [775/800], Loss: 0.0001\n",
      "Epoch [776/800], Loss: 0.0001\n",
      "Epoch [777/800], Loss: 0.0001\n",
      "Epoch [778/800], Loss: 0.0001\n",
      "Epoch [779/800], Loss: 0.0001\n",
      "Epoch [780/800], Loss: 0.0000\n",
      "Epoch [781/800], Loss: 0.0000\n",
      "Epoch [782/800], Loss: 0.0000\n",
      "Epoch [783/800], Loss: 0.0000\n",
      "Epoch [784/800], Loss: 0.0000\n",
      "Epoch [785/800], Loss: 0.0001\n",
      "Epoch [786/800], Loss: 0.0001\n",
      "Epoch [787/800], Loss: 0.0000\n",
      "Epoch [788/800], Loss: 0.0001\n",
      "Epoch [789/800], Loss: 0.0001\n",
      "Epoch [790/800], Loss: 0.0001\n",
      "Epoch [791/800], Loss: 0.0000\n",
      "Epoch [792/800], Loss: 0.0000\n",
      "Epoch [793/800], Loss: 0.0000\n",
      "Epoch [794/800], Loss: 0.0000\n",
      "Epoch [795/800], Loss: 0.0000\n",
      "Epoch [796/800], Loss: 0.0000\n",
      "Epoch [797/800], Loss: 0.0000\n",
      "Epoch [798/800], Loss: 0.0001\n",
      "Epoch [799/800], Loss: 0.0000\n",
      "Epoch [800/800], Loss: 0.0001\n",
      "Accuracy: 61.9048%\n",
      "Epoch [1/800], Loss: 0.0013\n",
      "Epoch [2/800], Loss: 0.0013\n",
      "Epoch [3/800], Loss: 0.0037\n",
      "Epoch [4/800], Loss: 0.0075\n",
      "Epoch [5/800], Loss: 0.0117\n",
      "Epoch [6/800], Loss: 0.0165\n",
      "Epoch [7/800], Loss: 0.0211\n",
      "Epoch [8/800], Loss: 0.0264\n",
      "Epoch [9/800], Loss: 0.0265\n",
      "Epoch [10/800], Loss: 0.0512\n",
      "Epoch [11/800], Loss: 0.0358\n",
      "Epoch [12/800], Loss: 0.0423\n",
      "Epoch [13/800], Loss: 0.0233\n",
      "Epoch [14/800], Loss: 0.0233\n",
      "Epoch [15/800], Loss: 0.0257\n",
      "Epoch [16/800], Loss: 0.0208\n",
      "Epoch [17/800], Loss: 0.0216\n",
      "Epoch [18/800], Loss: 0.0233\n",
      "Epoch [19/800], Loss: 0.0203\n",
      "Epoch [20/800], Loss: 0.0228\n",
      "Epoch [21/800], Loss: 0.0202\n",
      "Epoch [22/800], Loss: 0.0195\n",
      "Epoch [23/800], Loss: 0.0201\n",
      "Epoch [24/800], Loss: 0.0209\n",
      "Epoch [25/800], Loss: 0.0235\n",
      "Epoch [26/800], Loss: 0.0206\n",
      "Epoch [27/800], Loss: 0.0251\n",
      "Epoch [28/800], Loss: 0.0229\n",
      "Epoch [29/800], Loss: 0.0221\n",
      "Epoch [30/800], Loss: 0.0216\n",
      "Epoch [31/800], Loss: 0.0201\n",
      "Epoch [32/800], Loss: 0.0206\n",
      "Epoch [33/800], Loss: 0.0200\n",
      "Epoch [34/800], Loss: 0.0189\n",
      "Epoch [35/800], Loss: 0.0203\n",
      "Epoch [36/800], Loss: 0.0199\n",
      "Epoch [37/800], Loss: 0.0190\n",
      "Epoch [38/800], Loss: 0.0189\n",
      "Epoch [39/800], Loss: 0.0170\n",
      "Epoch [40/800], Loss: 0.0162\n",
      "Epoch [41/800], Loss: 0.0136\n",
      "Epoch [42/800], Loss: 0.0122\n",
      "Epoch [43/800], Loss: 0.0123\n",
      "Epoch [44/800], Loss: 0.0119\n",
      "Epoch [45/800], Loss: 0.0113\n",
      "Epoch [46/800], Loss: 0.0109\n",
      "Epoch [47/800], Loss: 0.0105\n",
      "Epoch [48/800], Loss: 0.0100\n",
      "Epoch [49/800], Loss: 0.0096\n",
      "Epoch [50/800], Loss: 0.0084\n",
      "Epoch [51/800], Loss: 0.0094\n",
      "Epoch [52/800], Loss: 0.0091\n",
      "Epoch [53/800], Loss: 0.0089\n",
      "Epoch [54/800], Loss: 0.0092\n",
      "Epoch [55/800], Loss: 0.0078\n",
      "Epoch [56/800], Loss: 0.0097\n",
      "Epoch [57/800], Loss: 0.0088\n",
      "Epoch [58/800], Loss: 0.0091\n",
      "Epoch [59/800], Loss: 0.0094\n",
      "Epoch [60/800], Loss: 0.0091\n",
      "Epoch [61/800], Loss: 0.0092\n",
      "Epoch [62/800], Loss: 0.0095\n",
      "Epoch [63/800], Loss: 0.0095\n",
      "Epoch [64/800], Loss: 0.0093\n",
      "Epoch [65/800], Loss: 0.0092\n",
      "Epoch [66/800], Loss: 0.0093\n",
      "Epoch [67/800], Loss: 0.0092\n",
      "Epoch [68/800], Loss: 0.0089\n",
      "Epoch [69/800], Loss: 0.0087\n",
      "Epoch [70/800], Loss: 0.0084\n",
      "Epoch [71/800], Loss: 0.0081\n",
      "Epoch [72/800], Loss: 0.0081\n",
      "Epoch [73/800], Loss: 0.0078\n",
      "Epoch [74/800], Loss: 0.0077\n",
      "Epoch [75/800], Loss: 0.0073\n",
      "Epoch [76/800], Loss: 0.0071\n",
      "Epoch [77/800], Loss: 0.0072\n",
      "Epoch [78/800], Loss: 0.0070\n",
      "Epoch [79/800], Loss: 0.0069\n",
      "Epoch [80/800], Loss: 0.0069\n",
      "Epoch [81/800], Loss: 0.0067\n",
      "Epoch [82/800], Loss: 0.0065\n",
      "Epoch [83/800], Loss: 0.0063\n",
      "Epoch [84/800], Loss: 0.0062\n",
      "Epoch [85/800], Loss: 0.0060\n",
      "Epoch [86/800], Loss: 0.0059\n",
      "Epoch [87/800], Loss: 0.0057\n",
      "Epoch [88/800], Loss: 0.0056\n",
      "Epoch [89/800], Loss: 0.0055\n",
      "Epoch [90/800], Loss: 0.0054\n",
      "Epoch [91/800], Loss: 0.0054\n",
      "Epoch [92/800], Loss: 0.0053\n",
      "Epoch [93/800], Loss: 0.0053\n",
      "Epoch [94/800], Loss: 0.0053\n",
      "Epoch [95/800], Loss: 0.0053\n",
      "Epoch [96/800], Loss: 0.0054\n",
      "Epoch [97/800], Loss: 0.0055\n",
      "Epoch [98/800], Loss: 0.0055\n",
      "Epoch [99/800], Loss: 0.0055\n",
      "Epoch [100/800], Loss: 0.0057\n",
      "Epoch [101/800], Loss: 0.0058\n",
      "Epoch [102/800], Loss: 0.0058\n",
      "Epoch [103/800], Loss: 0.0060\n",
      "Epoch [104/800], Loss: 0.0061\n",
      "Epoch [105/800], Loss: 0.0062\n",
      "Epoch [106/800], Loss: 0.0063\n",
      "Epoch [107/800], Loss: 0.0064\n",
      "Epoch [108/800], Loss: 0.0067\n",
      "Epoch [109/800], Loss: 0.0069\n",
      "Epoch [110/800], Loss: 0.0071\n",
      "Epoch [111/800], Loss: 0.0076\n",
      "Epoch [112/800], Loss: 0.0077\n",
      "Epoch [113/800], Loss: 0.0079\n",
      "Epoch [114/800], Loss: 0.0080\n",
      "Epoch [115/800], Loss: 0.0078\n",
      "Epoch [116/800], Loss: 0.0079\n",
      "Epoch [117/800], Loss: 0.0079\n",
      "Epoch [118/800], Loss: 0.0078\n",
      "Epoch [119/800], Loss: 0.0078\n",
      "Epoch [120/800], Loss: 0.0078\n",
      "Epoch [121/800], Loss: 0.0079\n",
      "Epoch [122/800], Loss: 0.0079\n",
      "Epoch [123/800], Loss: 0.0080\n",
      "Epoch [124/800], Loss: 0.0078\n",
      "Epoch [125/800], Loss: 0.0079\n",
      "Epoch [126/800], Loss: 0.0081\n",
      "Epoch [127/800], Loss: 0.0083\n",
      "Epoch [128/800], Loss: 0.0085\n",
      "Epoch [129/800], Loss: 0.0088\n",
      "Epoch [130/800], Loss: 0.0090\n",
      "Epoch [131/800], Loss: 0.0091\n",
      "Epoch [132/800], Loss: 0.0094\n",
      "Epoch [133/800], Loss: 0.0096\n",
      "Epoch [134/800], Loss: 0.0096\n",
      "Epoch [135/800], Loss: 0.0101\n",
      "Epoch [136/800], Loss: 0.0103\n",
      "Epoch [137/800], Loss: 0.0105\n",
      "Epoch [138/800], Loss: 0.0108\n",
      "Epoch [139/800], Loss: 0.0109\n",
      "Epoch [140/800], Loss: 0.0110\n",
      "Epoch [141/800], Loss: 0.0117\n",
      "Epoch [142/800], Loss: 0.0123\n",
      "Epoch [143/800], Loss: 0.0133\n",
      "Epoch [144/800], Loss: 0.0142\n",
      "Epoch [145/800], Loss: 0.0151\n",
      "Epoch [146/800], Loss: 0.0164\n",
      "Epoch [147/800], Loss: 0.0176\n",
      "Epoch [148/800], Loss: 0.0191\n",
      "Epoch [149/800], Loss: 0.0207\n",
      "Epoch [150/800], Loss: 0.0218\n",
      "Epoch [151/800], Loss: 0.0232\n",
      "Epoch [152/800], Loss: 0.0244\n",
      "Epoch [153/800], Loss: 0.0249\n",
      "Epoch [154/800], Loss: 0.0261\n",
      "Epoch [155/800], Loss: 0.0268\n",
      "Epoch [156/800], Loss: 0.0305\n",
      "Epoch [157/800], Loss: 0.0327\n",
      "Epoch [158/800], Loss: 0.0335\n",
      "Epoch [159/800], Loss: 0.0370\n",
      "Epoch [160/800], Loss: 0.0384\n",
      "Epoch [161/800], Loss: 0.0386\n",
      "Epoch [162/800], Loss: 0.0370\n",
      "Epoch [163/800], Loss: 0.0351\n",
      "Epoch [164/800], Loss: 0.0331\n",
      "Epoch [165/800], Loss: 0.0315\n",
      "Epoch [166/800], Loss: 0.0285\n",
      "Epoch [167/800], Loss: 0.0277\n",
      "Epoch [168/800], Loss: 0.0239\n",
      "Epoch [169/800], Loss: 0.0218\n",
      "Epoch [170/800], Loss: 0.0201\n",
      "Epoch [171/800], Loss: 0.0167\n",
      "Epoch [172/800], Loss: 0.0154\n",
      "Epoch [173/800], Loss: 0.0129\n",
      "Epoch [174/800], Loss: 0.0122\n",
      "Epoch [175/800], Loss: 0.0110\n",
      "Epoch [176/800], Loss: 0.0102\n",
      "Epoch [177/800], Loss: 0.0091\n",
      "Epoch [178/800], Loss: 0.0086\n",
      "Epoch [179/800], Loss: 0.0082\n",
      "Epoch [180/800], Loss: 0.0078\n",
      "Epoch [181/800], Loss: 0.0073\n",
      "Epoch [182/800], Loss: 0.0072\n",
      "Epoch [183/800], Loss: 0.0068\n",
      "Epoch [184/800], Loss: 0.0065\n",
      "Epoch [185/800], Loss: 0.0064\n",
      "Epoch [186/800], Loss: 0.0061\n",
      "Epoch [187/800], Loss: 0.0058\n",
      "Epoch [188/800], Loss: 0.0058\n",
      "Epoch [189/800], Loss: 0.0058\n",
      "Epoch [190/800], Loss: 0.0056\n",
      "Epoch [191/800], Loss: 0.0055\n",
      "Epoch [192/800], Loss: 0.0054\n",
      "Epoch [193/800], Loss: 0.0050\n",
      "Epoch [194/800], Loss: 0.0050\n",
      "Epoch [195/800], Loss: 0.0048\n",
      "Epoch [196/800], Loss: 0.0044\n",
      "Epoch [197/800], Loss: 0.0045\n",
      "Epoch [198/800], Loss: 0.0042\n",
      "Epoch [199/800], Loss: 0.0042\n",
      "Epoch [200/800], Loss: 0.0042\n",
      "Epoch [201/800], Loss: 0.0037\n",
      "Epoch [202/800], Loss: 0.0036\n",
      "Epoch [203/800], Loss: 0.0035\n",
      "Epoch [204/800], Loss: 0.0033\n",
      "Epoch [205/800], Loss: 0.0031\n",
      "Epoch [206/800], Loss: 0.0029\n",
      "Epoch [207/800], Loss: 0.0027\n",
      "Epoch [208/800], Loss: 0.0027\n",
      "Epoch [209/800], Loss: 0.0025\n",
      "Epoch [210/800], Loss: 0.0025\n",
      "Epoch [211/800], Loss: 0.0024\n",
      "Epoch [212/800], Loss: 0.0024\n",
      "Epoch [213/800], Loss: 0.0023\n",
      "Epoch [214/800], Loss: 0.0022\n",
      "Epoch [215/800], Loss: 0.0022\n",
      "Epoch [216/800], Loss: 0.0021\n",
      "Epoch [217/800], Loss: 0.0021\n",
      "Epoch [218/800], Loss: 0.0021\n",
      "Epoch [219/800], Loss: 0.0020\n",
      "Epoch [220/800], Loss: 0.0020\n",
      "Epoch [221/800], Loss: 0.0020\n",
      "Epoch [222/800], Loss: 0.0021\n",
      "Epoch [223/800], Loss: 0.0021\n",
      "Epoch [224/800], Loss: 0.0020\n",
      "Epoch [225/800], Loss: 0.0020\n",
      "Epoch [226/800], Loss: 0.0020\n",
      "Epoch [227/800], Loss: 0.0019\n",
      "Epoch [228/800], Loss: 0.0019\n",
      "Epoch [229/800], Loss: 0.0019\n",
      "Epoch [230/800], Loss: 0.0019\n",
      "Epoch [231/800], Loss: 0.0018\n",
      "Epoch [232/800], Loss: 0.0018\n",
      "Epoch [233/800], Loss: 0.0018\n",
      "Epoch [234/800], Loss: 0.0017\n",
      "Epoch [235/800], Loss: 0.0017\n",
      "Epoch [236/800], Loss: 0.0018\n",
      "Epoch [237/800], Loss: 0.0016\n",
      "Epoch [238/800], Loss: 0.0016\n",
      "Epoch [239/800], Loss: 0.0015\n",
      "Epoch [240/800], Loss: 0.0015\n",
      "Epoch [241/800], Loss: 0.0014\n",
      "Epoch [242/800], Loss: 0.0014\n",
      "Epoch [243/800], Loss: 0.0014\n",
      "Epoch [244/800], Loss: 0.0013\n",
      "Epoch [245/800], Loss: 0.0013\n",
      "Epoch [246/800], Loss: 0.0013\n",
      "Epoch [247/800], Loss: 0.0012\n",
      "Epoch [248/800], Loss: 0.0012\n",
      "Epoch [249/800], Loss: 0.0011\n",
      "Epoch [250/800], Loss: 0.0011\n",
      "Epoch [251/800], Loss: 0.0010\n",
      "Epoch [252/800], Loss: 0.0010\n",
      "Epoch [253/800], Loss: 0.0010\n",
      "Epoch [254/800], Loss: 0.0010\n",
      "Epoch [255/800], Loss: 0.0010\n",
      "Epoch [256/800], Loss: 0.0009\n",
      "Epoch [257/800], Loss: 0.0009\n",
      "Epoch [258/800], Loss: 0.0009\n",
      "Epoch [259/800], Loss: 0.0009\n",
      "Epoch [260/800], Loss: 0.0010\n",
      "Epoch [261/800], Loss: 0.0009\n",
      "Epoch [262/800], Loss: 0.0009\n",
      "Epoch [263/800], Loss: 0.0009\n",
      "Epoch [264/800], Loss: 0.0009\n",
      "Epoch [265/800], Loss: 0.0008\n",
      "Epoch [266/800], Loss: 0.0009\n",
      "Epoch [267/800], Loss: 0.0009\n",
      "Epoch [268/800], Loss: 0.0009\n",
      "Epoch [269/800], Loss: 0.0009\n",
      "Epoch [270/800], Loss: 0.0010\n",
      "Epoch [271/800], Loss: 0.0010\n",
      "Epoch [272/800], Loss: 0.0010\n",
      "Epoch [273/800], Loss: 0.0010\n",
      "Epoch [274/800], Loss: 0.0009\n",
      "Epoch [275/800], Loss: 0.0009\n",
      "Epoch [276/800], Loss: 0.0010\n",
      "Epoch [277/800], Loss: 0.0010\n",
      "Epoch [278/800], Loss: 0.0010\n",
      "Epoch [279/800], Loss: 0.0011\n",
      "Epoch [280/800], Loss: 0.0011\n",
      "Epoch [281/800], Loss: 0.0012\n",
      "Epoch [282/800], Loss: 0.0011\n",
      "Epoch [283/800], Loss: 0.0011\n",
      "Epoch [284/800], Loss: 0.0011\n",
      "Epoch [285/800], Loss: 0.0011\n",
      "Epoch [286/800], Loss: 0.0010\n",
      "Epoch [287/800], Loss: 0.0011\n",
      "Epoch [288/800], Loss: 0.0010\n",
      "Epoch [289/800], Loss: 0.0011\n",
      "Epoch [290/800], Loss: 0.0010\n",
      "Epoch [291/800], Loss: 0.0010\n",
      "Epoch [292/800], Loss: 0.0010\n",
      "Epoch [293/800], Loss: 0.0010\n",
      "Epoch [294/800], Loss: 0.0011\n",
      "Epoch [295/800], Loss: 0.0010\n",
      "Epoch [296/800], Loss: 0.0010\n",
      "Epoch [297/800], Loss: 0.0010\n",
      "Epoch [298/800], Loss: 0.0010\n",
      "Epoch [299/800], Loss: 0.0011\n",
      "Epoch [300/800], Loss: 0.0011\n",
      "Epoch [301/800], Loss: 0.0011\n",
      "Epoch [302/800], Loss: 0.0012\n",
      "Epoch [303/800], Loss: 0.0012\n",
      "Epoch [304/800], Loss: 0.0012\n",
      "Epoch [305/800], Loss: 0.0011\n",
      "Epoch [306/800], Loss: 0.0012\n",
      "Epoch [307/800], Loss: 0.0011\n",
      "Epoch [308/800], Loss: 0.0011\n",
      "Epoch [309/800], Loss: 0.0010\n",
      "Epoch [310/800], Loss: 0.0011\n",
      "Epoch [311/800], Loss: 0.0010\n",
      "Epoch [312/800], Loss: 0.0010\n",
      "Epoch [313/800], Loss: 0.0010\n",
      "Epoch [314/800], Loss: 0.0009\n",
      "Epoch [315/800], Loss: 0.0009\n",
      "Epoch [316/800], Loss: 0.0008\n",
      "Epoch [317/800], Loss: 0.0008\n",
      "Epoch [318/800], Loss: 0.0008\n",
      "Epoch [319/800], Loss: 0.0008\n",
      "Epoch [320/800], Loss: 0.0008\n",
      "Epoch [321/800], Loss: 0.0007\n",
      "Epoch [322/800], Loss: 0.0008\n",
      "Epoch [323/800], Loss: 0.0007\n",
      "Epoch [324/800], Loss: 0.0007\n",
      "Epoch [325/800], Loss: 0.0007\n",
      "Epoch [326/800], Loss: 0.0007\n",
      "Epoch [327/800], Loss: 0.0006\n",
      "Epoch [328/800], Loss: 0.0006\n",
      "Epoch [329/800], Loss: 0.0006\n",
      "Epoch [330/800], Loss: 0.0007\n",
      "Epoch [331/800], Loss: 0.0007\n",
      "Epoch [332/800], Loss: 0.0007\n",
      "Epoch [333/800], Loss: 0.0008\n",
      "Epoch [334/800], Loss: 0.0008\n",
      "Epoch [335/800], Loss: 0.0008\n",
      "Epoch [336/800], Loss: 0.0008\n",
      "Epoch [337/800], Loss: 0.0008\n",
      "Epoch [338/800], Loss: 0.0007\n",
      "Epoch [339/800], Loss: 0.0007\n",
      "Epoch [340/800], Loss: 0.0006\n",
      "Epoch [341/800], Loss: 0.0006\n",
      "Epoch [342/800], Loss: 0.0006\n",
      "Epoch [343/800], Loss: 0.0005\n",
      "Epoch [344/800], Loss: 0.0006\n",
      "Epoch [345/800], Loss: 0.0006\n",
      "Epoch [346/800], Loss: 0.0006\n",
      "Epoch [347/800], Loss: 0.0006\n",
      "Epoch [348/800], Loss: 0.0006\n",
      "Epoch [349/800], Loss: 0.0006\n",
      "Epoch [350/800], Loss: 0.0007\n",
      "Epoch [351/800], Loss: 0.0006\n",
      "Epoch [352/800], Loss: 0.0006\n",
      "Epoch [353/800], Loss: 0.0008\n",
      "Epoch [354/800], Loss: 0.0008\n",
      "Epoch [355/800], Loss: 0.0009\n",
      "Epoch [356/800], Loss: 0.0006\n",
      "Epoch [357/800], Loss: 0.0007\n",
      "Epoch [358/800], Loss: 0.0006\n",
      "Epoch [359/800], Loss: 0.0008\n",
      "Epoch [360/800], Loss: 0.0006\n",
      "Epoch [361/800], Loss: 0.0006\n",
      "Epoch [362/800], Loss: 0.0006\n",
      "Epoch [363/800], Loss: 0.0008\n",
      "Epoch [364/800], Loss: 0.0006\n",
      "Epoch [365/800], Loss: 0.0005\n",
      "Epoch [366/800], Loss: 0.0009\n",
      "Epoch [367/800], Loss: 0.0008\n",
      "Epoch [368/800], Loss: 0.0007\n",
      "Epoch [369/800], Loss: 0.0007\n",
      "Epoch [370/800], Loss: 0.0009\n",
      "Epoch [371/800], Loss: 0.0010\n",
      "Epoch [372/800], Loss: 0.0008\n",
      "Epoch [373/800], Loss: 0.0007\n",
      "Epoch [374/800], Loss: 0.0008\n",
      "Epoch [375/800], Loss: 0.0006\n",
      "Epoch [376/800], Loss: 0.0005\n",
      "Epoch [377/800], Loss: 0.0007\n",
      "Epoch [378/800], Loss: 0.0006\n",
      "Epoch [379/800], Loss: 0.0007\n",
      "Epoch [380/800], Loss: 0.0008\n",
      "Epoch [381/800], Loss: 0.0008\n",
      "Epoch [382/800], Loss: 0.0009\n",
      "Epoch [383/800], Loss: 0.0006\n",
      "Epoch [384/800], Loss: 0.0007\n",
      "Epoch [385/800], Loss: 0.0006\n",
      "Epoch [386/800], Loss: 0.0006\n",
      "Epoch [387/800], Loss: 0.0007\n",
      "Epoch [388/800], Loss: 0.0007\n",
      "Epoch [389/800], Loss: 0.0007\n",
      "Epoch [390/800], Loss: 0.0007\n",
      "Epoch [391/800], Loss: 0.0006\n",
      "Epoch [392/800], Loss: 0.0006\n",
      "Epoch [393/800], Loss: 0.0008\n",
      "Epoch [394/800], Loss: 0.0007\n",
      "Epoch [395/800], Loss: 0.0009\n",
      "Epoch [396/800], Loss: 0.0008\n",
      "Epoch [397/800], Loss: 0.0009\n",
      "Epoch [398/800], Loss: 0.0009\n",
      "Epoch [399/800], Loss: 0.0008\n",
      "Epoch [400/800], Loss: 0.0011\n",
      "Epoch [401/800], Loss: 0.0011\n",
      "Epoch [402/800], Loss: 0.0020\n",
      "Epoch [403/800], Loss: 0.0009\n",
      "Epoch [404/800], Loss: 0.0010\n",
      "Epoch [405/800], Loss: 0.0008\n",
      "Epoch [406/800], Loss: 0.0008\n",
      "Epoch [407/800], Loss: 0.0008\n",
      "Epoch [408/800], Loss: 0.0010\n",
      "Epoch [409/800], Loss: 0.0012\n",
      "Epoch [410/800], Loss: 0.0010\n",
      "Epoch [411/800], Loss: 0.0011\n",
      "Epoch [412/800], Loss: 0.0018\n",
      "Epoch [413/800], Loss: 0.0014\n",
      "Epoch [414/800], Loss: 0.0014\n",
      "Epoch [415/800], Loss: 0.0014\n",
      "Epoch [416/800], Loss: 0.0012\n",
      "Epoch [417/800], Loss: 0.0012\n",
      "Epoch [418/800], Loss: 0.0014\n",
      "Epoch [419/800], Loss: 0.0011\n",
      "Epoch [420/800], Loss: 0.0013\n",
      "Epoch [421/800], Loss: 0.0012\n",
      "Epoch [422/800], Loss: 0.0013\n",
      "Epoch [423/800], Loss: 0.0013\n",
      "Epoch [424/800], Loss: 0.0013\n",
      "Epoch [425/800], Loss: 0.0012\n",
      "Epoch [426/800], Loss: 0.0012\n",
      "Epoch [427/800], Loss: 0.0012\n",
      "Epoch [428/800], Loss: 0.0013\n",
      "Epoch [429/800], Loss: 0.0016\n",
      "Epoch [430/800], Loss: 0.0012\n",
      "Epoch [431/800], Loss: 0.0010\n",
      "Epoch [432/800], Loss: 0.0011\n",
      "Epoch [433/800], Loss: 0.0010\n",
      "Epoch [434/800], Loss: 0.0012\n",
      "Epoch [435/800], Loss: 0.0013\n",
      "Epoch [436/800], Loss: 0.0014\n",
      "Epoch [437/800], Loss: 0.0014\n",
      "Epoch [438/800], Loss: 0.0012\n",
      "Epoch [439/800], Loss: 0.0011\n",
      "Epoch [440/800], Loss: 0.0017\n",
      "Epoch [441/800], Loss: 0.0018\n",
      "Epoch [442/800], Loss: 0.0015\n",
      "Epoch [443/800], Loss: 0.0012\n",
      "Epoch [444/800], Loss: 0.0016\n",
      "Epoch [445/800], Loss: 0.0016\n",
      "Epoch [446/800], Loss: 0.0020\n",
      "Epoch [447/800], Loss: 0.0023\n",
      "Epoch [448/800], Loss: 0.0015\n",
      "Epoch [449/800], Loss: 0.0015\n",
      "Epoch [450/800], Loss: 0.0014\n",
      "Epoch [451/800], Loss: 0.0016\n",
      "Epoch [452/800], Loss: 0.0016\n",
      "Epoch [453/800], Loss: 0.0014\n",
      "Epoch [454/800], Loss: 0.0013\n",
      "Epoch [455/800], Loss: 0.0015\n",
      "Epoch [456/800], Loss: 0.0020\n",
      "Epoch [457/800], Loss: 0.0019\n",
      "Epoch [458/800], Loss: 0.0021\n",
      "Epoch [459/800], Loss: 0.0024\n",
      "Epoch [460/800], Loss: 0.0026\n",
      "Epoch [461/800], Loss: 0.0020\n",
      "Epoch [462/800], Loss: 0.0023\n",
      "Epoch [463/800], Loss: 0.0023\n",
      "Epoch [464/800], Loss: 0.0023\n",
      "Epoch [465/800], Loss: 0.0033\n",
      "Epoch [466/800], Loss: 0.0023\n",
      "Epoch [467/800], Loss: 0.0023\n",
      "Epoch [468/800], Loss: 0.0021\n",
      "Epoch [469/800], Loss: 0.0043\n",
      "Epoch [470/800], Loss: 0.0037\n",
      "Epoch [471/800], Loss: 0.0039\n",
      "Epoch [472/800], Loss: 0.0043\n",
      "Epoch [473/800], Loss: 0.0027\n",
      "Epoch [474/800], Loss: 0.0034\n",
      "Epoch [475/800], Loss: 0.0038\n",
      "Epoch [476/800], Loss: 0.0035\n",
      "Epoch [477/800], Loss: 0.0030\n",
      "Epoch [478/800], Loss: 0.0030\n",
      "Epoch [479/800], Loss: 0.0032\n",
      "Epoch [480/800], Loss: 0.0034\n",
      "Epoch [481/800], Loss: 0.0038\n",
      "Epoch [482/800], Loss: 0.0032\n",
      "Epoch [483/800], Loss: 0.0027\n",
      "Epoch [484/800], Loss: 0.0027\n",
      "Epoch [485/800], Loss: 0.0029\n",
      "Epoch [486/800], Loss: 0.0027\n",
      "Epoch [487/800], Loss: 0.0025\n",
      "Epoch [488/800], Loss: 0.0028\n",
      "Epoch [489/800], Loss: 0.0031\n",
      "Epoch [490/800], Loss: 0.0037\n",
      "Epoch [491/800], Loss: 0.0038\n",
      "Epoch [492/800], Loss: 0.0031\n",
      "Epoch [493/800], Loss: 0.0028\n",
      "Epoch [494/800], Loss: 0.0030\n",
      "Epoch [495/800], Loss: 0.0031\n",
      "Epoch [496/800], Loss: 0.0031\n",
      "Epoch [497/800], Loss: 0.0028\n",
      "Epoch [498/800], Loss: 0.0023\n",
      "Epoch [499/800], Loss: 0.0025\n",
      "Epoch [500/800], Loss: 0.0022\n",
      "Epoch [501/800], Loss: 0.0026\n",
      "Epoch [502/800], Loss: 0.0028\n",
      "Epoch [503/800], Loss: 0.0028\n",
      "Epoch [504/800], Loss: 0.0022\n",
      "Epoch [505/800], Loss: 0.0023\n",
      "Epoch [506/800], Loss: 0.0028\n",
      "Epoch [507/800], Loss: 0.0030\n",
      "Epoch [508/800], Loss: 0.0032\n",
      "Epoch [509/800], Loss: 0.0027\n",
      "Epoch [510/800], Loss: 0.0028\n",
      "Epoch [511/800], Loss: 0.0028\n",
      "Epoch [512/800], Loss: 0.0029\n",
      "Epoch [513/800], Loss: 0.0024\n",
      "Epoch [514/800], Loss: 0.0028\n",
      "Epoch [515/800], Loss: 0.0024\n",
      "Epoch [516/800], Loss: 0.0024\n",
      "Epoch [517/800], Loss: 0.0023\n",
      "Epoch [518/800], Loss: 0.0022\n",
      "Epoch [519/800], Loss: 0.0021\n",
      "Epoch [520/800], Loss: 0.0021\n",
      "Epoch [521/800], Loss: 0.0019\n",
      "Epoch [522/800], Loss: 0.0023\n",
      "Epoch [523/800], Loss: 0.0022\n",
      "Epoch [524/800], Loss: 0.0024\n",
      "Epoch [525/800], Loss: 0.0023\n",
      "Epoch [526/800], Loss: 0.0024\n",
      "Epoch [527/800], Loss: 0.0027\n",
      "Epoch [528/800], Loss: 0.0025\n",
      "Epoch [529/800], Loss: 0.0028\n",
      "Epoch [530/800], Loss: 0.0026\n",
      "Epoch [531/800], Loss: 0.0024\n",
      "Epoch [532/800], Loss: 0.0027\n",
      "Epoch [533/800], Loss: 0.0022\n",
      "Epoch [534/800], Loss: 0.0024\n",
      "Epoch [535/800], Loss: 0.0022\n",
      "Epoch [536/800], Loss: 0.0023\n",
      "Epoch [537/800], Loss: 0.0024\n",
      "Epoch [538/800], Loss: 0.0019\n",
      "Epoch [539/800], Loss: 0.0021\n",
      "Epoch [540/800], Loss: 0.0021\n",
      "Epoch [541/800], Loss: 0.0022\n",
      "Epoch [542/800], Loss: 0.0023\n",
      "Epoch [543/800], Loss: 0.0023\n",
      "Epoch [544/800], Loss: 0.0025\n",
      "Epoch [545/800], Loss: 0.0020\n",
      "Epoch [546/800], Loss: 0.0018\n",
      "Epoch [547/800], Loss: 0.0021\n",
      "Epoch [548/800], Loss: 0.0021\n",
      "Epoch [549/800], Loss: 0.0022\n",
      "Epoch [550/800], Loss: 0.0021\n",
      "Epoch [551/800], Loss: 0.0024\n",
      "Epoch [552/800], Loss: 0.0024\n",
      "Epoch [553/800], Loss: 0.0019\n",
      "Epoch [554/800], Loss: 0.0020\n",
      "Epoch [555/800], Loss: 0.0022\n",
      "Epoch [556/800], Loss: 0.0019\n",
      "Epoch [557/800], Loss: 0.0020\n",
      "Epoch [558/800], Loss: 0.0021\n",
      "Epoch [559/800], Loss: 0.0016\n",
      "Epoch [560/800], Loss: 0.0022\n",
      "Epoch [561/800], Loss: 0.0016\n",
      "Epoch [562/800], Loss: 0.0018\n",
      "Epoch [563/800], Loss: 0.0019\n",
      "Epoch [564/800], Loss: 0.0014\n",
      "Epoch [565/800], Loss: 0.0014\n",
      "Epoch [566/800], Loss: 0.0016\n",
      "Epoch [567/800], Loss: 0.0016\n",
      "Epoch [568/800], Loss: 0.0018\n",
      "Epoch [569/800], Loss: 0.0021\n",
      "Epoch [570/800], Loss: 0.0018\n",
      "Epoch [571/800], Loss: 0.0018\n",
      "Epoch [572/800], Loss: 0.0017\n",
      "Epoch [573/800], Loss: 0.0019\n",
      "Epoch [574/800], Loss: 0.0017\n",
      "Epoch [575/800], Loss: 0.0017\n",
      "Epoch [576/800], Loss: 0.0017\n",
      "Epoch [577/800], Loss: 0.0014\n",
      "Epoch [578/800], Loss: 0.0018\n",
      "Epoch [579/800], Loss: 0.0013\n",
      "Epoch [580/800], Loss: 0.0015\n",
      "Epoch [581/800], Loss: 0.0014\n",
      "Epoch [582/800], Loss: 0.0019\n",
      "Epoch [583/800], Loss: 0.0017\n",
      "Epoch [584/800], Loss: 0.0018\n",
      "Epoch [585/800], Loss: 0.0016\n",
      "Epoch [586/800], Loss: 0.0017\n",
      "Epoch [587/800], Loss: 0.0017\n",
      "Epoch [588/800], Loss: 0.0017\n",
      "Epoch [589/800], Loss: 0.0017\n",
      "Epoch [590/800], Loss: 0.0020\n",
      "Epoch [591/800], Loss: 0.0020\n",
      "Epoch [592/800], Loss: 0.0014\n",
      "Epoch [593/800], Loss: 0.0016\n",
      "Epoch [594/800], Loss: 0.0015\n",
      "Epoch [595/800], Loss: 0.0013\n",
      "Epoch [596/800], Loss: 0.0016\n",
      "Epoch [597/800], Loss: 0.0014\n",
      "Epoch [598/800], Loss: 0.0013\n",
      "Epoch [599/800], Loss: 0.0017\n",
      "Epoch [600/800], Loss: 0.0016\n",
      "Epoch [601/800], Loss: 0.0024\n",
      "Epoch [602/800], Loss: 0.0026\n",
      "Epoch [603/800], Loss: 0.0017\n",
      "Epoch [604/800], Loss: 0.0019\n",
      "Epoch [605/800], Loss: 0.0018\n",
      "Epoch [606/800], Loss: 0.0021\n",
      "Epoch [607/800], Loss: 0.0015\n",
      "Epoch [608/800], Loss: 0.0025\n",
      "Epoch [609/800], Loss: 0.0025\n",
      "Epoch [610/800], Loss: 0.0024\n",
      "Epoch [611/800], Loss: 0.0026\n",
      "Epoch [612/800], Loss: 0.0018\n",
      "Epoch [613/800], Loss: 0.0019\n",
      "Epoch [614/800], Loss: 0.0027\n",
      "Epoch [615/800], Loss: 0.0028\n",
      "Epoch [616/800], Loss: 0.0024\n",
      "Epoch [617/800], Loss: 0.0021\n",
      "Epoch [618/800], Loss: 0.0023\n",
      "Epoch [619/800], Loss: 0.0019\n",
      "Epoch [620/800], Loss: 0.0017\n",
      "Epoch [621/800], Loss: 0.0018\n",
      "Epoch [622/800], Loss: 0.0019\n",
      "Epoch [623/800], Loss: 0.0022\n",
      "Epoch [624/800], Loss: 0.0024\n",
      "Epoch [625/800], Loss: 0.0018\n",
      "Epoch [626/800], Loss: 0.0027\n",
      "Epoch [627/800], Loss: 0.0023\n",
      "Epoch [628/800], Loss: 0.0034\n",
      "Epoch [629/800], Loss: 0.0038\n",
      "Epoch [630/800], Loss: 0.0030\n",
      "Epoch [631/800], Loss: 0.0021\n",
      "Epoch [632/800], Loss: 0.0018\n",
      "Epoch [633/800], Loss: 0.0016\n",
      "Epoch [634/800], Loss: 0.0016\n",
      "Epoch [635/800], Loss: 0.0022\n",
      "Epoch [636/800], Loss: 0.0016\n",
      "Epoch [637/800], Loss: 0.0015\n",
      "Epoch [638/800], Loss: 0.0016\n",
      "Epoch [639/800], Loss: 0.0015\n",
      "Epoch [640/800], Loss: 0.0014\n",
      "Epoch [641/800], Loss: 0.0010\n",
      "Epoch [642/800], Loss: 0.0011\n",
      "Epoch [643/800], Loss: 0.0015\n",
      "Epoch [644/800], Loss: 0.0010\n",
      "Epoch [645/800], Loss: 0.0012\n",
      "Epoch [646/800], Loss: 0.0013\n",
      "Epoch [647/800], Loss: 0.0016\n",
      "Epoch [648/800], Loss: 0.0013\n",
      "Epoch [649/800], Loss: 0.0012\n",
      "Epoch [650/800], Loss: 0.0012\n",
      "Epoch [651/800], Loss: 0.0013\n",
      "Epoch [652/800], Loss: 0.0012\n",
      "Epoch [653/800], Loss: 0.0015\n",
      "Epoch [654/800], Loss: 0.0016\n",
      "Epoch [655/800], Loss: 0.0016\n",
      "Epoch [656/800], Loss: 0.0011\n",
      "Epoch [657/800], Loss: 0.0011\n",
      "Epoch [658/800], Loss: 0.0015\n",
      "Epoch [659/800], Loss: 0.0017\n",
      "Epoch [660/800], Loss: 0.0017\n",
      "Epoch [661/800], Loss: 0.0015\n",
      "Epoch [662/800], Loss: 0.0016\n",
      "Epoch [663/800], Loss: 0.0016\n",
      "Epoch [664/800], Loss: 0.0014\n",
      "Epoch [665/800], Loss: 0.0022\n",
      "Epoch [666/800], Loss: 0.0017\n",
      "Epoch [667/800], Loss: 0.0020\n",
      "Epoch [668/800], Loss: 0.0014\n",
      "Epoch [669/800], Loss: 0.0013\n",
      "Epoch [670/800], Loss: 0.0016\n",
      "Epoch [671/800], Loss: 0.0017\n",
      "Epoch [672/800], Loss: 0.0021\n",
      "Epoch [673/800], Loss: 0.0016\n",
      "Epoch [674/800], Loss: 0.0014\n",
      "Epoch [675/800], Loss: 0.0019\n",
      "Epoch [676/800], Loss: 0.0020\n",
      "Epoch [677/800], Loss: 0.0026\n",
      "Epoch [678/800], Loss: 0.0009\n",
      "Epoch [679/800], Loss: 0.0012\n",
      "Epoch [680/800], Loss: 0.0014\n",
      "Epoch [681/800], Loss: 0.0011\n",
      "Epoch [682/800], Loss: 0.0020\n",
      "Epoch [683/800], Loss: 0.0021\n",
      "Epoch [684/800], Loss: 0.0020\n",
      "Epoch [685/800], Loss: 0.0026\n",
      "Epoch [686/800], Loss: 0.0022\n",
      "Epoch [687/800], Loss: 0.0013\n",
      "Epoch [688/800], Loss: 0.0025\n",
      "Epoch [689/800], Loss: 0.0022\n",
      "Epoch [690/800], Loss: 0.0022\n",
      "Epoch [691/800], Loss: 0.0016\n",
      "Epoch [692/800], Loss: 0.0019\n",
      "Epoch [693/800], Loss: 0.0023\n",
      "Epoch [694/800], Loss: 0.0018\n",
      "Epoch [695/800], Loss: 0.0029\n",
      "Epoch [696/800], Loss: nan\n",
      "Epoch [697/800], Loss: nan\n",
      "Epoch [698/800], Loss: nan\n",
      "Epoch [699/800], Loss: nan\n",
      "Epoch [700/800], Loss: nan\n",
      "Epoch [701/800], Loss: nan\n",
      "Epoch [702/800], Loss: nan\n",
      "Epoch [703/800], Loss: nan\n",
      "Epoch [704/800], Loss: nan\n",
      "Epoch [705/800], Loss: nan\n",
      "Epoch [706/800], Loss: nan\n",
      "Epoch [707/800], Loss: nan\n",
      "Epoch [708/800], Loss: nan\n",
      "Epoch [709/800], Loss: nan\n",
      "Epoch [710/800], Loss: nan\n",
      "Epoch [711/800], Loss: nan\n",
      "Epoch [712/800], Loss: nan\n",
      "Epoch [713/800], Loss: nan\n",
      "Epoch [714/800], Loss: nan\n",
      "Epoch [715/800], Loss: nan\n",
      "Epoch [716/800], Loss: nan\n",
      "Epoch [717/800], Loss: nan\n",
      "Epoch [718/800], Loss: nan\n",
      "Epoch [719/800], Loss: nan\n",
      "Epoch [720/800], Loss: nan\n",
      "Epoch [721/800], Loss: nan\n",
      "Epoch [722/800], Loss: nan\n",
      "Epoch [723/800], Loss: nan\n",
      "Epoch [724/800], Loss: nan\n",
      "Epoch [725/800], Loss: nan\n",
      "Epoch [726/800], Loss: nan\n",
      "Epoch [727/800], Loss: nan\n",
      "Epoch [728/800], Loss: nan\n",
      "Epoch [729/800], Loss: nan\n",
      "Epoch [730/800], Loss: nan\n",
      "Epoch [731/800], Loss: nan\n",
      "Epoch [732/800], Loss: nan\n",
      "Epoch [733/800], Loss: nan\n",
      "Epoch [734/800], Loss: nan\n",
      "Epoch [735/800], Loss: nan\n",
      "Epoch [736/800], Loss: nan\n",
      "Epoch [737/800], Loss: nan\n",
      "Epoch [738/800], Loss: nan\n",
      "Epoch [739/800], Loss: nan\n",
      "Epoch [740/800], Loss: nan\n",
      "Epoch [741/800], Loss: nan\n",
      "Epoch [742/800], Loss: nan\n",
      "Epoch [743/800], Loss: nan\n",
      "Epoch [744/800], Loss: nan\n",
      "Epoch [745/800], Loss: nan\n",
      "Epoch [746/800], Loss: nan\n",
      "Epoch [747/800], Loss: nan\n",
      "Epoch [748/800], Loss: nan\n",
      "Epoch [749/800], Loss: nan\n",
      "Epoch [750/800], Loss: nan\n",
      "Epoch [751/800], Loss: nan\n",
      "Epoch [752/800], Loss: nan\n",
      "Epoch [753/800], Loss: nan\n",
      "Epoch [754/800], Loss: nan\n",
      "Epoch [755/800], Loss: nan\n",
      "Epoch [756/800], Loss: nan\n",
      "Epoch [757/800], Loss: nan\n",
      "Epoch [758/800], Loss: nan\n",
      "Epoch [759/800], Loss: nan\n",
      "Epoch [760/800], Loss: nan\n",
      "Epoch [761/800], Loss: nan\n",
      "Epoch [762/800], Loss: nan\n",
      "Epoch [763/800], Loss: nan\n",
      "Epoch [764/800], Loss: nan\n",
      "Epoch [765/800], Loss: nan\n",
      "Epoch [766/800], Loss: nan\n",
      "Epoch [767/800], Loss: nan\n",
      "Epoch [768/800], Loss: nan\n",
      "Epoch [769/800], Loss: nan\n",
      "Epoch [770/800], Loss: nan\n",
      "Epoch [771/800], Loss: nan\n",
      "Epoch [772/800], Loss: nan\n",
      "Epoch [773/800], Loss: nan\n",
      "Epoch [774/800], Loss: nan\n",
      "Epoch [775/800], Loss: nan\n",
      "Epoch [776/800], Loss: nan\n",
      "Epoch [777/800], Loss: nan\n",
      "Epoch [778/800], Loss: nan\n",
      "Epoch [779/800], Loss: nan\n",
      "Epoch [780/800], Loss: nan\n",
      "Epoch [781/800], Loss: nan\n",
      "Epoch [782/800], Loss: nan\n",
      "Epoch [783/800], Loss: nan\n",
      "Epoch [784/800], Loss: nan\n",
      "Epoch [785/800], Loss: nan\n",
      "Epoch [786/800], Loss: nan\n",
      "Epoch [787/800], Loss: nan\n",
      "Epoch [788/800], Loss: nan\n",
      "Epoch [789/800], Loss: nan\n",
      "Epoch [790/800], Loss: nan\n",
      "Epoch [791/800], Loss: nan\n",
      "Epoch [792/800], Loss: nan\n",
      "Epoch [793/800], Loss: nan\n",
      "Epoch [794/800], Loss: nan\n",
      "Epoch [795/800], Loss: nan\n",
      "Epoch [796/800], Loss: nan\n",
      "Epoch [797/800], Loss: nan\n",
      "Epoch [798/800], Loss: nan\n",
      "Epoch [799/800], Loss: nan\n",
      "Epoch [800/800], Loss: nan\n",
      "Accuracy: 50.0000%\n",
      "Epoch [1/800], Loss: 0.0004\n",
      "Epoch [2/800], Loss: 0.0007\n",
      "Epoch [3/800], Loss: 0.0022\n",
      "Epoch [4/800], Loss: 0.0045\n",
      "Epoch [5/800], Loss: 0.0078\n",
      "Epoch [6/800], Loss: 0.0043\n",
      "Epoch [7/800], Loss: 0.0084\n",
      "Epoch [8/800], Loss: 0.0127\n",
      "Epoch [9/800], Loss: 0.0031\n",
      "Epoch [10/800], Loss: 0.0061\n",
      "Epoch [11/800], Loss: 0.0065\n",
      "Epoch [12/800], Loss: 0.0068\n",
      "Epoch [13/800], Loss: 0.0107\n",
      "Epoch [14/800], Loss: 0.0069\n",
      "Epoch [15/800], Loss: 0.0103\n",
      "Epoch [16/800], Loss: 0.0080\n",
      "Epoch [17/800], Loss: 0.0100\n",
      "Epoch [18/800], Loss: 0.0014\n",
      "Epoch [19/800], Loss: 0.0042\n",
      "Epoch [20/800], Loss: 0.0062\n",
      "Epoch [21/800], Loss: 0.0060\n",
      "Epoch [22/800], Loss: 0.0026\n",
      "Epoch [23/800], Loss: 0.0036\n",
      "Epoch [24/800], Loss: 0.0054\n",
      "Epoch [25/800], Loss: 0.0016\n",
      "Epoch [26/800], Loss: 0.0031\n",
      "Epoch [27/800], Loss: 0.0018\n",
      "Epoch [28/800], Loss: 0.0032\n",
      "Epoch [29/800], Loss: 0.0001\n",
      "Epoch [30/800], Loss: 0.0010\n",
      "Epoch [31/800], Loss: 0.0032\n",
      "Epoch [32/800], Loss: 0.0008\n",
      "Epoch [33/800], Loss: 0.0024\n",
      "Epoch [34/800], Loss: 0.0001\n",
      "Epoch [35/800], Loss: 0.0016\n",
      "Epoch [36/800], Loss: 0.0006\n",
      "Epoch [37/800], Loss: 0.0023\n",
      "Epoch [38/800], Loss: 0.0028\n",
      "Epoch [39/800], Loss: 0.0040\n",
      "Epoch [40/800], Loss: 0.0002\n",
      "Epoch [41/800], Loss: 0.0003\n",
      "Epoch [42/800], Loss: 0.0003\n",
      "Epoch [43/800], Loss: 0.0027\n",
      "Epoch [44/800], Loss: 0.0025\n",
      "Epoch [45/800], Loss: 0.0022\n",
      "Epoch [46/800], Loss: 0.0020\n",
      "Epoch [47/800], Loss: 0.0013\n",
      "Epoch [48/800], Loss: 0.0007\n",
      "Epoch [49/800], Loss: 0.0009\n",
      "Epoch [50/800], Loss: 0.0019\n",
      "Epoch [51/800], Loss: 0.0031\n",
      "Epoch [52/800], Loss: 0.0007\n",
      "Epoch [53/800], Loss: 0.0007\n",
      "Epoch [54/800], Loss: 0.0062\n",
      "Epoch [55/800], Loss: 0.0005\n",
      "Epoch [56/800], Loss: 0.0049\n",
      "Epoch [57/800], Loss: 0.0084\n",
      "Epoch [58/800], Loss: 0.0055\n",
      "Epoch [59/800], Loss: 0.0039\n",
      "Epoch [60/800], Loss: 0.0062\n",
      "Epoch [61/800], Loss: 0.0081\n",
      "Epoch [62/800], Loss: 0.0102\n",
      "Epoch [63/800], Loss: 0.0117\n",
      "Epoch [64/800], Loss: 0.0043\n",
      "Epoch [65/800], Loss: 0.0030\n",
      "Epoch [66/800], Loss: 0.0108\n",
      "Epoch [67/800], Loss: 0.0056\n",
      "Epoch [68/800], Loss: 0.0051\n",
      "Epoch [69/800], Loss: 0.0078\n",
      "Epoch [70/800], Loss: 0.0089\n",
      "Epoch [71/800], Loss: 0.0075\n",
      "Epoch [72/800], Loss: 0.0075\n",
      "Epoch [73/800], Loss: 0.0059\n",
      "Epoch [74/800], Loss: 0.0025\n",
      "Epoch [75/800], Loss: 0.0011\n",
      "Epoch [76/800], Loss: 0.0090\n",
      "Epoch [77/800], Loss: 0.0068\n",
      "Epoch [78/800], Loss: 0.0077\n",
      "Epoch [79/800], Loss: 0.0051\n",
      "Epoch [80/800], Loss: 0.0033\n",
      "Epoch [81/800], Loss: 0.0024\n",
      "Epoch [82/800], Loss: 0.0025\n",
      "Epoch [83/800], Loss: 0.0032\n",
      "Epoch [84/800], Loss: 0.0033\n",
      "Epoch [85/800], Loss: 0.0032\n",
      "Epoch [86/800], Loss: 0.0031\n",
      "Epoch [87/800], Loss: 0.0031\n",
      "Epoch [88/800], Loss: 0.0031\n",
      "Epoch [89/800], Loss: 0.0031\n",
      "Epoch [90/800], Loss: 0.0031\n",
      "Epoch [91/800], Loss: 0.0030\n",
      "Epoch [92/800], Loss: 0.0028\n",
      "Epoch [93/800], Loss: 0.0026\n",
      "Epoch [94/800], Loss: 0.0024\n",
      "Epoch [95/800], Loss: 0.0021\n",
      "Epoch [96/800], Loss: 0.0019\n",
      "Epoch [97/800], Loss: 0.0017\n",
      "Epoch [98/800], Loss: 0.0016\n",
      "Epoch [99/800], Loss: 0.0014\n",
      "Epoch [100/800], Loss: 0.0013\n",
      "Epoch [101/800], Loss: 0.0012\n",
      "Epoch [102/800], Loss: 0.0010\n",
      "Epoch [103/800], Loss: 0.0008\n",
      "Epoch [104/800], Loss: 0.0007\n",
      "Epoch [105/800], Loss: 0.0007\n",
      "Epoch [106/800], Loss: 0.0006\n",
      "Epoch [107/800], Loss: 0.0006\n",
      "Epoch [108/800], Loss: 0.0006\n",
      "Epoch [109/800], Loss: 0.0006\n",
      "Epoch [110/800], Loss: 0.0006\n",
      "Epoch [111/800], Loss: 0.0006\n",
      "Epoch [112/800], Loss: 0.0006\n",
      "Epoch [113/800], Loss: 0.0006\n",
      "Epoch [114/800], Loss: 0.0006\n",
      "Epoch [115/800], Loss: 0.0006\n",
      "Epoch [116/800], Loss: 0.0006\n",
      "Epoch [117/800], Loss: 0.0006\n",
      "Epoch [118/800], Loss: 0.0006\n",
      "Epoch [119/800], Loss: 0.0006\n",
      "Epoch [120/800], Loss: 0.0007\n",
      "Epoch [121/800], Loss: 0.0008\n",
      "Epoch [122/800], Loss: 0.0008\n",
      "Epoch [123/800], Loss: 0.0007\n",
      "Epoch [124/800], Loss: 0.0007\n",
      "Epoch [125/800], Loss: 0.0007\n",
      "Epoch [126/800], Loss: 0.0006\n",
      "Epoch [127/800], Loss: 0.0007\n",
      "Epoch [128/800], Loss: 0.0007\n",
      "Epoch [129/800], Loss: 0.0006\n",
      "Epoch [130/800], Loss: 0.0007\n",
      "Epoch [131/800], Loss: 0.0008\n",
      "Epoch [132/800], Loss: 0.0009\n",
      "Epoch [133/800], Loss: 0.0009\n",
      "Epoch [134/800], Loss: 0.0010\n",
      "Epoch [135/800], Loss: 0.0012\n",
      "Epoch [136/800], Loss: 0.0011\n",
      "Epoch [137/800], Loss: 0.0017\n",
      "Epoch [138/800], Loss: 0.0014\n",
      "Epoch [139/800], Loss: 0.0012\n",
      "Epoch [140/800], Loss: 0.0013\n",
      "Epoch [141/800], Loss: 0.0019\n",
      "Epoch [142/800], Loss: 0.0014\n",
      "Epoch [143/800], Loss: 0.0011\n",
      "Epoch [144/800], Loss: 0.0011\n",
      "Epoch [145/800], Loss: 0.0012\n",
      "Epoch [146/800], Loss: 0.0010\n",
      "Epoch [147/800], Loss: 0.0011\n",
      "Epoch [148/800], Loss: 0.0010\n",
      "Epoch [149/800], Loss: 0.0008\n",
      "Epoch [150/800], Loss: 0.0010\n",
      "Epoch [151/800], Loss: 0.0010\n",
      "Epoch [152/800], Loss: 0.0011\n",
      "Epoch [153/800], Loss: 0.0010\n",
      "Epoch [154/800], Loss: 0.0007\n",
      "Epoch [155/800], Loss: 0.0007\n",
      "Epoch [156/800], Loss: 0.0007\n",
      "Epoch [157/800], Loss: 0.0003\n",
      "Epoch [158/800], Loss: 0.0004\n",
      "Epoch [159/800], Loss: 0.0004\n",
      "Epoch [160/800], Loss: 0.0003\n",
      "Epoch [161/800], Loss: 0.0003\n",
      "Epoch [162/800], Loss: 0.0003\n",
      "Epoch [163/800], Loss: 0.0003\n",
      "Epoch [164/800], Loss: 0.0003\n",
      "Epoch [165/800], Loss: 0.0002\n",
      "Epoch [166/800], Loss: 0.0002\n",
      "Epoch [167/800], Loss: 0.0002\n",
      "Epoch [168/800], Loss: 0.0002\n",
      "Epoch [169/800], Loss: 0.0002\n",
      "Epoch [170/800], Loss: 0.0003\n",
      "Epoch [171/800], Loss: 0.0001\n",
      "Epoch [172/800], Loss: 0.0001\n",
      "Epoch [173/800], Loss: 0.0002\n",
      "Epoch [174/800], Loss: 0.0001\n",
      "Epoch [175/800], Loss: 0.0001\n",
      "Epoch [176/800], Loss: 0.0001\n",
      "Epoch [177/800], Loss: 0.0001\n",
      "Epoch [178/800], Loss: 0.0002\n",
      "Epoch [179/800], Loss: 0.0002\n",
      "Epoch [180/800], Loss: 0.0001\n",
      "Epoch [181/800], Loss: 0.0001\n",
      "Epoch [182/800], Loss: 0.0001\n",
      "Epoch [183/800], Loss: 0.0001\n",
      "Epoch [184/800], Loss: 0.0001\n",
      "Epoch [185/800], Loss: 0.0001\n",
      "Epoch [186/800], Loss: 0.0001\n",
      "Epoch [187/800], Loss: 0.0001\n",
      "Epoch [188/800], Loss: 0.0001\n",
      "Epoch [189/800], Loss: 0.0001\n",
      "Epoch [190/800], Loss: 0.0001\n",
      "Epoch [191/800], Loss: 0.0001\n",
      "Epoch [192/800], Loss: 0.0001\n",
      "Epoch [193/800], Loss: 0.0001\n",
      "Epoch [194/800], Loss: 0.0000\n",
      "Epoch [195/800], Loss: 0.0000\n",
      "Epoch [196/800], Loss: 0.0000\n",
      "Epoch [197/800], Loss: 0.0000\n",
      "Epoch [198/800], Loss: 0.0000\n",
      "Epoch [199/800], Loss: 0.0000\n",
      "Epoch [200/800], Loss: 0.0000\n",
      "Epoch [201/800], Loss: 0.0000\n",
      "Epoch [202/800], Loss: 0.0000\n",
      "Epoch [203/800], Loss: 0.0000\n",
      "Epoch [204/800], Loss: 0.0000\n",
      "Epoch [205/800], Loss: 0.0000\n",
      "Epoch [206/800], Loss: 0.0000\n",
      "Epoch [207/800], Loss: 0.0000\n",
      "Epoch [208/800], Loss: 0.0000\n",
      "Epoch [209/800], Loss: 0.0000\n",
      "Epoch [210/800], Loss: 0.0000\n",
      "Epoch [211/800], Loss: 0.0000\n",
      "Epoch [212/800], Loss: 0.0000\n",
      "Epoch [213/800], Loss: 0.0000\n",
      "Epoch [214/800], Loss: 0.0000\n",
      "Epoch [215/800], Loss: 0.0000\n",
      "Epoch [216/800], Loss: 0.0000\n",
      "Epoch [217/800], Loss: 0.0000\n",
      "Epoch [218/800], Loss: 0.0000\n",
      "Epoch [219/800], Loss: 0.0000\n",
      "Epoch [220/800], Loss: 0.0000\n",
      "Epoch [221/800], Loss: 0.0000\n",
      "Epoch [222/800], Loss: 0.0000\n",
      "Epoch [223/800], Loss: 0.0000\n",
      "Epoch [224/800], Loss: 0.0000\n",
      "Epoch [225/800], Loss: 0.0000\n",
      "Epoch [226/800], Loss: 0.0000\n",
      "Epoch [227/800], Loss: 0.0000\n",
      "Epoch [228/800], Loss: 0.0000\n",
      "Epoch [229/800], Loss: 0.0000\n",
      "Epoch [230/800], Loss: 0.0000\n",
      "Epoch [231/800], Loss: 0.0000\n",
      "Epoch [232/800], Loss: 0.0000\n",
      "Epoch [233/800], Loss: 0.0000\n",
      "Epoch [234/800], Loss: 0.0000\n",
      "Epoch [235/800], Loss: 0.0000\n",
      "Epoch [236/800], Loss: 0.0000\n",
      "Epoch [237/800], Loss: 0.0000\n",
      "Epoch [238/800], Loss: 0.0000\n",
      "Epoch [239/800], Loss: 0.0000\n",
      "Epoch [240/800], Loss: 0.0000\n",
      "Epoch [241/800], Loss: 0.0000\n",
      "Epoch [242/800], Loss: 0.0000\n",
      "Epoch [243/800], Loss: 0.0000\n",
      "Epoch [244/800], Loss: 0.0000\n",
      "Epoch [245/800], Loss: 0.0000\n",
      "Epoch [246/800], Loss: 0.0000\n",
      "Epoch [247/800], Loss: 0.0000\n",
      "Epoch [248/800], Loss: 0.0000\n",
      "Epoch [249/800], Loss: 0.0000\n",
      "Epoch [250/800], Loss: 0.0000\n",
      "Epoch [251/800], Loss: 0.0000\n",
      "Epoch [252/800], Loss: 0.0000\n",
      "Epoch [253/800], Loss: 0.0000\n",
      "Epoch [254/800], Loss: 0.0000\n",
      "Epoch [255/800], Loss: 0.0000\n",
      "Epoch [256/800], Loss: 0.0000\n",
      "Epoch [257/800], Loss: 0.0000\n",
      "Epoch [258/800], Loss: 0.0000\n",
      "Epoch [259/800], Loss: 0.0000\n",
      "Epoch [260/800], Loss: 0.0000\n",
      "Epoch [261/800], Loss: 0.0000\n",
      "Epoch [262/800], Loss: 0.0000\n",
      "Epoch [263/800], Loss: 0.0000\n",
      "Epoch [264/800], Loss: 0.0000\n",
      "Epoch [265/800], Loss: 0.0000\n",
      "Epoch [266/800], Loss: 0.0000\n",
      "Epoch [267/800], Loss: 0.0000\n",
      "Epoch [268/800], Loss: 0.0000\n",
      "Epoch [269/800], Loss: 0.0000\n",
      "Epoch [270/800], Loss: 0.0000\n",
      "Epoch [271/800], Loss: 0.0000\n",
      "Epoch [272/800], Loss: 0.0000\n",
      "Epoch [273/800], Loss: 0.0000\n",
      "Epoch [274/800], Loss: 0.0000\n",
      "Epoch [275/800], Loss: 0.0000\n",
      "Epoch [276/800], Loss: 0.0000\n",
      "Epoch [277/800], Loss: 0.0000\n",
      "Epoch [278/800], Loss: 0.0000\n",
      "Epoch [279/800], Loss: 0.0000\n",
      "Epoch [280/800], Loss: 0.0000\n",
      "Epoch [281/800], Loss: 0.0000\n",
      "Epoch [282/800], Loss: 0.0000\n",
      "Epoch [283/800], Loss: 0.0000\n",
      "Epoch [284/800], Loss: 0.0001\n",
      "Epoch [285/800], Loss: 0.0001\n",
      "Epoch [286/800], Loss: 0.0001\n",
      "Epoch [287/800], Loss: 0.0000\n",
      "Epoch [288/800], Loss: 0.0000\n",
      "Epoch [289/800], Loss: 0.0000\n",
      "Epoch [290/800], Loss: 0.0000\n",
      "Epoch [291/800], Loss: 0.0000\n",
      "Epoch [292/800], Loss: 0.0000\n",
      "Epoch [293/800], Loss: 0.0000\n",
      "Epoch [294/800], Loss: 0.0000\n",
      "Epoch [295/800], Loss: 0.0000\n",
      "Epoch [296/800], Loss: 0.0000\n",
      "Epoch [297/800], Loss: 0.0000\n",
      "Epoch [298/800], Loss: 0.0000\n",
      "Epoch [299/800], Loss: 0.0000\n",
      "Epoch [300/800], Loss: 0.0000\n",
      "Epoch [301/800], Loss: 0.0000\n",
      "Epoch [302/800], Loss: 0.0000\n",
      "Epoch [303/800], Loss: 0.0000\n",
      "Epoch [304/800], Loss: 0.0000\n",
      "Epoch [305/800], Loss: 0.0000\n",
      "Epoch [306/800], Loss: 0.0000\n",
      "Epoch [307/800], Loss: 0.0000\n",
      "Epoch [308/800], Loss: 0.0000\n",
      "Epoch [309/800], Loss: 0.0000\n",
      "Epoch [310/800], Loss: 0.0000\n",
      "Epoch [311/800], Loss: 0.0000\n",
      "Epoch [312/800], Loss: 0.0000\n",
      "Epoch [313/800], Loss: 0.0000\n",
      "Epoch [314/800], Loss: 0.0000\n",
      "Epoch [315/800], Loss: 0.0000\n",
      "Epoch [316/800], Loss: 0.0000\n",
      "Epoch [317/800], Loss: 0.0000\n",
      "Epoch [318/800], Loss: 0.0000\n",
      "Epoch [319/800], Loss: 0.0000\n",
      "Epoch [320/800], Loss: 0.0000\n",
      "Epoch [321/800], Loss: 0.0000\n",
      "Epoch [322/800], Loss: 0.0000\n",
      "Epoch [323/800], Loss: 0.0000\n",
      "Epoch [324/800], Loss: 0.0000\n",
      "Epoch [325/800], Loss: 0.0000\n",
      "Epoch [326/800], Loss: 0.0000\n",
      "Epoch [327/800], Loss: 0.0000\n",
      "Epoch [328/800], Loss: 0.0000\n",
      "Epoch [329/800], Loss: 0.0000\n",
      "Epoch [330/800], Loss: 0.0000\n",
      "Epoch [331/800], Loss: 0.0000\n",
      "Epoch [332/800], Loss: 0.0000\n",
      "Epoch [333/800], Loss: 0.0000\n",
      "Epoch [334/800], Loss: 0.0000\n",
      "Epoch [335/800], Loss: 0.0000\n",
      "Epoch [336/800], Loss: 0.0000\n",
      "Epoch [337/800], Loss: 0.0000\n",
      "Epoch [338/800], Loss: 0.0000\n",
      "Epoch [339/800], Loss: 0.0000\n",
      "Epoch [340/800], Loss: 0.0000\n",
      "Epoch [341/800], Loss: 0.0000\n",
      "Epoch [342/800], Loss: 0.0000\n",
      "Epoch [343/800], Loss: 0.0000\n",
      "Epoch [344/800], Loss: 0.0000\n",
      "Epoch [345/800], Loss: 0.0000\n",
      "Epoch [346/800], Loss: 0.0000\n",
      "Epoch [347/800], Loss: 0.0000\n",
      "Epoch [348/800], Loss: 0.0000\n",
      "Epoch [349/800], Loss: 0.0000\n",
      "Epoch [350/800], Loss: 0.0000\n",
      "Epoch [351/800], Loss: 0.0000\n",
      "Epoch [352/800], Loss: 0.0000\n",
      "Epoch [353/800], Loss: 0.0000\n",
      "Epoch [354/800], Loss: 0.0000\n",
      "Epoch [355/800], Loss: 0.0000\n",
      "Epoch [356/800], Loss: 0.0000\n",
      "Epoch [357/800], Loss: 0.0000\n",
      "Epoch [358/800], Loss: 0.0000\n",
      "Epoch [359/800], Loss: 0.0000\n",
      "Epoch [360/800], Loss: 0.0000\n",
      "Epoch [361/800], Loss: 0.0000\n",
      "Epoch [362/800], Loss: 0.0000\n",
      "Epoch [363/800], Loss: 0.0000\n",
      "Epoch [364/800], Loss: 0.0000\n",
      "Epoch [365/800], Loss: 0.0000\n",
      "Epoch [366/800], Loss: 0.0000\n",
      "Epoch [367/800], Loss: 0.0000\n",
      "Epoch [368/800], Loss: 0.0000\n",
      "Epoch [369/800], Loss: 0.0000\n",
      "Epoch [370/800], Loss: 0.0000\n",
      "Epoch [371/800], Loss: 0.0000\n",
      "Epoch [372/800], Loss: 0.0000\n",
      "Epoch [373/800], Loss: 0.0000\n",
      "Epoch [374/800], Loss: 0.0000\n",
      "Epoch [375/800], Loss: 0.0000\n",
      "Epoch [376/800], Loss: 0.0000\n",
      "Epoch [377/800], Loss: 0.0000\n",
      "Epoch [378/800], Loss: 0.0000\n",
      "Epoch [379/800], Loss: 0.0000\n",
      "Epoch [380/800], Loss: 0.0000\n",
      "Epoch [381/800], Loss: 0.0000\n",
      "Epoch [382/800], Loss: 0.0000\n",
      "Epoch [383/800], Loss: 0.0000\n",
      "Epoch [384/800], Loss: 0.0000\n",
      "Epoch [385/800], Loss: 0.0000\n",
      "Epoch [386/800], Loss: 0.0000\n",
      "Epoch [387/800], Loss: 0.0000\n",
      "Epoch [388/800], Loss: 0.0000\n",
      "Epoch [389/800], Loss: 0.0000\n",
      "Epoch [390/800], Loss: 0.0000\n",
      "Epoch [391/800], Loss: 0.0000\n",
      "Epoch [392/800], Loss: 0.0000\n",
      "Epoch [393/800], Loss: 0.0000\n",
      "Epoch [394/800], Loss: 0.0000\n",
      "Epoch [395/800], Loss: 0.0000\n",
      "Epoch [396/800], Loss: 0.0000\n",
      "Epoch [397/800], Loss: 0.0000\n",
      "Epoch [398/800], Loss: 0.0000\n",
      "Epoch [399/800], Loss: 0.0000\n",
      "Epoch [400/800], Loss: 0.0005\n",
      "Epoch [401/800], Loss: 0.0000\n",
      "Epoch [402/800], Loss: 0.0002\n",
      "Epoch [403/800], Loss: 0.0001\n",
      "Epoch [404/800], Loss: 0.0000\n",
      "Epoch [405/800], Loss: 0.0000\n",
      "Epoch [406/800], Loss: 0.0002\n",
      "Epoch [407/800], Loss: 0.0005\n",
      "Epoch [408/800], Loss: 0.0005\n",
      "Epoch [409/800], Loss: 0.0000\n",
      "Epoch [410/800], Loss: 0.0000\n",
      "Epoch [411/800], Loss: 0.0000\n",
      "Epoch [412/800], Loss: 0.0000\n",
      "Epoch [413/800], Loss: 0.0000\n",
      "Epoch [414/800], Loss: 0.0009\n",
      "Epoch [415/800], Loss: 0.0000\n",
      "Epoch [416/800], Loss: 0.0010\n",
      "Epoch [417/800], Loss: 0.0007\n",
      "Epoch [418/800], Loss: 0.0000\n",
      "Epoch [419/800], Loss: 0.0000\n",
      "Epoch [420/800], Loss: 0.0000\n",
      "Epoch [421/800], Loss: 0.0000\n",
      "Epoch [422/800], Loss: 0.0000\n",
      "Epoch [423/800], Loss: 0.0000\n",
      "Epoch [424/800], Loss: 0.0001\n",
      "Epoch [425/800], Loss: 0.0005\n",
      "Epoch [426/800], Loss: 0.0000\n",
      "Epoch [427/800], Loss: 0.0000\n",
      "Epoch [428/800], Loss: 0.0000\n",
      "Epoch [429/800], Loss: 0.0000\n",
      "Epoch [430/800], Loss: 0.0000\n",
      "Epoch [431/800], Loss: 0.0000\n",
      "Epoch [432/800], Loss: 0.0000\n",
      "Epoch [433/800], Loss: 0.0000\n",
      "Epoch [434/800], Loss: 0.0001\n",
      "Epoch [435/800], Loss: 0.0000\n",
      "Epoch [436/800], Loss: 0.0029\n",
      "Epoch [437/800], Loss: 0.0016\n",
      "Epoch [438/800], Loss: 0.0031\n",
      "Epoch [439/800], Loss: 0.0001\n",
      "Epoch [440/800], Loss: 0.0000\n",
      "Epoch [441/800], Loss: 0.0013\n",
      "Epoch [442/800], Loss: 0.0021\n",
      "Epoch [443/800], Loss: 0.0006\n",
      "Epoch [444/800], Loss: 0.0008\n",
      "Epoch [445/800], Loss: 0.0030\n",
      "Epoch [446/800], Loss: 0.0000\n",
      "Epoch [447/800], Loss: 0.0000\n",
      "Epoch [448/800], Loss: 0.0000\n",
      "Epoch [449/800], Loss: 0.0000\n",
      "Epoch [450/800], Loss: 0.0000\n",
      "Epoch [451/800], Loss: 0.0002\n",
      "Epoch [452/800], Loss: 0.0118\n",
      "Epoch [453/800], Loss: 0.0000\n",
      "Epoch [454/800], Loss: 0.0028\n",
      "Epoch [455/800], Loss: 0.0041\n",
      "Epoch [456/800], Loss: 0.0037\n",
      "Epoch [457/800], Loss: 0.0078\n",
      "Epoch [458/800], Loss: 0.0076\n",
      "Epoch [459/800], Loss: 0.0247\n",
      "Epoch [460/800], Loss: 0.0039\n",
      "Epoch [461/800], Loss: 0.1535\n",
      "Epoch [462/800], Loss: 0.0137\n",
      "Epoch [463/800], Loss: 0.0329\n",
      "Epoch [464/800], Loss: 0.0083\n",
      "Epoch [465/800], Loss: 0.0080\n",
      "Epoch [466/800], Loss: 0.0004\n",
      "Epoch [467/800], Loss: 0.0001\n",
      "Epoch [468/800], Loss: 0.0000\n",
      "Epoch [469/800], Loss: 0.0000\n",
      "Epoch [470/800], Loss: 0.0000\n",
      "Epoch [471/800], Loss: 0.0000\n",
      "Epoch [472/800], Loss: 0.0000\n",
      "Epoch [473/800], Loss: 0.0000\n",
      "Epoch [474/800], Loss: 0.0000\n",
      "Epoch [475/800], Loss: 0.0000\n",
      "Epoch [476/800], Loss: 0.0000\n",
      "Epoch [477/800], Loss: 0.0000\n",
      "Epoch [478/800], Loss: 0.0000\n",
      "Epoch [479/800], Loss: 0.0004\n",
      "Epoch [480/800], Loss: 0.0015\n",
      "Epoch [481/800], Loss: 0.0000\n",
      "Epoch [482/800], Loss: 0.0000\n",
      "Epoch [483/800], Loss: 0.0000\n",
      "Epoch [484/800], Loss: 0.0001\n",
      "Epoch [485/800], Loss: 0.0002\n",
      "Epoch [486/800], Loss: 0.0001\n",
      "Epoch [487/800], Loss: 0.0000\n",
      "Epoch [488/800], Loss: 0.0022\n",
      "Epoch [489/800], Loss: 0.0132\n",
      "Epoch [490/800], Loss: 0.0002\n",
      "Epoch [491/800], Loss: 0.0000\n",
      "Epoch [492/800], Loss: 0.0002\n",
      "Epoch [493/800], Loss: 0.0000\n",
      "Epoch [494/800], Loss: 0.0000\n",
      "Epoch [495/800], Loss: 0.0000\n",
      "Epoch [496/800], Loss: 0.0000\n",
      "Epoch [497/800], Loss: 0.0000\n",
      "Epoch [498/800], Loss: 0.0000\n",
      "Epoch [499/800], Loss: 0.0000\n",
      "Epoch [500/800], Loss: 0.0000\n",
      "Epoch [501/800], Loss: 0.0000\n",
      "Epoch [502/800], Loss: 0.0000\n",
      "Epoch [503/800], Loss: 0.0011\n",
      "Epoch [504/800], Loss: 0.0000\n",
      "Epoch [505/800], Loss: 0.0000\n",
      "Epoch [506/800], Loss: 0.0000\n",
      "Epoch [507/800], Loss: 0.0000\n",
      "Epoch [508/800], Loss: 0.0008\n",
      "Epoch [509/800], Loss: 0.0000\n",
      "Epoch [510/800], Loss: 0.0000\n",
      "Epoch [511/800], Loss: 0.0000\n",
      "Epoch [512/800], Loss: 0.0000\n",
      "Epoch [513/800], Loss: 0.0000\n",
      "Epoch [514/800], Loss: 0.0000\n",
      "Epoch [515/800], Loss: 0.0000\n",
      "Epoch [516/800], Loss: 0.0000\n",
      "Epoch [517/800], Loss: 0.0000\n",
      "Epoch [518/800], Loss: 0.0000\n",
      "Epoch [519/800], Loss: 0.0007\n",
      "Epoch [520/800], Loss: 0.0000\n",
      "Epoch [521/800], Loss: 0.0055\n",
      "Epoch [522/800], Loss: 0.0003\n",
      "Epoch [523/800], Loss: 0.0003\n",
      "Epoch [524/800], Loss: 0.0000\n",
      "Epoch [525/800], Loss: 0.0001\n",
      "Epoch [526/800], Loss: 0.0001\n",
      "Epoch [527/800], Loss: 0.0002\n",
      "Epoch [528/800], Loss: 0.0000\n",
      "Epoch [529/800], Loss: 0.0004\n",
      "Epoch [530/800], Loss: 0.0000\n",
      "Epoch [531/800], Loss: 0.0000\n",
      "Epoch [532/800], Loss: 0.0007\n",
      "Epoch [533/800], Loss: 0.0002\n",
      "Epoch [534/800], Loss: 0.0006\n",
      "Epoch [535/800], Loss: 0.0000\n",
      "Epoch [536/800], Loss: 0.0009\n",
      "Epoch [537/800], Loss: 0.0000\n",
      "Epoch [538/800], Loss: 0.0000\n",
      "Epoch [539/800], Loss: 0.0000\n",
      "Epoch [540/800], Loss: 0.0005\n",
      "Epoch [541/800], Loss: 0.0005\n",
      "Epoch [542/800], Loss: 0.0000\n",
      "Epoch [543/800], Loss: 0.0000\n",
      "Epoch [544/800], Loss: 0.0013\n",
      "Epoch [545/800], Loss: 0.0007\n",
      "Epoch [546/800], Loss: 0.0006\n",
      "Epoch [547/800], Loss: 0.0073\n",
      "Epoch [548/800], Loss: 0.0009\n",
      "Epoch [549/800], Loss: 0.0002\n",
      "Epoch [550/800], Loss: 0.0000\n",
      "Epoch [551/800], Loss: 0.0666\n",
      "Epoch [552/800], Loss: 0.0000\n",
      "Epoch [553/800], Loss: 0.0000\n",
      "Epoch [554/800], Loss: 0.0000\n",
      "Epoch [555/800], Loss: 0.0000\n",
      "Epoch [556/800], Loss: 0.0001\n",
      "Epoch [557/800], Loss: 0.0000\n",
      "Epoch [558/800], Loss: 0.0003\n",
      "Epoch [559/800], Loss: 0.0019\n",
      "Epoch [560/800], Loss: 0.0008\n",
      "Epoch [561/800], Loss: 0.0007\n",
      "Epoch [562/800], Loss: 0.0001\n",
      "Epoch [563/800], Loss: 0.0002\n",
      "Epoch [564/800], Loss: 0.0000\n",
      "Epoch [565/800], Loss: 0.0012\n",
      "Epoch [566/800], Loss: 0.0001\n",
      "Epoch [567/800], Loss: 0.0000\n",
      "Epoch [568/800], Loss: 0.0001\n",
      "Epoch [569/800], Loss: 0.0002\n",
      "Epoch [570/800], Loss: 0.0469\n",
      "Epoch [571/800], Loss: 0.0000\n",
      "Epoch [572/800], Loss: 0.0000\n",
      "Epoch [573/800], Loss: 0.0000\n",
      "Epoch [574/800], Loss: 0.0000\n",
      "Epoch [575/800], Loss: 0.0000\n",
      "Epoch [576/800], Loss: 0.0000\n",
      "Epoch [577/800], Loss: 0.0000\n",
      "Epoch [578/800], Loss: 0.0000\n",
      "Epoch [579/800], Loss: 0.0009\n",
      "Epoch [580/800], Loss: 0.0009\n",
      "Epoch [581/800], Loss: 0.0000\n",
      "Epoch [582/800], Loss: 0.0000\n",
      "Epoch [583/800], Loss: 0.0000\n",
      "Epoch [584/800], Loss: 0.0000\n",
      "Epoch [585/800], Loss: 0.0006\n",
      "Epoch [586/800], Loss: 0.0000\n",
      "Epoch [587/800], Loss: 0.0000\n",
      "Epoch [588/800], Loss: 0.0000\n",
      "Epoch [589/800], Loss: 0.0000\n",
      "Epoch [590/800], Loss: 0.0000\n",
      "Epoch [591/800], Loss: 0.0148\n",
      "Epoch [592/800], Loss: 0.0087\n",
      "Epoch [593/800], Loss: 0.0414\n",
      "Epoch [594/800], Loss: 0.0025\n",
      "Epoch [595/800], Loss: 0.0080\n",
      "Epoch [596/800], Loss: 0.0000\n",
      "Epoch [597/800], Loss: 0.0000\n",
      "Epoch [598/800], Loss: 0.0000\n",
      "Epoch [599/800], Loss: 0.0000\n",
      "Epoch [600/800], Loss: 0.0000\n",
      "Epoch [601/800], Loss: 0.0000\n",
      "Epoch [602/800], Loss: 0.0000\n",
      "Epoch [603/800], Loss: 0.0000\n",
      "Epoch [604/800], Loss: 0.0000\n",
      "Epoch [605/800], Loss: 0.0000\n",
      "Epoch [606/800], Loss: 0.0000\n",
      "Epoch [607/800], Loss: 0.0000\n",
      "Epoch [608/800], Loss: 0.0000\n",
      "Epoch [609/800], Loss: 0.0000\n",
      "Epoch [610/800], Loss: 0.0001\n",
      "Epoch [611/800], Loss: 0.0000\n",
      "Epoch [612/800], Loss: 0.0000\n",
      "Epoch [613/800], Loss: 0.0000\n",
      "Epoch [614/800], Loss: 0.0000\n",
      "Epoch [615/800], Loss: 0.0000\n",
      "Epoch [616/800], Loss: 0.0003\n",
      "Epoch [617/800], Loss: 0.0001\n",
      "Epoch [618/800], Loss: 0.0007\n",
      "Epoch [619/800], Loss: 0.0000\n",
      "Epoch [620/800], Loss: 0.0000\n",
      "Epoch [621/800], Loss: 0.0000\n",
      "Epoch [622/800], Loss: 0.0000\n",
      "Epoch [623/800], Loss: 0.0000\n",
      "Epoch [624/800], Loss: 0.0000\n",
      "Epoch [625/800], Loss: 0.0000\n",
      "Epoch [626/800], Loss: 0.0000\n",
      "Epoch [627/800], Loss: 0.0000\n",
      "Epoch [628/800], Loss: 0.0000\n",
      "Epoch [629/800], Loss: 0.0000\n",
      "Epoch [630/800], Loss: 0.0000\n",
      "Epoch [631/800], Loss: 0.0000\n",
      "Epoch [632/800], Loss: 0.0002\n",
      "Epoch [633/800], Loss: 0.0000\n",
      "Epoch [634/800], Loss: 0.0000\n",
      "Epoch [635/800], Loss: 0.0000\n",
      "Epoch [636/800], Loss: 0.0000\n",
      "Epoch [637/800], Loss: 0.0000\n",
      "Epoch [638/800], Loss: 0.0000\n",
      "Epoch [639/800], Loss: 0.0000\n",
      "Epoch [640/800], Loss: 0.0000\n",
      "Epoch [641/800], Loss: 0.0000\n",
      "Epoch [642/800], Loss: 0.0000\n",
      "Epoch [643/800], Loss: 0.0000\n",
      "Epoch [644/800], Loss: 0.0000\n",
      "Epoch [645/800], Loss: 0.0000\n",
      "Epoch [646/800], Loss: 0.0000\n",
      "Epoch [647/800], Loss: 0.0018\n",
      "Epoch [648/800], Loss: 0.0018\n",
      "Epoch [649/800], Loss: 0.0000\n",
      "Epoch [650/800], Loss: 0.0007\n",
      "Epoch [651/800], Loss: 0.0000\n",
      "Epoch [652/800], Loss: 0.0014\n",
      "Epoch [653/800], Loss: 0.0029\n",
      "Epoch [654/800], Loss: 0.0000\n",
      "Epoch [655/800], Loss: 0.0000\n",
      "Epoch [656/800], Loss: 0.0001\n",
      "Epoch [657/800], Loss: 0.0000\n",
      "Epoch [658/800], Loss: 0.0000\n",
      "Epoch [659/800], Loss: 0.0000\n",
      "Epoch [660/800], Loss: 0.0000\n",
      "Epoch [661/800], Loss: 0.0000\n",
      "Epoch [662/800], Loss: 0.0000\n",
      "Epoch [663/800], Loss: 0.0000\n",
      "Epoch [664/800], Loss: 0.0000\n",
      "Epoch [665/800], Loss: 0.0000\n",
      "Epoch [666/800], Loss: 0.0000\n",
      "Epoch [667/800], Loss: 0.0000\n",
      "Epoch [668/800], Loss: 0.0000\n",
      "Epoch [669/800], Loss: 0.0002\n",
      "Epoch [670/800], Loss: 0.0000\n",
      "Epoch [671/800], Loss: 0.0000\n",
      "Epoch [672/800], Loss: 0.0000\n",
      "Epoch [673/800], Loss: 0.0000\n",
      "Epoch [674/800], Loss: 0.0000\n",
      "Epoch [675/800], Loss: 0.0000\n",
      "Epoch [676/800], Loss: 0.0001\n",
      "Epoch [677/800], Loss: 0.0111\n",
      "Epoch [678/800], Loss: 0.0000\n",
      "Epoch [679/800], Loss: 0.0000\n",
      "Epoch [680/800], Loss: 0.0000\n",
      "Epoch [681/800], Loss: 0.0000\n",
      "Epoch [682/800], Loss: 0.0000\n",
      "Epoch [683/800], Loss: 0.0000\n",
      "Epoch [684/800], Loss: 0.0000\n",
      "Epoch [685/800], Loss: 0.0052\n",
      "Epoch [686/800], Loss: 0.0000\n",
      "Epoch [687/800], Loss: 0.0000\n",
      "Epoch [688/800], Loss: 0.0010\n",
      "Epoch [689/800], Loss: 0.0003\n",
      "Epoch [690/800], Loss: 0.0000\n",
      "Epoch [691/800], Loss: 0.0012\n",
      "Epoch [692/800], Loss: 0.0000\n",
      "Epoch [693/800], Loss: 0.0000\n",
      "Epoch [694/800], Loss: 0.0000\n",
      "Epoch [695/800], Loss: 0.0000\n",
      "Epoch [696/800], Loss: 0.0000\n",
      "Epoch [697/800], Loss: 0.0000\n",
      "Epoch [698/800], Loss: 0.0000\n",
      "Epoch [699/800], Loss: 0.0000\n",
      "Epoch [700/800], Loss: 0.0000\n",
      "Epoch [701/800], Loss: 0.0000\n",
      "Epoch [702/800], Loss: 0.0000\n",
      "Epoch [703/800], Loss: 0.0000\n",
      "Epoch [704/800], Loss: 0.0000\n",
      "Epoch [705/800], Loss: 0.0000\n",
      "Epoch [706/800], Loss: 0.0000\n",
      "Epoch [707/800], Loss: 0.0000\n",
      "Epoch [708/800], Loss: 0.0000\n",
      "Epoch [709/800], Loss: 0.0000\n",
      "Epoch [710/800], Loss: 0.0000\n",
      "Epoch [711/800], Loss: 0.0000\n",
      "Epoch [712/800], Loss: 0.0000\n",
      "Epoch [713/800], Loss: 0.1438\n",
      "Epoch [714/800], Loss: 0.0004\n",
      "Epoch [715/800], Loss: 0.0000\n",
      "Epoch [716/800], Loss: 0.0006\n",
      "Epoch [717/800], Loss: 0.0002\n",
      "Epoch [718/800], Loss: 0.0012\n",
      "Epoch [719/800], Loss: 0.0000\n",
      "Epoch [720/800], Loss: 0.0002\n",
      "Epoch [721/800], Loss: 0.0000\n",
      "Epoch [722/800], Loss: 0.0000\n",
      "Epoch [723/800], Loss: 0.0007\n",
      "Epoch [724/800], Loss: 0.0000\n",
      "Epoch [725/800], Loss: 0.0000\n",
      "Epoch [726/800], Loss: 0.0000\n",
      "Epoch [727/800], Loss: 0.0001\n",
      "Epoch [728/800], Loss: 0.0000\n",
      "Epoch [729/800], Loss: 0.0000\n",
      "Epoch [730/800], Loss: 0.0000\n",
      "Epoch [731/800], Loss: 0.0000\n",
      "Epoch [732/800], Loss: 0.0000\n",
      "Epoch [733/800], Loss: 0.0000\n",
      "Epoch [734/800], Loss: 0.0000\n",
      "Epoch [735/800], Loss: 0.0000\n",
      "Epoch [736/800], Loss: 0.0000\n",
      "Epoch [737/800], Loss: 0.0000\n",
      "Epoch [738/800], Loss: 0.0000\n",
      "Epoch [739/800], Loss: 0.0000\n",
      "Epoch [740/800], Loss: 0.0004\n",
      "Epoch [741/800], Loss: 0.0000\n",
      "Epoch [742/800], Loss: 0.0000\n",
      "Epoch [743/800], Loss: 0.0000\n",
      "Epoch [744/800], Loss: 0.0000\n",
      "Epoch [745/800], Loss: 0.0000\n",
      "Epoch [746/800], Loss: 0.0000\n",
      "Epoch [747/800], Loss: 0.0000\n",
      "Epoch [748/800], Loss: 0.0000\n",
      "Epoch [749/800], Loss: 0.0000\n",
      "Epoch [750/800], Loss: 0.0000\n",
      "Epoch [751/800], Loss: 0.0000\n",
      "Epoch [752/800], Loss: 0.0000\n",
      "Epoch [753/800], Loss: 0.0000\n",
      "Epoch [754/800], Loss: 0.0000\n",
      "Epoch [755/800], Loss: 0.0000\n",
      "Epoch [756/800], Loss: 0.0000\n",
      "Epoch [757/800], Loss: 0.0000\n",
      "Epoch [758/800], Loss: 0.0000\n",
      "Epoch [759/800], Loss: 0.0000\n",
      "Epoch [760/800], Loss: 0.0000\n",
      "Epoch [761/800], Loss: 0.0000\n",
      "Epoch [762/800], Loss: 0.0000\n",
      "Epoch [763/800], Loss: 0.0000\n",
      "Epoch [764/800], Loss: 0.0000\n",
      "Epoch [765/800], Loss: 0.0000\n",
      "Epoch [766/800], Loss: 0.0000\n",
      "Epoch [767/800], Loss: 0.0000\n",
      "Epoch [768/800], Loss: 0.0000\n",
      "Epoch [769/800], Loss: 0.0000\n",
      "Epoch [770/800], Loss: 0.0000\n",
      "Epoch [771/800], Loss: 0.0000\n",
      "Epoch [772/800], Loss: 0.0000\n",
      "Epoch [773/800], Loss: 0.0000\n",
      "Epoch [774/800], Loss: 0.0000\n",
      "Epoch [775/800], Loss: 0.0000\n",
      "Epoch [776/800], Loss: 0.0000\n",
      "Epoch [777/800], Loss: 0.0000\n",
      "Epoch [778/800], Loss: 0.0000\n",
      "Epoch [779/800], Loss: 0.0000\n",
      "Epoch [780/800], Loss: 0.0000\n",
      "Epoch [781/800], Loss: 0.0000\n",
      "Epoch [782/800], Loss: 0.0000\n",
      "Epoch [783/800], Loss: 0.0000\n",
      "Epoch [784/800], Loss: 0.0000\n",
      "Epoch [785/800], Loss: 0.0000\n",
      "Epoch [786/800], Loss: 0.0000\n",
      "Epoch [787/800], Loss: 0.0000\n",
      "Epoch [788/800], Loss: 0.0000\n",
      "Epoch [789/800], Loss: 0.0000\n",
      "Epoch [790/800], Loss: 0.0000\n",
      "Epoch [791/800], Loss: 0.0000\n",
      "Epoch [792/800], Loss: 0.0000\n",
      "Epoch [793/800], Loss: 0.0000\n",
      "Epoch [794/800], Loss: 0.0000\n",
      "Epoch [795/800], Loss: 0.0000\n",
      "Epoch [796/800], Loss: 0.0000\n",
      "Epoch [797/800], Loss: 0.0000\n",
      "Epoch [798/800], Loss: 0.0000\n",
      "Epoch [799/800], Loss: 0.0000\n",
      "Epoch [800/800], Loss: 0.0000\n",
      "Accuracy: 63.8889%\n",
      "Epoch [1/200], Loss: 0.0132\n",
      "Epoch [2/200], Loss: 0.0138\n",
      "Epoch [3/200], Loss: 0.0142\n",
      "Epoch [4/200], Loss: 0.0150\n",
      "Epoch [5/200], Loss: 0.0172\n",
      "Epoch [6/200], Loss: 0.0228\n",
      "Epoch [7/200], Loss: 0.0308\n",
      "Epoch [8/200], Loss: 0.0391\n",
      "Epoch [9/200], Loss: 0.0461\n",
      "Epoch [10/200], Loss: 0.0505\n",
      "Epoch [11/200], Loss: 0.0518\n",
      "Epoch [12/200], Loss: 0.0500\n",
      "Epoch [13/200], Loss: 0.0457\n",
      "Epoch [14/200], Loss: 0.0404\n",
      "Epoch [15/200], Loss: 0.0353\n",
      "Epoch [16/200], Loss: 0.0307\n",
      "Epoch [17/200], Loss: 0.0273\n",
      "Epoch [18/200], Loss: 0.0247\n",
      "Epoch [19/200], Loss: 0.0228\n",
      "Epoch [20/200], Loss: 0.0212\n",
      "Epoch [21/200], Loss: 0.0199\n",
      "Epoch [22/200], Loss: 0.0187\n",
      "Epoch [23/200], Loss: 0.0175\n",
      "Epoch [24/200], Loss: 0.0165\n",
      "Epoch [25/200], Loss: 0.0155\n",
      "Epoch [26/200], Loss: 0.0146\n",
      "Epoch [27/200], Loss: 0.0139\n",
      "Epoch [28/200], Loss: 0.0131\n",
      "Epoch [29/200], Loss: 0.0124\n",
      "Epoch [30/200], Loss: 0.0118\n",
      "Epoch [31/200], Loss: 0.0111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# find the best hyperparameters for the model\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005,0.0005, 0.0001]\n",
    "num_epochs = [200,400,800]\n",
    "hidden_sizes = [2,3,6,12, 24, 36, 48]\n",
    "num_classes = 2\n",
    "input_size = 3\n",
    "optimizers = [torch.optim.Adam]\n",
    "# create a list of all possible combinations of hyperparameters\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, num_epochs, hidden_sizes, optimizers))\n",
    "# create a list to store the accuracy of each combination of hyperparameters\n",
    "accuracy_list = []\n",
    "# loop through each combination of hyperparameters\n",
    "\n",
    "for combination in hyperparameter_combinations:\n",
    "\n",
    "    # create pytorch lstm variable recurrent classifier\n",
    "    model = SitUpDetectorSimpleRNN(input_size, num_classes, combination[2]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = combination[3](model.parameters(), lr=combination[0])\n",
    "\n",
    "    myDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(train_x_numpy_no_pad,train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=myDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    losses = []\n",
    "    loss = None\n",
    "    total_steps = len(train_loader)\n",
    "    for epoch in range(combination[1]):\n",
    "        for i, (features, target) in enumerate(train_loader):\n",
    "            hidden_state = model.init_hidden()\n",
    "            for xyz in features:\n",
    "                xyz = torch.tensor(xyz)\n",
    "                output, hidden_state = model(xyz.float(), hidden_state)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "        print(\n",
    "                f\"Epoch [{epoch + 1}/{combination[1]}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "    myTestDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(test_x_numpy_no_pad,test_y)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=myTestDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    myTestDatasetVariableSequenceLengthDataset = MyVariableLengthDataset(test_x_numpy_no_pad,test_y)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=myTestDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = len(test_loader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (features, target) in enumerate(test_loader):\n",
    "            hidden_state = model.init_hidden()\n",
    "            for xyz in features:\n",
    "                xyz = torch.tensor(xyz)\n",
    "                output, hidden_state = model(xyz.float(), hidden_state)\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            num_correct += bool(pred == target)\n",
    "\n",
    "    print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "    accuracy_list.append(num_correct / num_samples * 100)\n",
    "\n",
    "# display the best hyperparameters\n",
    "print(f\"Best hyperparameters: {hyperparameter_combinations[accuracy_list.index(max(accuracy_list))]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: (0.001, 800, 48, <class 'torch.optim.adam.Adam'>)\n",
      "Best accuracy: 63.888888888888886\n",
      "(0.001, 200, 2, <class 'torch.optim.adam.Adam'>): 46.82539682539682\n",
      "(0.001, 200, 3, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 200, 6, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 200, 12, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 200, 24, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 200, 36, <class 'torch.optim.adam.Adam'>): 50.79365079365079\n",
      "(0.001, 200, 48, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 400, 2, <class 'torch.optim.adam.Adam'>): 49.60317460317461\n",
      "(0.001, 400, 3, <class 'torch.optim.adam.Adam'>): 49.2063492063492\n",
      "(0.001, 400, 6, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 400, 12, <class 'torch.optim.adam.Adam'>): 51.19047619047619\n",
      "(0.001, 400, 24, <class 'torch.optim.adam.Adam'>): 52.38095238095239\n",
      "(0.001, 400, 36, <class 'torch.optim.adam.Adam'>): 55.158730158730165\n",
      "(0.001, 400, 48, <class 'torch.optim.adam.Adam'>): 51.587301587301596\n",
      "(0.001, 800, 2, <class 'torch.optim.adam.Adam'>): 49.60317460317461\n",
      "(0.001, 800, 3, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 800, 6, <class 'torch.optim.adam.Adam'>): 50.79365079365079\n",
      "(0.001, 800, 12, <class 'torch.optim.adam.Adam'>): 53.57142857142857\n",
      "(0.001, 800, 24, <class 'torch.optim.adam.Adam'>): 61.904761904761905\n",
      "(0.001, 800, 36, <class 'torch.optim.adam.Adam'>): 50.0\n",
      "(0.001, 800, 48, <class 'torch.optim.adam.Adam'>): 63.888888888888886\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest accuracy: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mmax\u001b[39m(accuracy_list)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(hyperparameter_combinations)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X61sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mhyperparameter_combinations[i]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00maccuracy_list[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "best_hyperparameters = hyperparameter_combinations[accuracy_list.index(max(accuracy_list))]\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "print(f\"Best accuracy: {max(accuracy_list)}\")\n",
    "\n",
    "for i in range(len(hyperparameter_combinations)):\n",
    "    print(f\"{hyperparameter_combinations[i]}: {accuracy_list[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sktime.classification.compose import ColumnEnsembleClassifier\n",
    "from sktime.classification.dictionary_based import BOSSEnsemble\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "# from sktime.classification.shapelet_based import MrSEQLClassifier\n",
    "from sktime.datasets import load_basic_motions\n",
    "from sktime.transformations.panel.compose import ColumnConcatenator\n",
    "\n",
    "\n",
    "from sktime.datatypes._panel._convert import (\n",
    "    from_3d_numpy_to_nested\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_basic_motions(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.head()\n",
    "np.unique(y_train)\n",
    "\n",
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score\", score)\n",
    "\n",
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n",
    "        (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=5), [3]),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"score \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement on my data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert current dataset to sktime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "y = train_y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        col_x = []\n",
    "        col_y = []\n",
    "        col_z = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            col_x.append(col_float[0])   \n",
    "            col_y.append(col_float[1])\n",
    "            col_z.append(col_float[2]) \n",
    "        row_array.append(col_x)\n",
    "        row_array.append(col_y)\n",
    "        row_array.append(col_z)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "\n",
    "print(train_x_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multivariate_dataframe_train_x = from_3d_numpy_to_nested(train_x_numpy_x_y_z_separate_arrays)\n",
    "# multivariate_dataframe_test_x = from_3d_numpy_to_nested(multivariate_list_np_test)\n",
    "print(multivariate_dataframe_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(multivariate_dataframe_train_x, y, random_state=42)\n",
    "\n",
    "\n",
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score is\", score)\n",
    "\n",
    "\n",
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n",
    "        (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=5,n_jobs=-1), [2]),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('time-series-data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36938ddbb6c08077ca1515a91809541f7be305de7f4efe514c09144966008f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

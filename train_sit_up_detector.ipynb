{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torcheval.metrics.functional import binary_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])\n",
    "    \n",
    "\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "\n",
    "# drop the collumn Y from the dataframe\n",
    "\n",
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "\n",
    "# make the tran_x data rows have the same length as the sequence_length by padding with [0,0,0] for each collumn\n",
    "\n",
    "# train_x = train_x.reindex(range(sequence_length), fill_value=[0,0,0])\n",
    "\n",
    "test_x = test_df.drop(['Y'], axis=1)\n",
    "\n",
    "\n",
    "test_y = test_df['Y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3)\n",
      "[ 0.51171875 -0.10406494 -0.10406494]\n"
     ]
    }
   ],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            row_array.append(col_float)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "test_x_numpy = convert_rows_to_nupy_array(test_x)\n",
    "\n",
    "print(train_x_numpy.shape)\n",
    "print(test_x_numpy[1][5])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Padding from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13502/3299080516.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n"
     ]
    }
   ],
   "source": [
    "train_y_no_pad = train_y\n",
    "test_y_no_pad = test_y\n",
    "\n",
    "# remove the numpy array where you hava a [0,0,0] array\n",
    "\n",
    "test_np_array = np.array([[1,2,1], [0,0,0], [1,2,1]])\n",
    "\n",
    "# remove the [0,0,0] array from the numpy array test_np_array\n",
    "\n",
    "test_np_array = test_np_array[test_np_array[:,0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "train_x_numpy_no_pad = []\n",
    "# row_index = 0\n",
    "for row in range(len(train_x_numpy)):\n",
    "    temp = train_x_numpy[row][train_x_numpy[row][:,0] != 0]\n",
    "    temp_list = []\n",
    "    for col in temp:\n",
    "        item_list = []\n",
    "        for item in col:\n",
    "            item_list.append(item)\n",
    "        temp_list.append(item_list)\n",
    "    train_x_numpy_no_pad.append(temp_list)\n",
    "train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n",
    "print(train_x_numpy_no_pad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3) (2057,)\n",
      "0.7704280155642024\n"
     ]
    }
   ],
   "source": [
    "# Dummy Classifer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "print(train_x_numpy.shape, train_y.shape)\n",
    "dummy_clf.fit(train_x_numpy, train_y)\n",
    "y_pred = dummy_clf.predict(test_x_numpy)\n",
    "print(accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    41\n",
      "1     9\n",
      "Name: Y, dtype: int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m val_h \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([each\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m val_h])\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m inp, lab \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mto(device), lab\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m out, val_h \u001b[39m=\u001b[39m model(inp, val_h)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m val_loss \u001b[39m=\u001b[39m criterion(out\u001b[39m.\u001b[39msqueeze(), lab\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m val_losses\u001b[39m.\u001b[39mappend(val_loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 9\u001b[0m in \u001b[0;36mSitUpDetector.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hidden):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    690\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    692\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    693\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    695\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return  self.input.shape[0]\n",
    "\n",
    "# test_input = np.arange(1,8).reshape(-1,1)\n",
    "# input = torch.tensor(test_input, dtype=torch.float)\n",
    "\n",
    "# dataset = MyDataset(input, 3)\n",
    "\n",
    "# dl = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "# for inp, label in dl:\n",
    "#     print(\"inp shape\",inp.shape)\n",
    "#     print(\"inp\",inp.numpy())\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "input_size = 3\n",
    "num_classes = 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "\n",
    "class SitUpDetector(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers,drop_prob=0.5):\n",
    "        super(SitUpDetector, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, _ = self.lstm(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:, -1]\n",
    "        return out,hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device)) \n",
    "        return hidden\n",
    "    \n",
    "\n",
    "\n",
    "model = SitUpDetector(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# reduce size of train_x_numpy_no_pad to 1000\n",
    "train_x_numpy = train_x_numpy[:50]\n",
    "train_y = train_y[:50]\n",
    "\n",
    "test_x_numpy = test_x_numpy[:50]\n",
    "test_y = test_y[:50]\n",
    "\n",
    "\n",
    "# print how many 1 and 0 in the train_y\n",
    "print(train_y.value_counts())\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=batch_size)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "num_epochs = 10\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        # features = features.float()\n",
    "\n",
    "        features, target = features.to(device), target.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs, h = model(features.float(), h)\n",
    "        loss = criterion(outputs, target.float())\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 50 == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in train_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, num_epochs),\n",
    "                  \"Step: {}...\".format(i),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "\n",
    "# graph the losses \n",
    "\n",
    "# get the lenght of the losses array\n",
    "\n",
    "# create an array of the same length as the losses array with index values\n",
    "x = np.arange(0,num_epochs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,losses)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cm = confusion_matrix(test_y, all_predictions)\n",
    "# print(cm)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "torch.save(model.state_dict(), 'sit_up_detector.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([749, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y_pred\u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#binary accuracy \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 10\u001b[0m in \u001b[0;36mSitUpDetectorVariableSequenceLength.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m s, n \u001b[39m=\u001b[39m xx\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(xx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(xx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(s, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m outputs\u001b[39m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:689\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 689\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    690\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    692\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:632\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    628\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    629\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    630\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    631\u001b[0m                        ):\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    634\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    635\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    636\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:201\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    199\u001b[0m expected_input_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m3\u001b[39m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m expected_input_dim:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    203\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    205\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "# Check accuracy\n",
    "test_dataset = MyDataset(test_x_numpy, test_y, sequence_length)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1)\n",
    "n_correct = 0\n",
    "n_samples = 0\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_dataloader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_dataloader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with no Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDatasetVariableSequenceLength(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([749, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (features, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# convert Double tensor to Float tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, target\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 12\u001b[0m in \u001b[0;36mSitUpDetectorVariableSequenceLength.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m s, n \u001b[39m=\u001b[39m xx\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(xx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(xx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(s, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m outputs\u001b[39m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:689\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 689\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    690\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    692\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:632\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    628\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    629\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    630\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    631\u001b[0m                        ):\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    634\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    635\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    636\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/torch/nn/modules/rnn.py:201\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    199\u001b[0m expected_input_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m3\u001b[39m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m expected_input_dim:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    203\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    205\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "#  create pytorch lstm variable recurrent classifier\n",
    "\n",
    "input_size = 3\n",
    "num_classes = 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "class SitUpDetectorVariableSequenceLength(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers):\n",
    "        super(SitUpDetectorVariableSequenceLength, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        \n",
    "        # x = torch.squeeze(x) # remove the batch dimension maybe sometime allow for batch size > 1\n",
    "        \n",
    "        for xx in x:\n",
    "            s, n = xx.shape\n",
    "            print(xx.shape)\n",
    "            out, _ = self.lstm(xx)\n",
    "            out = out.view(s, -1)\n",
    "            outputs.append(out)\n",
    "\n",
    "        outputs = torch.stack(outputs)\n",
    "\n",
    "        out = self.fc(outputs)\n",
    "        return out\n",
    "\n",
    "# train SitUpDetectorVariableInput\n",
    "model = SitUpDetectorVariableSequenceLength(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "myDatasetVariableSequenceLengthDataset = MyDatasetVariableSequenceLength(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "num_epochs = 1\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        features = features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "        \n",
    "        loss = criterion(outputs, target.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_steps, loss.item()))\n",
    "    losses.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# univariate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate time series classifier\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('time-series-data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36938ddbb6c08077ca1515a91809541f7be305de7f4efe514c09144966008f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

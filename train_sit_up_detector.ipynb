{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])\n",
    "    \n",
    "\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "\n",
    "# drop the collumn Y from the dataframe\n",
    "\n",
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "\n",
    "# make the tran_x data rows have the same length as the sequence_length by padding with [0,0,0] for each collumn\n",
    "\n",
    "# train_x = train_x.reindex(range(sequence_length), fill_value=[0,0,0])\n",
    "\n",
    "test_x = test_df.drop(['Y'], axis=1)\n",
    "\n",
    "\n",
    "test_y = test_df['Y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3)\n",
      "[ 0.51171875 -0.10406494 -0.10406494]\n"
     ]
    }
   ],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            row_array.append(col_float)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "test_x_numpy = convert_rows_to_nupy_array(test_x)\n",
    "\n",
    "print(train_x_numpy.shape)\n",
    "print(test_x_numpy[1][5])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Padding from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3)\n"
     ]
    }
   ],
   "source": [
    "train_y_no_pad = train_y\n",
    "test_y_no_pad = test_y\n",
    "\n",
    "# remove the numpy array at the axis 3 that has values of a numpy array with all zeros\n",
    "\n",
    "train_x_numpy_no_pad = np.delete(train_x_numpy, np.where(~train_x_numpy.any(axis=(0,1,2))), axis=0)\n",
    "test_x_numpy_no_pad = np.delete(test_x_numpy, np.where(~test_x_numpy.any(axis=(0,1,2))), axis=0)\n",
    "print(train_x_numpy_no_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3) (2057,)\n",
      "0.7704280155642024\n"
     ]
    }
   ],
   "source": [
    "# Dummy Classifer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "print(train_x_numpy.shape, train_y.shape)\n",
    "dummy_clf.fit(train_x_numpy, train_y)\n",
    "y_pred = dummy_clf.predict(test_x_numpy)\n",
    "print(accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/2057], Loss: 2.0248\n",
      "Epoch [1/1], Step [200/2057], Loss: 1.3374\n",
      "Epoch [1/1], Step [300/2057], Loss: 0.1863\n",
      "Epoch [1/1], Step [400/2057], Loss: 0.3374\n",
      "Epoch [1/1], Step [500/2057], Loss: 1.4976\n",
      "Epoch [1/1], Step [600/2057], Loss: 0.2362\n",
      "Epoch [1/1], Step [700/2057], Loss: 1.4465\n",
      "Epoch [1/1], Step [800/2057], Loss: 1.2325\n",
      "Epoch [1/1], Step [900/2057], Loss: 1.3849\n",
      "Epoch [1/1], Step [1000/2057], Loss: 1.3731\n",
      "Epoch [1/1], Step [1100/2057], Loss: 1.3837\n",
      "Epoch [1/1], Step [1200/2057], Loss: 0.3613\n",
      "Epoch [1/1], Step [1300/2057], Loss: 0.3642\n",
      "Epoch [1/1], Step [1400/2057], Loss: 1.2773\n",
      "Epoch [1/1], Step [1500/2057], Loss: 0.3934\n",
      "Epoch [1/1], Step [1600/2057], Loss: 1.5072\n",
      "Epoch [1/1], Step [1700/2057], Loss: 0.3032\n",
      "Epoch [1/1], Step [1800/2057], Loss: 1.4454\n",
      "Epoch [1/1], Step [1900/2057], Loss: 1.1267\n",
      "Epoch [1/1], Step [2000/2057], Loss: 0.3620\n",
      "Got 396 / 514 with accuracy 77.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]\n",
    "\n",
    "# test_input = np.arange(1,8).reshape(-1,1)\n",
    "# input = torch.tensor(test_input, dtype=torch.float)\n",
    "\n",
    "# dataset = MyDataset(input, 3)\n",
    "\n",
    "# dl = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "# for inp, label in dl:\n",
    "#     print(\"inp shape\",inp.shape)\n",
    "#     print(\"inp\",inp.numpy())\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "input_size = 3\n",
    "num_classes = 2\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "class SitUpDetector(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers):\n",
    "        super(SitUpDetector, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = SitUpDetector(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=1)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        features = features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_steps, loss.item()))\n",
    "\n",
    "# Check accuracy\n",
    "test_dataset = MyDataset(test_x_numpy, test_y, sequence_length)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1)\n",
    "n_correct = 0\n",
    "n_samples = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dataloader:\n",
    "        x = x.to(device=device).squeeze(1)\n",
    "        y = y.to(device=device)\n",
    "\n",
    "        x = x.float()\n",
    "\n",
    "        scores = model(x)\n",
    "\n",
    "        _, predictions = scores.max(1)\n",
    "\n",
    "        n_correct += (predictions == y).sum()\n",
    "        n_samples += predictions.size(0)\n",
    "\n",
    "    print(f'Got {n_correct} / {n_samples} with accuracy {float(n_correct)/float(n_samples)*100:.2f}')\n",
    "\n",
    "# save the model\n",
    "\n",
    "torch.save(model.state_dict(), 'sit_up_detector.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with no Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new Pytorch LSTM model that takes a variable input\n",
    "\n",
    "class SitUpDetectorVariable(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers):\n",
    "        super(SitUpDetectorVariable, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# train the model\n",
    "\n",
    "model = SitUpDetectorVariable(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy_no_pad,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=1)\n",
    "\n",
    "num_epochs = 5\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        features = features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_steps, loss.item()))\n",
    "                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('time-series-data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36938ddbb6c08077ca1515a91809541f7be305de7f4efe514c09144966008f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

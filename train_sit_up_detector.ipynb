{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torcheval.metrics.functional import binary_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing/ setting up data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data from tsv and get the sequence length to use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance the positive and negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sit_ups = train_df[train_df['Y'] == 1]\n",
    "train_df_non_sit_ups = train_df[train_df['Y'] == 0]\n",
    "\n",
    "\n",
    "train_df_non_sit_ups_balanced = train_df_non_sit_ups.sample(n=len(train_df_sit_ups), random_state=0)\n",
    "\n",
    "\n",
    "train_df = pd.concat([train_df_sit_ups, train_df_non_sit_ups])\n",
    "\n",
    "test_df_sit_ups = test_df[test_df['Y'] == 1]\n",
    "test_df_non_sit_ups = test_df[test_df['Y'] == 0]\n",
    "\n",
    "test_df_non_sit_ups = test_df_non_sit_ups.sample(n=len(test_df_sit_ups), random_state=0)\n",
    "\n",
    "test_df_balanced = pd.concat([test_df_sit_ups, test_df_non_sit_ups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate x and y from test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "\n",
    "test_x = test_df.drop(['Y'], axis=1)\n",
    "\n",
    "\n",
    "test_y = test_df['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the cells in each collumn of the dataframe from string to a numpy array and convert the whole dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3)\n",
      "[ 0.51171875 -0.10406494 -0.10406494]\n"
     ]
    }
   ],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            row_array.append(col_float)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "test_x_numpy = convert_rows_to_nupy_array(test_x)\n",
    "\n",
    "print(train_x_numpy.shape)\n",
    "print(test_x_numpy[1][5])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Padding from numpy and create separate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_766139/3299080516.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n"
     ]
    }
   ],
   "source": [
    "train_y_no_pad = train_y\n",
    "test_y_no_pad = test_y\n",
    "\n",
    "# remove the numpy array where you hava a [0,0,0] array\n",
    "\n",
    "test_np_array = np.array([[1,2,1], [0,0,0], [1,2,1]])\n",
    "\n",
    "# remove the [0,0,0] array from the numpy array test_np_array\n",
    "\n",
    "test_np_array = test_np_array[test_np_array[:,0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "train_x_numpy_no_pad = []\n",
    "# row_index = 0\n",
    "for row in range(len(train_x_numpy)):\n",
    "    temp = train_x_numpy[row][train_x_numpy[row][:,0] != 0]\n",
    "    temp_list = []\n",
    "    for col in temp:\n",
    "        item_list = []\n",
    "        for item in col:\n",
    "            item_list.append(item)\n",
    "        temp_list.append(item_list)\n",
    "    train_x_numpy_no_pad.append(temp_list)\n",
    "train_x_numpy_no_pad = np.array(train_x_numpy_no_pad)\n",
    "print(train_x_numpy_no_pad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 749, 3) (2057,)\n",
      "0.7704280155642024\n"
     ]
    }
   ],
   "source": [
    "# Dummy Classifer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "print(train_x_numpy.shape, train_y.shape)\n",
    "dummy_clf.fit(train_x_numpy, train_y)\n",
    "y_pred = dummy_clf.predict(test_x_numpy)\n",
    "print(accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target, seq_len):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, item):\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return  self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3                                                                           \n",
    "num_classes = 1\n",
    "hidden_size = 3\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing reduce to certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    50\n",
      "Name: Y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# reduce size of train_x_numpy_no_pad to 1000\n",
    "train_x_numpy = train_x_numpy[:50]\n",
    "train_y = train_y[:50]\n",
    "\n",
    "test_x_numpy = test_x_numpy[:50]\n",
    "test_y = test_y[:50]\n",
    "\n",
    "\n",
    "# print how many 1 and 0 in the train_y\n",
    "print(train_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitUpDetector(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size, num_layers,drop_prob=0.5):\n",
    "        super(SitUpDetector, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, _ = self.lstm(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:, -1]\n",
    "        return out,hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_().float().to(device)) \n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Optimizers, dataloader and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SitUpDetector(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "myDataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size=batch_size)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "valid_loss_min = np.Inf\n",
    "total_steps = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 49... Loss: 0.807959... Val Loss: 0.715517\n",
      "losses as np array [0.80795854]\n",
      "Validation loss decreased (inf --> 0.715517).  Saving model ...\n",
      "plt y shpae loss (1,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "        features, target = features.to(device), target.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs, h = model(features.float(), h)\n",
    "        loss = criterion(outputs, target.float())\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 50 == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in train_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp.float(), val_h)\n",
    "                val_loss = criterion(out, lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "                  \"Step: {}...\".format(i),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            losses.append(loss.item())\n",
    "            # print(\"why\",\"{:.6f}\".format(loss.item().detach().numpy()))\n",
    "\n",
    "\n",
    "            print(\"losses as np array\",np.array(losses))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "\n",
    "# graph the losses \n",
    "\n",
    "# get the lenght of the losses array\n",
    "\n",
    "# create an array of the same length as the losses array with index values\n",
    "for i in range(len(losses)):\n",
    "    if type(losses[i]) == torch.Tensor:\n",
    "        losses[i] = losses[i].detach().numpy()\n",
    "\n",
    "y = np.array(losses)\n",
    "print(\"plt y shpae loss\", y.shape)\n",
    "# print(\"y shpae\")\n",
    "# print(y)\n",
    "\n",
    "x = np.arange(0,len(losses))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot()\n",
    "plt.savefig(f\"loss-for-params-num-{num_layers}-learning_rate{learning_rate}.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cm = confusion_matrix(test_y, all_predictions)\n",
    "# print(cm)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "torch.save(model.state_dict(), 'sit_up_detector.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.716\n",
      "Test accuracy: 0.000%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy\n",
    "test_dataset = MyDataset(train_x_numpy,train_y, sequence_length)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1)\n",
    "n_correct = 0\n",
    "n_samples = 0\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for inputs, label in train_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, label = inputs.to(device), label.to(device)\n",
    "    output, h = model(inputs.float(), h)\n",
    "    test_loss = criterion(output, label.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(label.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_dataloader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with different network with data spread out and with simpler rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flatten = train_x_numpy[0].flatten()\n",
    "\n",
    "# flatten the rows in train_x_numpy to 1D array\n",
    "train_x_numpy_flatten_list = []\n",
    "for i in range(len(train_x_numpy)):\n",
    "    train_x_numpy_flatten_list.append(train_x_numpy[i].flatten().tolist())\n",
    "\n",
    "train_x_flatten_numpy = np.array(train_x_numpy_flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFlattenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "    def __getitem__(self, item):\n",
    "        # print(\"input item\", self.input[item])\n",
    "        return self.input[item], self.target.iloc[item]\n",
    "\n",
    "        # return input[item:item+self.seq_len], input[item+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "num_classes = 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitUpDetectorSimplerFlattened(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_size):\n",
    "        super(SitUpDetectorSimplerFlattened, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, num_classes)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SitUpDetectorVariableSequenceLength' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#  create pytorch lstm variable recurrent classifier\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# train SitUpDetectorVariableInput\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m SitUpDetectorVariableSequenceLength(input_size, num_classes, hidden_size, num_layers)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SitUpDetectorVariableSequenceLength' is not defined"
     ]
    }
   ],
   "source": [
    "#  create pytorch lstm variable recurrent classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train SitUpDetectorVariableInput\n",
    "model = SitUpDetectorVariableSequenceLength(input_size, num_classes, hidden_size, num_layers).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "myDatasetVariableSequenceLengthDataset = MyDatasetVariableSequenceLength(train_x_numpy,train_y, sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDatasetVariableSequenceLengthDataset, batch_size=1, shuffle=False)\n",
    "\n",
    "losses = []\n",
    "loss = None\n",
    "num_epochs = 1\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "\n",
    "        # convert Double tensor to Float tensor\n",
    "        features = features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "        \n",
    "        loss = criterion(outputs, target.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_steps, loss.item()))\n",
    "    losses.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sktime.classification.compose import ColumnEnsembleClassifier\n",
    "from sktime.classification.dictionary_based import BOSSEnsemble\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "# from sktime.classification.shapelet_based import MrSEQLClassifier\n",
    "from sktime.datasets import load_basic_motions\n",
    "from sktime.transformations.panel.compose import ColumnConcatenator\n",
    "\n",
    "\n",
    "from sktime.datatypes._panel._convert import (\n",
    "    from_3d_numpy_to_nested,\n",
    "    from_multi_index_to_3d_numpy,\n",
    "    from_nested_to_3d_numpy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 6)\n",
      "(80,)\n",
      "standing\n",
      "(60, 6) (60,) (20, 6) (20,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m clf \u001b[39m=\u001b[39m Pipeline(steps)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m score \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mscore(X_test, y_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m, score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m clf \u001b[39m=\u001b[39m ColumnEnsembleClassifier(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     estimators\u001b[39m=\u001b[39m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mTSF0\u001b[39m\u001b[39m\"\u001b[39m, TimeSeriesForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m), [\u001b[39m0\u001b[39m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mBOSSEnsemble3\u001b[39m\u001b[39m\"\u001b[39m, BOSSEnsemble(max_ensemble_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m), [\u001b[39m3\u001b[39m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kf7mxe/Projects/situp-detector/train_sit_up_detector.ipynb#X60sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sklearn/pipeline.py:699\u001b[0m, in \u001b[0;36mPipeline.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    698\u001b[0m     score_params[\u001b[39m\"\u001b[39m\u001b[39msample_weight\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sample_weight\n\u001b[0;32m--> 699\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mscore(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mscore_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 666\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/interval_based/_tsf.py:115\u001b[0m, in \u001b[0;36mTimeSeriesForestClassifier.predict\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    114\u001b[0m     \u001b[39m\"\"\"Wrap predict to call BaseClassifier.predict.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m BaseClassifier\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m, X\u001b[39m=\u001b[39;49mX, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/base.py:228\u001b[0m, in \u001b[0;36mBaseClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_single_class_y_pred(X, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[39m# call internal _predict_proba\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/interval_based/_tsf.py:139\u001b[0m, in \u001b[0;36mTimeSeriesForestClassifier._predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, X) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    125\u001b[0m     \u001b[39m\"\"\"Find predictions for all cases in X. Built on top of predict_proba.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m    output : array of shape = [n_test_instances]\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(prob)] \u001b[39mfor\u001b[39;00m prob \u001b[39min\u001b[39;00m proba])\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/interval_based/_tsf.py:119\u001b[0m, in \u001b[0;36mTimeSeriesForestClassifier.predict_proba\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_proba\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    118\u001b[0m     \u001b[39m\"\"\"Wrap predict_proba to call BaseClassifier.predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m BaseClassifier\u001b[39m.\u001b[39;49mpredict_proba(\u001b[39mself\u001b[39;49m, X\u001b[39m=\u001b[39;49mX, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/base.py:262\u001b[0m, in \u001b[0;36mBaseClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_single_class_y_pred(X, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpredict_proba\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[39m# call internal _predict_proba\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_proba(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/interval_based/_tsf.py:161\u001b[0m, in \u001b[0;36mTimeSeriesForestClassifier._predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m\"\"\"Find probability estimates for each class for all cases in X.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m    Predicted probabilities\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m y_probas \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    162\u001b[0m     delayed(_predict_single_classifier_proba)(\n\u001b[1;32m    163\u001b[0m         X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintervals_[i]\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators)\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    168\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(y_probas, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m (\n\u001b[1;32m    169\u001b[0m     np\u001b[39m.\u001b[39mones(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/interval_based/_tsf.py:203\u001b[0m, in \u001b[0;36m_predict_single_classifier_proba\u001b[0;34m(X, estimator, intervals)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict_single_classifier_proba\u001b[39m(X, estimator, intervals):\n\u001b[1;32m    202\u001b[0m     \u001b[39m\"\"\"Find probability estimates for each class for all cases in X.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     Xt \u001b[39m=\u001b[39m _transform(X, intervals)\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39mpredict_proba(Xt)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/series_as_features/base/estimators/interval_based/_tsf.py:131\u001b[0m, in \u001b[0;36m_transform\u001b[0;34m(X, intervals)\u001b[0m\n\u001b[1;32m    129\u001b[0m X_slice \u001b[39m=\u001b[39m X[:, intervals[j][\u001b[39m0\u001b[39m] : intervals[j][\u001b[39m1\u001b[39m]]\n\u001b[1;32m    130\u001b[0m means \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(X_slice, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m std_dev \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstd(X_slice, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    132\u001b[0m slope \u001b[39m=\u001b[39m _slope(X_slice, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    133\u001b[0m transformed_x[\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m j] \u001b[39m=\u001b[39m means\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mstd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3581\u001b[0m, in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3578\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3579\u001b[0m         \u001b[39mreturn\u001b[39;00m std(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, ddof\u001b[39m=\u001b[39mddof, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3581\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_std(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, ddof\u001b[39m=\u001b[39;49mddof,\n\u001b[1;32m   3582\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/numpy/core/_methods.py:262\u001b[0m, in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_std\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ddof\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[1;32m    261\u001b[0m          where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 262\u001b[0m     ret \u001b[39m=\u001b[39m _var(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, ddof\u001b[39m=\u001b[39;49mddof,\n\u001b[1;32m    263\u001b[0m                keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    266\u001b[0m         ret \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39msqrt(ret, out\u001b[39m=\u001b[39mret)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/numpy/core/_methods.py:230\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    225\u001b[0m     arrmean \u001b[39m=\u001b[39m arrmean\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype(arrmean \u001b[39m/\u001b[39m rcount)\n\u001b[1;32m    227\u001b[0m \u001b[39m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39m# not a scalar.\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m x \u001b[39m=\u001b[39m asanyarray(arr \u001b[39m-\u001b[39;49m arrmean)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(arr\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, (nt\u001b[39m.\u001b[39mfloating, nt\u001b[39m.\u001b[39minteger)):\n\u001b[1;32m    233\u001b[0m     x \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmultiply(x, x, out\u001b[39m=\u001b[39mx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = load_basic_motions(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "X_train.head()\n",
    "np.unique(y_train)\n",
    "\n",
    "\n",
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score\", score)\n",
    "\n",
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n",
    "        (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=5), [3]),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"score \", score)\n",
    "\n",
    "# clf = MrSEQLClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement on my data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert current dataset to sktime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data.tsv file and create a dataframe\n",
    "\n",
    "df = pd.read_csv('data.tsv', sep='\\t')\n",
    "\n",
    "# open file data_info.txt and read the first line\n",
    "\n",
    "sequence_length = None\n",
    "with open('data_info.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    split_line_on_collen = line.split(':')\n",
    "    sequence_length = int(split_line_on_collen[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = df.sample(frac=0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "train_x = train_df.drop(['Y'], axis=1).iloc[1:]\n",
    "train_y = train_df['Y'].iloc[1:]\n",
    "y = train_y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 3, 749)\n"
     ]
    }
   ],
   "source": [
    "def convert_rows_to_nupy_array(df):\n",
    "    numpy_array = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_array = []\n",
    "        col_x = []\n",
    "        col_y = []\n",
    "        col_z = []\n",
    "        for col in row.iteritems():\n",
    "            col_float = []\n",
    "            for item in col[1].split(','):\n",
    "                col_float.append(float(item.replace('[', '').replace(']', '')))\n",
    "            col_x.append(col_float[0])   \n",
    "            col_y.append(col_float[1])\n",
    "            col_z.append(col_float[2]) \n",
    "        row_array.append(col_x)\n",
    "        row_array.append(col_y)\n",
    "        row_array.append(col_z)\n",
    "        numpy_array.append(row_array)\n",
    "    return np.array(numpy_array)\n",
    "\n",
    "train_x_numpy = convert_rows_to_nupy_array(train_x)\n",
    "\n",
    "print(train_x_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2057, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multivariate_dataframe_train_x = from_3d_numpy_to_nested(train_x_numpy)\n",
    "# multivariate_dataframe_test_x = from_3d_numpy_to_nested(multivariate_list_np_test)\n",
    "print(multivariate_dataframe_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is 0.6699029126213593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kf7mxe/anaconda3/envs/time-series-data-mining/lib/python3.8/site-packages/sktime/classification/dictionary_based/_boss.py:215: UserWarning: ``typed_dict`` was deprecated in version 0.13.3 and will be removed in 0.15.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9844660194174757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(multivariate_dataframe_train_x, y, random_state=42)\n",
    "\n",
    "\n",
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score is\", score)\n",
    "\n",
    "\n",
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n",
    "        (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=5,n_jobs=-1), [2]),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('time-series-data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36938ddbb6c08077ca1515a91809541f7be305de7f4efe514c09144966008f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
